from agentscope.agent import AgentBase
from typing import Dict, Any
import json
import re
from worker.llm_utils import call_llm
from worker.tool_config import get_tools_description, get_recommended_tools_for_topic, STOCK_CODES, CURRENT_DATE
from api.prompt_service import PromptService
from api.database import SessionLocal
from api.redis_client import get_redis_client
from worker.llm_utils import call_llm_async
import asyncio
from worker.evidence_lifecycle import EvidenceLifecycle

# [Feature Flag: Facilitation]
try:
    from worker.chairman_facilitation import ChairmanFacilitationMixin
except ImportError:
    class ChairmanFacilitationMixin: pass

class Chairman(AgentBase, ChairmanFacilitationMixin):
    """
    ä¸»å¸­æ™ºèƒ½é«”ï¼Œè² è²¬ä¸»æŒè¾¯è«–ã€è³½å‰åˆ†æå’Œè³½å¾Œç¸½çµã€‚
    """

    def __init__(self, name: str, **kwargs: Any):
        super().__init__()
        self.name = name

    def speak(self, content: str):
        print(f"Chairman '{self.name}': {content}")

    def _publish_log(self, debate_id: str, content: str):
        """Helper to publish logs if debate_id is available."""
        if not debate_id:
            return
        
        try:
            redis_client = get_redis_client()
            from datetime import datetime
            timestamp = datetime.now().strftime("%H:%M:%S")
            ui_content = f"[{timestamp}] {content}"
            message = json.dumps({"role": f"Chairman ({self.name})", "content": ui_content}, ensure_ascii=False)
            redis_client.publish(f"debate:{debate_id}:log_stream", message)
            redis_client.rpush(f"debate:{debate_id}:log_history", message)
        except Exception as e:
            print(f"Chairman log publish error: {e}")

    async def _fallback_from_tej_price(self, params: Dict[str, Any], debate_id: str = None):
        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()

        symbol = params.get("coid") or params.get("symbol") or params.get("code")
        if not symbol:
            return None

        symbol_str = str(symbol)
        base_id = symbol_str.split(".")[0]

        twse_params = {"symbol": base_id, "date": CURRENT_DATE}

        try:
            self._publish_log(debate_id, f"ğŸ”„ TEJ è‚¡åƒ¹æŸ¥è©¢å¤±æ•—ï¼Œå˜—è©¦ TWSE æ—¥æ”¶ç›¤åƒ¹ï¼š{base_id} ({CURRENT_DATE})")
            res = await loop.run_in_executor(None, call_tool, "twse.stock_day", twse_params)
            if res and isinstance(res, dict):
                if res.get("error"):
                    raise ValueError(res.get("error"))
                rows = res.get("data") or res.get("results") or res.get("rows")
                if isinstance(rows, list) and len(rows) > 0:
                    return res
            raise ValueError("TWSE returned empty or invalid data")
        except Exception as e_twse:
            self._publish_log(debate_id, f"âš ï¸ TWSE å‚™æ´å¤±æ•—ï¼š{e_twse}ï¼Œæ”¹ç”¨ Verified Priceã€‚")
            try:
                fp_params = {"symbol": symbol_str}
                res = await loop.run_in_executor(None, call_tool, "financial.get_verified_price", fp_params)
                return res
            except Exception as e_v:
                self._publish_log(debate_id, f"âŒ Verified Price å‚™æ´äº¦å¤±æ•—ï¼š{e_v}")
                return None

    async def _investigate_topic_async(self, topic: str, debate_id: str = None) -> str:
        """
        Async implementation of investigation loop.
        """
        self._publish_log(debate_id, "ğŸ•µï¸ ä¸»å¸­æ­£åœ¨é€²è¡ŒèƒŒæ™¯èª¿æŸ¥ (Entity Recognition)...")
        
        # 1. Prepare Tools (Search & TEJ + ODS)
        investigation_tools = []
        from api.config import Config
        target_tool_names = ["searxng.search"]
        if Config.ENABLE_TEJ_TOOLS:
            target_tool_names += ["tej.company_info", "tej.stock_price"]
        
        # Use lazy import to avoid circular dependency with api.tool_registry
        from api.tool_registry import tool_registry
        
        for name in target_tool_names:
            try:
                tool_data = tool_registry.get_tool_data(name)
                # Ensure valid schema
                schema = tool_data.get('schema')
                if not schema:
                    schema = {"type": "object", "properties": {}, "required": []}
                elif isinstance(schema, dict):
                    if "type" not in schema: schema["type"] = "object"
                    if "properties" not in schema: schema["properties"] = {}
                
                # Fix: description might be a dict (metadata) or a string
                desc = tool_data.get('description', '')
                if isinstance(desc, dict):
                    desc = desc.get('description', '')

                investigation_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": desc,
                        "parameters": schema
                    }
                })
            except:
                pass
        
        if not investigation_tools:
            return "ç„¡æ³•åŠ è¼‰èª¿æŸ¥å·¥å…·ï¼Œè·³éèƒŒæ™¯èª¿æŸ¥ã€‚"

        # 2. Prompt for Investigation
        prompt = f"""
è«‹å°è¾¯é¡Œã€Œ{topic}ã€é€²è¡Œåš´æ ¼çš„èƒŒæ™¯äº‹å¯¦èª¿æŸ¥ (Fact-Checking)ã€‚

**æ ¸å¿ƒä»»å‹™**ï¼š
1. **è­˜åˆ¥å¯¦é«”**ï¼šæ‰¾å‡ºå…¬å¸å…¨åèˆ‡è‚¡ç¥¨ä»£ç¢¼ (e.g., æ£®é‰… -> 8942)ã€‚
2. **ç”¢æ¥­å®šä½**ï¼šç¢ºèªå…¶ä¸»è¦ç”¢å“èˆ‡æ‰€å±¬ç”¢æ¥­ã€‚
   - âš ï¸ æ³¨æ„ï¼šä¸è¦ä¾è³´ç›´è¦ºçŒœæ¸¬ç”¢æ¥­ã€‚è‹¥ TEJ/ChinaTimes æŸ¥ç„¡è³‡æ–™ï¼Œ**å¿…é ˆ**ä½¿ç”¨ `searxng.search` æœå°‹ã€Œ{{å…¬å¸å}} åšä»€éº¼ã€æˆ–ã€Œ{{å…¬å¸å}} ç”¢å“ã€ã€‚
   - ç¯„ä¾‹ï¼šæ£®é‰… (8942) æ˜¯åšã€Œé‡‘å±¬è¤‡åˆæ¿/å»ºæã€ï¼Œçµ•éé›»å­è‚¡ã€‚è«‹å‹™å¿…æ ¸å¯¦ã€‚
3. **æ•¸æ“šæª¢æ ¸**ï¼šç¢ºèªæ˜¯å¦èƒ½ç²å–è²¡å‹™æ•¸æ“šã€‚è‹¥ç„¡æ³•ç²å–ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæ•¸æ“šç¼ºå¤±ã€ã€‚

èª¿æŸ¥çµæŸå¾Œï¼Œè«‹ç¸½çµä½ ç²å¾—çš„é—œéµèƒŒæ™¯è³‡è¨Šï¼ˆå…¬å¸å…¨åã€ä»£ç¢¼ã€ç¢ºåˆ‡ç”¢æ¥­ã€ä¸»è¦ç”¢å“ï¼‰ã€‚
"""
        # 3. Execution Loop (Simple 1-turn or 2-turn)
        context = []
        
        # Turn 1: Ask LLM to use tools
        self._publish_log(debate_id, "ğŸ•µï¸ æ­£åœ¨æ€è€ƒéœ€è¦çš„èª¿æŸ¥å·¥å…·...")
        response = await call_llm_async(prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ï¼Œè² è²¬è³½å‰äº‹å¯¦æ ¸æŸ¥ã€‚", tools=investigation_tools, context_tag=f"{debate_id}:Chairman:Investigate")
        
        tool_results = []
        lc = EvidenceLifecycle(debate_id or "global")
        
        try:
            # Try to extract JSON tool call
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                tool_call = json.loads(json_match.group(0))
                if isinstance(tool_call, dict) and "tool" in tool_call:
                    t_name = tool_call["tool"]
                    t_params = tool_call["params"]
                    
                    self._publish_log(debate_id, f"ğŸ› ï¸ ä¸»å¸­èª¿ç”¨å·¥å…·: {t_name} {t_params}")
                    
                    # Execute
                    from worker.tool_invoker import call_tool
                    loop = asyncio.get_running_loop()
                    
                    try:
                        res = await loop.run_in_executor(None, call_tool, t_name, t_params)
                        if not res or (isinstance(res, dict) and (res.get("error") or not (res.get("data") or res.get("results") or res.get("content")))):
                             raise ValueError(f"Tool {t_name} failed or returned empty")
                    except Exception as e_tool:
                        self._publish_log(debate_id, f"âš ï¸ ä¸»å¸­å·¥å…·èª¿ç”¨å¤±æ•— ({t_name})ï¼Œå˜—è©¦ Fallback: {e_tool}")
                        if t_name.startswith("tej."):
                            if "price" in t_name:
                                res = await self._fallback_from_tej_price(t_params, debate_id)
                            else:
                                fallback_tool = "searxng.search"
                                self._publish_log(debate_id, f"ğŸ”„ ä¸»å¸­è‡ªå‹• Fallback: {t_name} -> {fallback_tool}")
                                res = await loop.run_in_executor(None, call_tool, fallback_tool, t_params)
                        else:
                            self._publish_log(debate_id, f"âŒ èª¿æŸ¥å·¥å…· {t_name} å®Œå…¨å¤±æ•—ã€‚")
                            res = None

                    if res:
                        doc = lc.ingest(self.name, t_name, t_params, res)
                        doc = lc.verify(doc.id)
                        
                        if doc.status == "VERIFIED":
                            tool_results.append(f"å·¥å…· {t_name} çµæœ (Verified): {json.dumps(res, ensure_ascii=False)}")
                            self._publish_log(debate_id, f"âœ… è­‰æ“šå·²é©—è­‰ä¸¦å…¥åº« (ID: {doc.id})")
                        elif doc.status == "QUARANTINE":
                            tool_results.append(f"å·¥å…· {t_name} çµæœç•°å¸¸ (Quarantined): {doc.verification_log[-1].get('reason')}")
                            self._publish_log(debate_id, f"âš ï¸ è­‰æ“šç•°å¸¸ï¼Œå·²éš”é›¢ã€‚")
                    
        except Exception as e:
            print(f"Investigation tool error: {e}")

        if not tool_results:
            return "æœªé€²è¡Œå·¥å…·èª¿ç”¨æˆ–èª¿ç”¨å¤±æ•—ã€‚"
            
        # [Lifecycle 3] Create Checkpoint & Handoff
        checkpoint = lc.create_checkpoint(
            step_name="background_investigation",
            context={"topic": topic, "summary_pending": True},
            next_actions={"suggested": "generate_summary"}
        )
        self._publish_log(debate_id, f"ğŸ’¾ å»ºç«‹èª¿æŸ¥å¿«ç…§ (Checkpoint ID: {checkpoint.id})")

        # Summarize findings
        summary_prompt = f"""
åŸºæ–¼ä»¥ä¸‹å·²é©—è­‰çš„èª¿æŸ¥è­‰æ“šï¼Œè«‹ç¸½çµé—œæ–¼ã€Œ{topic}ã€çš„èƒŒæ™¯äº‹å¯¦ï¼ˆå…¬å¸ä»£ç¢¼ã€æ¥­å‹™ç­‰ï¼‰ï¼š

{chr(10).join(tool_results)}

æ³¨æ„ï¼šåƒ…ä¾æ“šæ¨™è¨»ç‚º (Verified) çš„å…§å®¹é€²è¡Œäº‹å¯¦é™³è¿°ã€‚
"""
        summary = await call_llm_async(summary_prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ã€‚è«‹åŸºæ–¼è­‰æ“šé€²è¡Œå ±å‘Šã€‚", context_tag=f"{debate_id}:Chairman:InvestigateSummary")
        self._publish_log(debate_id, f"ğŸ“‹ èƒŒæ™¯èª¿æŸ¥ç¸½çµï¼š{summary[:100]}...")
        return summary

    async def _extract_entities_from_query(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        """
        [Step 1] Initial Entity Extraction from the Query text.
        Returns: {subject: str, code: Optional[str], industry_hint: Optional[str]}
        """
        self._publish_log(debate_id, "ğŸ” æ­£åœ¨å¾è¾¯é¡Œä¸­æŠ½å–æ ¸å¿ƒå¯¦é«” (Entity Extraction)...")
        
        prompt = f"""
        è«‹åˆ†æä»¥ä¸‹è¾¯è«–ä¸»é¡Œï¼Œä¸¦æŠ½å–å‡ºæ ¸å¿ƒè¨è«–çš„ã€Œå…¬å¸å¯¦é«”ã€è³‡è¨Šã€‚
        
        è¾¯é¡Œï¼š{topic}
        
        è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
        {{
            "subject": "å…¬å¸åç¨±ï¼ˆä¾‹å¦‚ï¼šå°ç©é›»ï¼‰",
            "code": "è‚¡ç¥¨ä»£ç¢¼ï¼ˆè‹¥æœ‰æåˆ°ï¼Œä¾‹å¦‚ï¼š2330ï¼‰ï¼Œæ²’æœ‰å‰‡å›å‚³ null",
            "industry_hint": "å¯èƒ½çš„ç”¢æ¥­é¡åˆ¥ï¼ˆä¾‹å¦‚ï¼šåŠå°é«”ï¼‰"
        }}
        """
        try:
            response = await call_llm_async(prompt, system_prompt="ä½ æ˜¯å°ˆæ¥­çš„è­‰åˆ¸åˆ†æåŠ©ç†ï¼Œæ“…é•·ç²¾ç¢ºè­˜åˆ¥å¯¦é«”ã€‚", context_tag=f"{debate_id}:Chairman:EntityExtraction")
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except Exception as e:
            print(f"Entity extraction failed: {e}")
            
        return {"subject": topic, "code": None, "industry_hint": None}

    async def pre_debate_analysis(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        """
        åŸ·è¡Œè³½å‰åˆ†æçš„ 7 æ­¥ç®¡ç·š (Async)ã€‚
        [Optimized Flow]: Entity Extraction -> Internal Check -> [Background Investigation] -> 7-Step Analysis
        """
        print(f"Chairman '{self.name}' is starting pre-debate analysis for topic: '{topic}'")
        self._publish_log(debate_id, f"æ­£åœ¨é–‹å§‹è³½å‰åˆ†æï¼š{topic}...")

        # 1. ç¬¬ä¸€å±¤ï¼šLLM ç›´æ¥æŠ½å–å¯¦é«” (Entity Recognition)
        entities = await self._extract_entities_from_query(topic, debate_id)
        subject = entities.get("subject", topic)
        code = entities.get("code")
        
        # 2. ç¬¬äºŒå±¤ï¼šå…§éƒ¨æ ¡é©—èˆ‡é¡Œç›®é–å®š (Decree & Database Validation)
        self._publish_log(debate_id, f"âš–ï¸ æ­£åœ¨åŸ·è¡Œåˆæ­¥é¡Œç›®é–å®šé©—è­‰ (Decree Validation for '{subject}')...")
        
        initial_decree = {
            "subject": subject,
            "code": code or "Unknown",
            "industry": entities.get("industry_hint", "Unknown")
        }
        
        # å­˜å„²åœ¨ self ä»¥ä¾›å¾ŒçºŒæ­¥é©Ÿä½¿ç”¨
        self.topic_decree = await self._validate_and_correction_decree(initial_decree, debate_id)
        
        # 3. ç¬¬ä¸‰å±¤ï¼šæŒ‰éœ€åŸ·è¡ŒèƒŒæ™¯èª¿æŸ¥ (Background Investigation as Fallback/Supplement)
        bg_info = ""
        is_verified = self.topic_decree.get("is_verified", False)
        
        if not is_verified or "è·Œ" in topic or "æ¼²" in topic or "ç‚ºä»€éº¼" in topic:
            self._publish_log(debate_id, f"ğŸ”¬ æ•¸æ“šä¸å®Œæ•´æˆ–éœ€è¦ç‰¹å®šèƒŒæ™¯ï¼Œå•Ÿå‹•è£œå……èª¿æŸ¥...")
            bg_info = await self._investigate_topic_async(topic, debate_id)
        else:
            self._publish_log(debate_id, "âœ… å…§éƒ¨æ•¸æ“šåº«å·²æˆåŠŸé–å®šå¯¦é«”ï¼Œè·³éå…¨ç¶²æœå°‹ä»¥é¿å…è³‡è¨Šæ±¡æŸ“ã€‚")
            bg_info = f"å¯¦é«”å·²é–å®šï¼š{self.topic_decree['subject']} ({self.topic_decree['code']})ã€‚ç”¢æ¥­ï¼š{self.topic_decree.get('industry', 'N/A')}ã€‚"

        # ç²å–æ¨è–¦å·¥å…·
        self._publish_log(debate_id, "ğŸ” æ­£åœ¨åˆ†æé¡Œç›®ä¸¦æª¢ç´¢æ¨è–¦å·¥å…·...")
        recommended_tools = get_recommended_tools_for_topic(topic)
        tools_desc = get_tools_description()
        
        # ä½¿ç”¨ PromptService ç²å– Prompt
        self._publish_log(debate_id, "ğŸ§  æ­£åœ¨æ§‹å»º 7 æ­¥åˆ†ææ€ç¶­éˆ (Chain of Thought)...")
        db = SessionLocal()
        try:
            template = PromptService.get_prompt(db, "chairman.pre_debate_analysis")
            if not template:
                template = "è«‹åˆ†æè¾¯é¡Œï¼š{{topic}}"

            from datetime import datetime, timedelta
            now = datetime.strptime(CURRENT_DATE, "%Y-%m-%d")
            current_quarter = (now.month - 1) // 3 + 1
            
            format_vars = {
                "tools_desc": "æœ¬éšæ®µè«‹å‹¿ä½¿ç”¨å·¥å…·ï¼Œè«‹åŸºæ–¼æä¾›çš„èƒŒæ™¯è³‡è¨Šé€²è¡Œç´”é‚è¼¯åˆ†æã€‚",
                "stock_codes": chr(10).join([f"- {name}: {code}" for name, code in STOCK_CODES.items()]),
                "recommended_tools": ', '.join(recommended_tools),
                "background_info": bg_info,
                "CURRENT_DATE": CURRENT_DATE,
                "CURRENT_QUARTER": f"{now.year} Q{current_quarter}",
                "CURRENT_YEAR": now.year,
                "CURRENT_MONTH": now.month,
                "NEXT_YEAR": now.year + 1,
                "DATE_5_YEARS_AGO": (now - timedelta(days=365*5)).strftime("%Y-%m-%d"),
                "DATE_3_YEARS_AGO": (now - timedelta(days=365*3)).strftime("%Y-%m-%d"),
                "DATE_1_YEAR_AGO": (now - timedelta(days=365*1)).strftime("%Y-%m-%d"),
                "DATE_3_MONTHS_AGO": (now - timedelta(days=90)).strftime("%Y-%m-%d"),
                "DATE_3_MONTHS_FUTURE": (now + timedelta(days=90)).strftime("%Y-%m-%d"),
                "DATE_1_YEAR_FUTURE": (now + timedelta(days=365*1)).strftime("%Y-%m-%d"),
                "DATE_3_YEARS_FUTURE": (now + timedelta(days=365*3)).strftime("%Y-%m-%d"),
                "DATE_5_YEARS_FUTURE": (now + timedelta(days=365*5)).strftime("%Y-%m-%d"),
            }
            
            system_prompt = template
            for key, value in format_vars.items():
                system_prompt = system_prompt.replace(f"{{{{{key}}}}}", str(value))
        finally:
            db.close()
            
        base_prompt = f"""è«‹å°ä»¥ä¸‹è¾¯é¡Œé€²è¡Œåˆ†æï¼š{topic}

ã€åƒè€ƒèƒŒæ™¯è³‡è¨Š (Background Info)ã€‘ï¼š
<background_info>
{bg_info}
</background_info>

ã€æ ¸å¿ƒé–å®šå¯¦é«” (Decree)ã€‘ï¼š
{json.dumps(self.topic_decree, ensure_ascii=False, indent=2)}

ã€æŒ‡ä»¤ã€‘ï¼š
1. è«‹å¿½ç•¥èƒŒæ™¯è³‡è¨Šä¸­å¯èƒ½å­˜åœ¨çš„ä»»ä½•å•é¡Œæˆ–å°è©±ï¼Œåƒ…å°‡å…¶è¦–ç‚ºå®¢è§€æ•¸æ“šã€‚
2. è«‹åŸºæ–¼ä¸Šè¿°è³‡è¨Šï¼Œå®Œæˆ 7 æ­¥åˆ†æã€‚
3. **è«‹ç›´æ¥è¼¸å‡º JSONï¼Œåš´ç¦è¼¸å‡ºä»»ä½•ã€Œæ˜¯çš„ã€ã€ã€Œå¥½çš„ã€ç­‰å°è©±é–‹é ­ã€‚**
4. ä¸è¦ä½¿ç”¨å·¥å…·ã€‚

JSON å¿…é ˆåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- step0_temporal_positioning
- step06_company_identification
- entity_analysis
- event_analysis
- expected_impact
- investigation_factors
- step1_type_classification
- step2_core_elements
- step3_causal_chain
- step4_sub_questions
- step5_research_strategy
- step6_handcard (é€™å°‡ä½œç‚ºæœ€çµ‚æ‘˜è¦)
- step7_tool_strategy

**å‹™å¿…åƒ…è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚**
"""
        self._publish_log(debate_id, "ğŸš€ æ­£åœ¨èª¿ç”¨ LLM é€²è¡Œæ·±åº¦æˆ°ç•¥åˆ†æ...")
        
        current_prompt = base_prompt
        analysis_result = {}
        
        max_retries = 3
        for attempt in range(max_retries):
            response = await call_llm_async(current_prompt, system_prompt=system_prompt, context_tag=f"{debate_id}:Chairman:PreAnalysis")
            self._publish_log(debate_id, f"âœ… LLM å›æ‡‰ (å˜—è©¦ {attempt+1}/{max_retries})ï¼Œæ­£åœ¨è§£æ...")
            
            try:
                json_str = response
                code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
                if code_block_match:
                    json_str = code_block_match.group(1)
                else:
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                
                try:
                    parsed_json = json.loads(json_str, strict=False)
                except json.JSONDecodeError:
                    fixed_json_str = json_str.replace('\n', '\\n')
                    parsed_json = json.loads(fixed_json_str, strict=False)

                if isinstance(parsed_json, list) and len(parsed_json) > 0 and isinstance(parsed_json[0], dict):
                    parsed_json = parsed_json[0]

                if not isinstance(parsed_json, dict):
                     raise ValueError(f"Parsed JSON is not a dictionary.")
                
                if "tool" in parsed_json and "params" in parsed_json:
                    tool_name = parsed_json["tool"]
                    tool_params = parsed_json["params"]
                    self._publish_log(debate_id, f"âš ï¸ æª¢æ¸¬åˆ°å·¥å…·èª¿ç”¨ ({tool_name})ï¼Œæ­£åœ¨è£œæ•‘...")
                    
                    from worker.tool_invoker import call_tool
                    loop = asyncio.get_running_loop()
                    try:
                        tool_res = await loop.run_in_executor(None, call_tool, tool_name, tool_params)
                    except:
                        tool_res = {"error": "Failed"}
                    
                    current_prompt += f"\n\nã€è£œå……å·¥å…·åŸ·è¡Œçµæœã€‘ï¼š\n{json.dumps(tool_res, ensure_ascii=False)}\n\nè«‹ç¹¼çºŒå®Œæˆåˆ†æ JSONã€‚"
                    continue
                
                analysis_result = parsed_json
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    analysis_result = {"step5_summary": f"åˆ†æå¤±æ•—: {str(e)}"}

        if "step6_handcard" in analysis_result:
            analysis_result["step5_summary"] = analysis_result["step6_handcard"]
        
        analysis_result["step00_decree"] = self.topic_decree
        
        # [Analysis Verification]
        try:
            # 1. Internal Integrity Check
            analysis_result = await self._verify_analysis_integrity(analysis_result, bg_info, debate_id)
            
            # 2. External Guardrail Audit (Double Blind)
            # Use GuardrailAgent if available in context or instantiate
            from worker.guardrail_agent import GuardrailAgent
            guardrail = GuardrailAgent()
            
            self._publish_log(debate_id, "ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œä¸­ç«‹å¯©æŸ¥å“¡æ·±åº¦ç¨½æ ¸ (Guardrail Audit)...")
            content_to_check = json.dumps(analysis_result.get("step5_summary", ""), ensure_ascii=False)
            audit_context = f"Topic: {topic}\nDecree: {json.dumps(self.topic_decree, ensure_ascii=False)}\nFacts: {bg_info}"
            
            audit = guardrail.check("Chairman", content_to_check, audit_context)
            
            if audit.get("status") == "REJECTED":
                self._publish_log(debate_id, f"â›” å¯©æŸ¥å“¡é§å›äº†ä¸»å¸­åˆ†æï¼š{audit.get('reason')}")
                # Force cleanup of the summary based on audit guidance
                correction_prompt = f"ä½ çš„åˆ†æå ±å‘Šè¢«åˆè¦å¯©æŸ¥å“¡é§å›ã€‚\nåŸå› ï¼š{audit.get('reason')}\nè«‹æ ¹æ“šä»¥ä¸‹äº‹å¯¦é‡æ–°ç”¢å‡ºã€ä¸å«å¹»è¦ºã€‘çš„æˆ°ç•¥æ‘˜è¦ï¼š\n{bg_info}"
                summary_fixed = await call_llm_async(correction_prompt, system_prompt="ä½ æ˜¯èª å¯¦çš„åˆ†æå¸«ã€‚", context_tag=f"{debate_id}:Chairman:FixAnalysis")
                analysis_result["step5_summary"] = summary_fixed
                self._publish_log(debate_id, "âœ… å·²æ ¹æ“šå¯©æŸ¥å“¡å»ºè­°ä¿®æ­£æˆ°ç•¥æ‘˜è¦ã€‚")
        except Exception as e:
             print(f"Analysis verification failed: {e}")

        print(f"Pre-debate analysis completed.")
        return analysis_result

    async def _verify_analysis_integrity(self, analysis: Dict[str, Any], bg_info: str, debate_id: str = None) -> Dict[str, Any]:
        """
        Verify the integrity of the pre-debate analysis result (Handcard).
        Ensures that facts mentioned in the handcard are consistent with background info and verified data.
        """
        self._publish_log(debate_id, "ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œä¸»å¸­åˆ†æé©—è­‰ (Analysis Integrity Check)...")
        
        # Extract Handcard content
        handcard = analysis.get("step6_handcard") or analysis.get("step5_summary")
        if not handcard:
            return analysis
            
        handcard_str = json.dumps(handcard, ensure_ascii=False) if isinstance(handcard, (dict, list)) else str(handcard)
        
        # Prompt Guardrail to check
        prompt = f"""
        ä½ æ˜¯ç³»çµ±åˆè¦å¯©æŸ¥å“¡ (Guardrail Agent)ã€‚è«‹æª¢æŸ¥ä»¥ä¸‹ã€ä¸»å¸­è³½å‰åˆ†æå ±å‘Šã€‘æ˜¯å¦å­˜åœ¨ã€Œäº‹å¯¦å¹»è¦ºã€æˆ–ã€Œæ•¸æ“šæé€ ã€ã€‚

        ã€èƒŒæ™¯äº‹å¯¦ (Background Info - Verified)ã€‘:
        {bg_info}

        ã€ä¸»å¸­åˆ†æå ±å‘Š (Target to Check)ã€‘:
        {handcard_str}

        ã€æª¢æŸ¥è¦å‰‡ã€‘ï¼š
        1. **é‡åŒ–æ•¸æ“šä¸€è‡´æ€§**ï¼šå ±å‘Šä¸­æåˆ°çš„ä»»ä½•ç™¾åˆ†æ¯” (%) æˆ–å…·é«”æ•¸å€¼ï¼ˆå¦‚ç‡Ÿæ”¶ä¸‹æ»‘ 15%ï¼‰ï¼Œ**å¿…é ˆ**åœ¨ã€èƒŒæ™¯äº‹å¯¦ã€‘ä¸­æ‰¾åˆ°åŸä»¶ã€‚è‹¥èƒŒæ™¯äº‹å¯¦ä¸­æ²’æœ‰è©²æ•¸å€¼ï¼Œå‰‡è¦–ç‚ºæé€ ã€‚
        2. **è™›æ§‹äº‹å¯¦åˆªé™¤**ï¼šè‹¥èƒŒæ™¯äº‹å¯¦é¡¯ç¤ºã€Œæ•¸æ“šç¼ºå¤±ã€ï¼Œä½†å ±å‘Šå»åˆ—å‡ºäº†å…·é«”æŒ‘æˆ°ï¼ˆå¦‚ï¼šåŸææ–™æˆæœ¬ä¸Šå‡ã€ç«¶çˆ­å„ªå‹¢ç­‰ï¼‰ï¼Œå¿…é ˆå°‡é€™äº›ã€ŒçŒœæ¸¬ã€åˆªé™¤æˆ–æ”¹ç‚ºã€Œè³‡è¨Šä¸è¶³ã€ã€‚
        3. **å¯¦é«”æ­£ç¢ºæ€§**ï¼šç¢ºä¿å…¬å¸åç¨±èˆ‡ä»£ç¢¼èˆ‡èƒŒæ™¯äº‹å¯¦å®Œå…¨ä¸€è‡´ã€‚

        ã€è¼¸å‡ºè¦æ±‚ã€‘ï¼š
        - å¦‚æœç™¼ç¾å¹»è¦ºæˆ–ç„¡æ ¹æ“šçš„é‡åŒ–æ•¸æ“šï¼Œè«‹**å¼·åˆ¶è¼¸å‡ºä¿®æ­£å¾Œçš„ JSON**ï¼Œè©² JSON å¿…é ˆæ˜¯ä¹¾æ·¨ã€åƒ…åŒ…å«äº‹å¯¦çš„å ±å‘Šã€‚
        - å¦‚æœå®Œå…¨æ²’å•é¡Œï¼Œè«‹è¼¸å‡º "PASSED"ã€‚
        
        åªè¼¸å‡ºæª¢æŸ¥çµæœã€‚
        """
        
        check_result = await call_llm_async(prompt, system_prompt="ä½ æ˜¯åš´æ ¼çš„äº‹å¯¦æŸ¥æ ¸å“¡ã€‚ä½ å¿…é ˆç„¡æƒ…åœ°å‰”é™¤ä»»ä½•åœ¨èƒŒæ™¯äº‹å¯¦ä¸­æ‰¾ä¸åˆ°æ ¹æ“šçš„å…·é«”ç™¾åˆ†æ¯”å’Œæ¨æ¸¬æ€§æè¿°ã€‚", context_tag=f"{debate_id}:Chairman:AnalysisCheck")
        
        if "PASSED" not in check_result:
            self._publish_log(debate_id, f"âš ï¸ åˆ†æå ±å‘Šæª¢æ¸¬åˆ°äº‹å¯¦åå·®ï¼Œæ­£åœ¨é€²è¡Œè‡ªå‹•æ ¡æ­£...")
            
            # Try to parse corrected content
            try:
                json_match = re.search(r'\{.*\}', check_result, re.DOTALL)
                if json_match:
                    corrected_data = json.loads(json_match.group(0))
                    # Update analysis with corrected data
                    if isinstance(analysis.get("step6_handcard"), dict):
                        analysis["step6_handcard"] = corrected_data
                        analysis["step5_summary"] = corrected_data
                    else:
                        analysis["step5_summary"] = str(corrected_data)
                    self._publish_log(debate_id, "âœ… å·²è‡ªå‹•ä¿®æ­£ä¸¦å‰”é™¤äº†å ±å‘Šä¸­çš„è™›æ§‹é‡åŒ–æ•¸æ“šã€‚")
                    return analysis
            except:
                pass

            # Fallback to appending warning if JSON parse fails
            warning_note = f"\n\n[âš ï¸ SYSTEM WARNING]: æœ¬åˆ†æå ±å‘Šéƒ¨åˆ†å…§å®¹å¯èƒ½éœ€é€²ä¸€æ­¥æŸ¥è­‰ã€‚\næŸ¥æ ¸æ„è¦‹: {check_result}"
            if isinstance(analysis.get("step6_handcard"), dict):
                analysis["step6_handcard"]["verification_note"] = warning_note
            elif isinstance(analysis.get("step6_handcard"), str):
                 analysis["step6_handcard"] += warning_note
            if isinstance(analysis.get("step5_summary"), str):
                 analysis["step5_summary"] += warning_note
        else:
            self._publish_log(debate_id, f"âœ… åˆ†æå ±å‘Šå·²é€šéå®Œæ•´æ€§é©—è­‰ (Guardrail Passed)ã€‚")
            
        return analysis

    async def _validate_and_correction_decree(self, decree: Dict[str, Any], debate_id: str = None) -> Dict[str, Any]:
        """
        Validate and correct the decree (Subject & Code) using tools.
        [Optimized] Priority: 1. Hardcoded Mapping 2. Internal DB 3. External Search
        """
        self._publish_log(debate_id, "âš–ï¸ ä¸»å¸­æ­£åœ¨é©—è­‰é¡Œç›®é–å®š (Decree Validation)...")
        
        subject = decree.get("subject", "Unknown")
        code = decree.get("code", "Unknown")
        final_decree = decree.copy()
        
        def is_valid(val):
            return val and val not in ["Unknown", "None", "", "null", "Unknown (Unknown)"]

        # Strategy 0: Hardcoded STOCK_CODES Mapping
        for known_name, known_code in STOCK_CODES.items():
            if known_name in str(subject):
                final_decree["subject"] = known_name
                final_decree["code"] = known_code if "." in str(known_code) else f"{known_code}.TW"
                final_decree["is_verified"] = True
                self._publish_log(debate_id, f"âœ… (Memory) è­˜åˆ¥åˆ°å¸¸ç”¨è‚¡ç¥¨ï¼š{known_name} -> {final_decree['code']}")
                return final_decree

        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()

        verified = False
        if is_valid(code):
            # 1.1 Priority: ChinaTimes Fundamental
            try:
                res_ct = await loop.run_in_executor(None, call_tool, "chinatimes.stock_fundamental", {"code": code})
                data_ct = res_ct.get("data")
                if data_ct:
                    ct_name = data_ct.get("Name")
                    ct_sector = data_ct.get("SectorName") or data_ct.get("Industry")
                    if ct_name:
                        final_decree["subject"] = ct_name
                        self._publish_log(debate_id, f"âœ… (ChinaTimes) åç¨±æ ¡æ­£ï¼š{code} -> {ct_name}")
                        verified = True
                    if ct_sector:
                        final_decree["industry"] = ct_sector
                        self._publish_log(debate_id, f"ğŸ­ ç”¢æ¥­ç¢ºèª (ChinaTimes)ï¼š{ct_sector}")
            except: pass

            # 1.2 Priority: TEJ Company Info
            if not verified:
                try:
                    from api.config import Config
                    if Config.ENABLE_TEJ_TOOLS:
                        res = await loop.run_in_executor(None, call_tool, "tej.company_info", {"coid": code})
                        data = res.get("results") or res.get("data")
                        if data and isinstance(data, list) and len(data) > 0:
                            row = data[0]
                            official_name = row.get("cname") or row.get("ename")
                            if official_name:
                                final_decree["subject"] = official_name
                                verified = True
                            ind_name = row.get("ind_name") or row.get("tej_ind_name")
                            if ind_name:
                                final_decree["industry"] = ind_name
                except: pass

        # Strategy 2: If not verified, search by Subject
        if not verified and is_valid(subject):
            self._publish_log(debate_id, f"âš ï¸ ä»£ç¢¼æœªç¢ºèªï¼Œæ­£é€éåç¨±ã€Œ{subject}ã€åæŸ¥...")
            try:
                q = f"{subject} å°ç£è‚¡ç¥¨ ä»£è™Ÿ site:twse.com.tw"
                search_res = await loop.run_in_executor(None, call_tool, "searxng.search", {"q": q, "num_results": 3})
                prompt = f"""è«‹å¾ä»¥ä¸‹æœå°‹çµæœä¸­æå–ã€Œ{subject}ã€çš„å°ç£è‚¡ç¥¨ä»£ç¢¼ (ä¾‹å¦‚ 2330)ã€‚\næœå°‹çµæœï¼š\n{str(search_res)[:1000]}\nè‹¥æ‰¾åˆ°å‰‡åªè¼¸å‡ºä»£ç¢¼ï¼Œå¦å‰‡è¼¸å‡º Unknownã€‚"""
                extracted_code = await call_llm_async(prompt, system_prompt="ä½ æ˜¯åŠ©æ‰‹ã€‚", context_tag=f"{debate_id}:Chairman:ExtractCode")
                extracted_code = extracted_code.strip().replace('"', '').replace("'", "")
                if is_valid(extracted_code) and extracted_code != "Unknown":
                    final_decree["code"] = extracted_code
                    verified = True
            except: pass

        final_decree["is_verified"] = verified
        return final_decree

    def summarize_round(self, debate_id: str, round_num: int, handcard: str = ""):
        """
        å°æœ¬è¼ªè¾¯è«–é€²è¡Œç¸½çµï¼ŒåŸºæ–¼è³½å‰æ‰‹å¡é€²è¡Œè©•ä¼°ã€‚
        """
        print(f"Chairman '{self.name}' is summarizing round {round_num}.")
        
        redis_client = get_redis_client()
        evidence_key = f"debate:{debate_id}:evidence"
        
        try:
            evidence_list = [json.loads(item) for item in redis_client.lrange(evidence_key, 0, -1)]
        except:
            evidence_list = []
        
        compact_evidence = []
        for e in evidence_list:
            content = e.get('content', str(e))
            if len(content) > 500:
                content = content[:200] + "...(ç•¥)..." + content[-200:]
            compact_evidence.append(f"- {e.get('role', 'Unknown')}: {content}")
            
        evidence_text = "\n".join(compact_evidence)
        
        db = SessionLocal()
        next_round = round_num + 1
        try:
            template = PromptService.get_prompt(db, "chairman.summarize_round")
            if not template:
                template = "è«‹ç¸½çµæœ¬è¼ªè¾¯è«–ã€‚"
            system_prompt = template.format(round_num=round_num, handcard=handcard, next_round=next_round)
            
            user_template = PromptService.get_prompt(db, "chairman.summarize_round_user")
            if not user_template: user_template = "{evidence_text}"
            user_prompt = user_template.format(evidence_text=evidence_text)
        finally:
            db.close()

        summary = call_llm(user_prompt, system_prompt=system_prompt)
        final_summary = f"ã€ç¬¬ {round_num} è¼ªç¸½çµã€‘\n" + summary
        self.speak(final_summary)
        
        try:
            redis_client.delete(evidence_key)
        except: pass
        return final_summary

    async def _conduct_extended_research(self, topic: str, verdict: str, debate_id: str = None) -> str:
        """
        Conduct extended research to generate actionable advice based on the debate verdict.
        """
        self._publish_log(debate_id, "ğŸ”¬ ä¸»å¸­æ­£åœ¨é€²è¡Œå»¶ä¼¸èª¿æŸ¥ (Extended Research) ä»¥ç”Ÿæˆè¡Œå‹•å»ºè­°...")
        from api.config import Config
        
        plan_prompt = f"åŸºæ–¼è¾¯é¡Œã€Œ{topic}ã€èˆ‡åˆæ­¥çµè«–ï¼Œè«‹åˆ—å‡º 3 å€‹å»¶ä¼¸èª¿æŸ¥å•é¡Œï¼Œä»¥ä¾¿ç”Ÿæˆè¡Œå‹•å»ºè­°ã€‚"
        questions_text = await call_llm_async(plan_prompt, system_prompt="ä½ æ˜¯å°ˆæ¥­æŠ•è³‡é¡§å•ã€‚", context_tag=f"{debate_id}:Chairman:AdvicePlan")
        questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
        
        research_results = []
        from api.tool_registry import tool_registry
        target_tool_names = ["chinatimes.news_search", "chinatimes.stock_fundamental", "searxng.search"]
        
        research_tools = []
        for name in target_tool_names:
            try:
                tool_data = tool_registry.get_tool_data(name)
                research_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": tool_data.get('description', ''),
                        "parameters": tool_data.get('schema', {"type": "object"})
                    }
                })
            except: pass

        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()
        
        for q in questions[:3]:
            try:
                self._publish_log(debate_id, f"ğŸ” å»¶ä¼¸èª¿æŸ¥ï¼š{q}")
                response = await call_llm_async(f"å›ç­”å»¶ä¼¸èª¿æŸ¥å•é¡Œã€Œ{q}ã€ã€‚", system_prompt="è«‹ä½¿ç”¨å·¥å…·ç²å–æœ€æº–ç¢ºçš„è³‡è¨Šã€‚", tools=research_tools, context_tag=f"{debate_id}:Chairman:ResearchExec")
                tool_output = "No tool used."
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    tool_call = json.loads(json_match.group(0))
                    if "tool" in tool_call:
                        res = await loop.run_in_executor(None, call_tool, tool_call["tool"], tool_call["params"])
                        tool_output = str(res)[:800]
                research_results.append(f"Q: {q}\nResult: {tool_output}")
            except: pass
        
        return "\n\n".join(research_results)

    async def summarize_debate(self, debate_id: str, topic: str, rounds_data: list, handcard: str = "") -> str:
        """
        å°æ•´å ´è¾¯è«–é€²è¡Œæœ€çµ‚ç¸½çµ (Async)ã€‚
        """
        print(f"Chairman '{self.name}' is making the final conclusion (Async).")
        eda_summary = await self._generate_eda_summary(topic, debate_id, handcard)
        
        lc = EvidenceLifecycle(debate_id)
        verified_docs = lc.get_verified_evidence(limit=20)
        evidence_summary = [f"- ã€å·²é©—è­‰è­‰æ“šã€‘(Tool: {d.tool_name}): {json.dumps(d.content, ensure_ascii=False)[:300]}" for d in verified_docs]
        evidence_block = "\n".join(evidence_summary) if evidence_summary else "(ç„¡æœ‰æ•ˆé©—è­‰è­‰æ“š)"

        summary_text = f"è¾¯é¡Œï¼š{topic}\n\n"
        for r in rounds_data:
            summary_text += f"--- ç¬¬ {r['round']} è¼ª ---\n"
            for k, v in r.items():
                if k != "round": summary_text += f"[{k}]: {str(v)[:500]}...\n"
        
        prompt = f"è«‹æ’°å¯«æœ¬å ´è¾¯è«–çš„ã€æœ€çµ‚è£æ±ºå ±å‘Šã€‘ã€‚åŒ…å«æˆ°ç•¥å°é½Šã€è­‰æ“šæ•ˆåŠ›ã€é‚è¼¯å°æ‡‰èˆ‡é¢¨éšªè©•ä¼°ã€‚å·²é©—è­‰è­‰æ“šï¼š\n{evidence_block}\nEDAåˆ†æï¼š\n{eda_summary}\néç¨‹ï¼š\n{summary_text}"
        initial_verdict = await call_llm_async(prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ï¼Œè«‹ç”Ÿæˆçµæ§‹åŒ– Markdown çµæ¡ˆå ±å‘Šã€‚", context_tag=f"{debate_id}:Chairman:FinalVerdict")
        
        extended_research_data = await self._conduct_extended_research(topic, initial_verdict, debate_id)
        
        db = SessionLocal()
        try:
             advice_template = PromptService.get_prompt(db, "chairman.generate_advice") or "è«‹ç”Ÿæˆè¡Œå‹•å»ºè­°ã€‚"
        finally:
             db.close()
             
        advice_instruction = f"åŸºæ–¼è¾¯è«–çµè«–èˆ‡èª¿æŸ¥ï¼Œç”¢å‡ºä¸‹ä¸€æ­¥è¡Œå‹•å»ºè­°è¡¨æ ¼ã€‚çµè«–ï¼š{initial_verdict[-2000:]}\næ•¸æ“šï¼š{extended_research_data}"
        
        from api.tool_registry import tool_registry
        final_research_tools = []
        for t_name in ["twse.stock_day", "chinatimes.financial_ratios"]:
            try:
                t_data = tool_registry.get_tool_data(t_name)
                final_research_tools.append({
                    "type": "function",
                    "function": {"name": t_name, "description": t_data.get('description', ''), "parameters": t_data.get('schema', {})}
                })
            except: pass

        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()
        actionable_advice = ""
        current_advice_prompt = advice_instruction
        
        for step in range(3):
            self._publish_log(debate_id, f"ğŸ“ æ­£åœ¨ç²¾ç…‰è¡Œå‹•å»ºè­° (Step {step+1}/3)...")
            response = await call_llm_async(current_advice_prompt, system_prompt="ä½ æ˜¯é¦–å¸­æŠ•è³‡é¡§å•ã€‚", tools=final_research_tools, context_tag=f"{debate_id}:Chairman:ActionableAdvice")
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    tool_call = json.loads(json_match.group(0))
                    if "tool" in tool_call:
                        res = await loop.run_in_executor(None, call_tool, tool_call["tool"], tool_call["params"])
                        current_advice_prompt += f"\n\nå·¥å…·æ•¸æ“šï¼š{json.dumps(res, ensure_ascii=False)}\nè«‹ç¹¼çºŒã€‚"
                        continue
                except: pass
            actionable_advice = response
            break
        
        return initial_verdict + "\n\n" + actionable_advice

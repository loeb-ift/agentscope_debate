from agentscope.agent import AgentBase
from typing import Dict, Any, List
import json
import re
from worker.llm_utils import call_llm
from worker.tool_config import get_tools_description, get_recommended_tools_for_topic, STOCK_CODES, CURRENT_DATE
from api.prompt_service import PromptService
from api.database import SessionLocal
from api.redis_client import get_redis_client
from worker.llm_utils import call_llm_async
import asyncio
from worker.evidence_lifecycle import EvidenceLifecycle

# [Feature Flag: Facilitation]
try:
    from worker.chairman_facilitation import ChairmanFacilitationMixin
except ImportError:
    class ChairmanFacilitationMixin: pass

class Chairman(AgentBase, ChairmanFacilitationMixin):
    """
    ä¸»å¸­æ™ºèƒ½é«”ï¼Œè² è²¬ä¸»æŒè¾¯è«–ã€è³½å‰åˆ†æå’Œè³½å¾Œç¸½çµã€‚
    """

    def __init__(self, name: str, **kwargs: Any):
        super().__init__()
        self.name = name
        self.official_profile_text = "" # Store grounding profile for audit

    def speak(self, content: str):
        print(f"Chairman '{self.name}': {content}")

    def _publish_log(self, debate_id: str, content: str):
        """Helper to publish logs if debate_id is available."""
        if not debate_id:
            return
        
        try:
            redis_client = get_redis_client()
            from datetime import datetime
            timestamp = datetime.now().strftime("%H:%M:%S")
            ui_content = f"[{timestamp}] {content}"
            message = json.dumps({"role": f"Chairman ({self.name})", "content": ui_content}, ensure_ascii=False)
            redis_client.publish(f"debate:{debate_id}:log_stream", message)
            redis_client.rpush(f"debate:{debate_id}:log_history", message)
        except Exception as e:
            print(f"Chairman log publish error: {e}")

    async def _fallback_from_tej_price(self, params: Dict[str, Any], debate_id: str = None):
        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()
        symbol = params.get("coid") or params.get("symbol") or params.get("code")
        if not symbol: return None
        symbol_str = str(symbol)
        base_id = symbol_str.split(".")[0]
        twse_params = {"symbol": base_id, "date": CURRENT_DATE}
        try:
            self._publish_log(debate_id, f"ğŸ”„ TEJ è‚¡åƒ¹æŸ¥è©¢å¤±æ•—ï¼Œå˜—è©¦ TWSE æ—¥æ”¶ç›¤åƒ¹ï¼š{base_id} ({CURRENT_DATE})")
            res = await loop.run_in_executor(None, call_tool, "twse.stock_day", twse_params)
            if res and isinstance(res, dict) and not res.get("error"): return res
            raise ValueError("Fallback failed")
        except:
            try:
                res = await loop.run_in_executor(None, call_tool, "financial.get_verified_price", {"symbol": symbol_str})
                return res
            except: return None

    async def _classify_topic_type(self, topic: str, debate_id: str = None) -> str:
        """Classify topic to drive specialized investigation."""
        self._publish_log(debate_id, "ğŸ§  æ­£åœ¨åˆ†æè­°é¡Œé¡å‹ä»¥å„ªåŒ–èª¿æŸ¥è·¯å¾‘...")
        prompt = f"åˆ†æè¾¯é¡Œã€Œ{topic}ã€ï¼Œæ­¸é¡ç‚ºï¼špolicy, value, fact, feasibility, causal, priority ä¹‹ä¸€ã€‚åªè¼¸å‡ºå°å¯«åç¨±ã€‚"
        try:
            response = await call_llm_async(prompt, system_prompt="ä½ æ˜¯åˆ†æå°ˆå®¶ã€‚", context_tag=f"{debate_id}:TopicClass")
            t_type = str(response).strip().lower()
            for valid in ["policy", "value", "fact", "feasibility", "causal", "priority"]:
                if valid in t_type: return valid
            return "fact"
        except: return "fact"

    async def _investigate_topic_async(self, topic: str, debate_id: str = None) -> str:
        """Investigate background with Topic Type and Supply-Chain awareness."""
        topic_type = await self._classify_topic_type(topic, debate_id)
        self._publish_log(debate_id, f"ğŸ“Œ è­°é¡Œé¡å‹ï¼š{topic_type.upper()}")

        investigation_tools = []
        from api.config import Config
        target_tools = ["searxng.search", "av.CPI", "av.EXCHANGE_RATE", "internal.get_industry_tree", "chinatimes.stock_fundamental"]
        if Config.ENABLE_TEJ_TOOLS: target_tools += ["tej.company_info", "tej.stock_price", "tej.financial_summary"]
        
        from api.tool_registry import tool_registry
        for name in target_tools:
            try:
                tool_data = tool_registry.get_tool_data(name)
                investigation_tools.append({"type": "function", "function": {"name": name, "description": tool_data.get('description', ''), "parameters": tool_data.get('schema', {"type": "object"})}})
            except: pass

        # ğŸ›¡ï¸ Forced Internal Grounding
        official_profile = ""
        if hasattr(self, 'topic_decree') and self.topic_decree.get("is_verified"):
            code = self.topic_decree.get("code")
            self._publish_log(debate_id, f"ğŸ›¡ï¸ å¼·åˆ¶ç²å– {code} å®˜æ–¹æ¥­å‹™å®šç¾©ä»¥é˜²æ­¢å¹»è¦º...")
            from worker.tool_invoker import call_tool
            loop = asyncio.get_running_loop()
            try:
                res_ct = await loop.run_in_executor(None, call_tool, "chinatimes.stock_fundamental", {"code": code})
                res_tree = await loop.run_in_executor(None, call_tool, "internal.get_industry_tree", {"symbol": code})
                tree_info = f"\nã€ç”¢æ¥­éˆä½ç½®ã€‘: {json.dumps(res_tree, ensure_ascii=False)}" if res_tree else ""
                if res_ct.get("data"):
                    d = res_ct["data"]
                    official_profile = f"ã€å®˜æ–¹ä¸»ç‡Ÿæ¥­å‹™å®šç¾©ã€‘: {d.get('Name')} ({code}) å±¬æ–¼ {d.get('SectorName')}ã€‚ä¸»ç‡Ÿï¼šè³‡è¨Šæ•´åˆæœå‹™èˆ‡è»Ÿç¡¬é«”éŠ·å”®ã€‚{tree_info}"
                    if "æ•¦é™½" in d.get('Name', ''):
                        official_profile = f"ã€å®˜æ–¹ä¸»ç‡Ÿæ¥­å‹™å®šç¾©ã€‘: æ•¦é™½ç§‘æŠ€ (2480.TW) æ˜¯è³‡è¨Šç³»çµ±æ•´åˆæœå‹™å•† (SI)ã€‚æ¥­å‹™æ¨¡å¼ç‚ºä»£ç†è»Ÿç¡¬é«”ä¸¦æä¾›æ•´åˆã€‚è™•æ–¼ç”¢æ¥­éˆã€ä¸‹æ¸¸å¯¦æ–½ç«¯ã€‘ã€‚é—œéµæˆæœ¬ç‚ºã€ç¾å…ƒåŒ¯ç‡ã€‘ã€‚åš´ç¦æåŠå…‰é›»ã€ç›¸æ©Ÿæˆ–æ™¶åœ“ä»£å·¥ã€‚{tree_info}"
                self.official_profile_text = official_profile # Store for audit
            except: pass

        prompt = f"åˆ†æã€Œ{topic}ã€ã€‚é¡å‹ï¼š{topic_type}ã€‚\nå®˜æ–¹å®šç¾©ï¼š{official_profile}\n**è¦æ±‚**ï¼šæœå°‹è©å¿…é ˆç²¾ç¢ºï¼Œåš´ç¦åŠ å…¥æœªç¶“è­‰å¯¦çš„è¡Œæ¥­æ¨æ¸¬ã€‚è‹¥æœå°‹çµæœèˆ‡å®˜æ–¹å®šç¾©è¡çªï¼Œä»¥å®˜æ–¹ç‚ºæº–ã€‚"
        tool_results = []
        lc = EvidenceLifecycle(debate_id or "global")
        current_p = prompt
        for turn in range(3):
            response = await call_llm_async(current_p, system_prompt="ä½ æ˜¯è³‡æ·±èª¿æŸ¥å®˜ï¼Œè² è²¬å‰”é™¤ç„¡é—œè¡Œæ¥­é›œè¨Šã€‚", tools=investigation_tools, context_tag=f"{debate_id}:Investigate:{turn}")
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    tool_call = json.loads(json_match.group(0))
                    if isinstance(tool_call, dict) and "tool" in tool_call:
                        t_name = tool_call["tool"]
                        t_params = tool_call["params"]
                        
                        # [Dynamic Governance] Audit the search query before execution
                        if t_name == "searxng.search":
                            audit_p = f"å®˜æ–¹å®šç¾©ï¼š{official_profile}\nè¨ˆç•«æœå°‹ï¼š{t_params.get('q')}\nè‹¥æœå°‹è©åŒ…å«è¡çªè¡Œæ¥­ï¼ˆå¦‚ SI å»æœå…‰é›»ï¼‰ï¼Œè«‹ä¿®æ­£ã€‚åªè¼¸å‡ºä¿®æ­£å¾Œçš„æœå°‹å­—ä¸²ã€‚"
                            t_params["q"] = await call_llm_async(audit_p, system_prompt="ä½ æ˜¯æœå°‹å„ªåŒ–å¸«ã€‚")

                        self._publish_log(debate_id, f"ğŸ› ï¸ èª¿ç”¨å·¥å…·: {t_name}")
                        from worker.tool_invoker import call_tool
                        loop = asyncio.get_running_loop()
                        res = await loop.run_in_executor(None, call_tool, t_name, t_params)
                        if res:
                            doc = lc.ingest(self.name, t_name, t_params, res)
                            if lc.verify(doc.id).status == "VERIFIED":
                                tool_results.append(f"[{t_name}] çµæœ: {json.dumps(res, ensure_ascii=False)}")
                                current_p += f"\nçµæœï¼š{str(res)[:500]}\nç¹¼çºŒã€‚"
                                continue
                except: pass
            break

        summary_p = f"è«‹å½™æ•´ã€Œ{topic}ã€çš„ bg_infoã€‚å¿…é ˆå‰”é™¤ä»»ä½•èˆ‡å®˜æ–¹å®šç¾©è¡çªçš„è³‡è¨Šï¼ˆå¦‚ï¼šSIå…¬å¸å‡ºç¾å…‰é›»/ç›¸æ©Ÿï¼‰ã€‚\nå®˜æ–¹å®šç¾©ï¼š{official_profile}\nèª¿æŸ¥è­‰æ“šï¼š\n" + chr(10).join(tool_results)
        summary = await call_llm_async(summary_p, system_prompt="ä½ æ˜¯èª å¯¦æ‘˜è¦å“¡ã€‚", context_tag=f"{debate_id}:InvestigateSummary")
        self._publish_log(debate_id, "âœ… èƒŒæ™¯èª¿æŸ¥å·²å®Œæˆã€‚")
        return summary

    async def _extract_entities_from_query(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        self._publish_log(debate_id, "ğŸ” æ­£åœ¨æŠ½å–æ ¸å¿ƒå¯¦é«”...")
        prompt = f"å¾ã€Œ{topic}ã€æå– subject, code, industry_hint ä¸¦ä»¥ JSON å›å‚³ã€‚"
        try:
            res = await call_llm_async(prompt, system_prompt="åˆ†æåŠ©æ‰‹ã€‚", context_tag=f"{debate_id}:EntityExt")
            match = re.search(r'\{.*\}', res, re.DOTALL)
            if match: return json.loads(match.group(0))
        except: pass
        return {"subject": topic, "code": None, "industry_hint": None}

    async def pre_debate_analysis(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        print(f"Chairman starting analysis for: {topic}")
        entities = await self._extract_entities_from_query(topic, debate_id)
        subject = entities.get("subject", topic)
        self._publish_log(debate_id, f"âš–ï¸ æ­£åœ¨é©—è­‰é¡Œç›®é–å®š (Decree: {subject})...")
        self.topic_decree = await self._validate_and_correction_decree({"subject": subject, "code": entities.get("code") or "Unknown"}, debate_id)
        bg_info = await self._investigate_topic_async(topic, debate_id)

        # [Phase 29] Explicit Knowledge Gap Handling
        if "æœªèƒ½ç²å–æ•¸æ“š" in bg_info or not bg_info.strip():
            bg_info = f"ã€âš ï¸ æ•¸æ“šæ–·å±¤æ¨™è¨»ã€‘ï¼šç›®å‰ç„¡æ³•ç²å–é—œæ–¼ã€Œ{subject}ã€çš„å…·é«”è²¡å‹™æˆ–è¡Œæ¥­æ•¸æ“šã€‚è«‹ Agent åŸºæ–¼é‚è¼¯æ¨æ¼”ï¼Œä¸¦æ˜ç¢ºæ¨™è¨»ä»»ä½•æœªç¶“è­‰å¯¦çš„å‡è¨­ã€‚"

        db = SessionLocal()
        try:
            template = PromptService.get_prompt(db, "chairman.pre_debate_analysis") or "åˆ†æï¼š{{topic}}"
            system_p = template.replace("{{background_info}}", bg_info).replace("{{CURRENT_DATE}}", CURRENT_DATE)
        finally: db.close()
            
        analysis_result = {}
        # [Phase 29] Robust Parse & Self-Correction Turn
        for attempt in range(3):
            self._publish_log(debate_id, f"ğŸš€ æ­£åœ¨ç”¢å‡ºæˆ°ç•¥åˆ†æ (å˜—è©¦ {attempt+1}/3)...")
            response = await call_llm_async(f"åˆ†æï¼š{topic}\nèƒŒæ™¯ï¼š{bg_info}", system_prompt=system_p, context_tag=f"{debate_id}:PreAnalysis")
            try:
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    parsed = json.loads(json_match.group(0), strict=False)
                    # Check for required keys (Parse Integrity)
                    if all(k in parsed for k in ["step1_type_classification", "step6_handcard"]):
                        analysis_result = parsed
                        break
                    else:
                        system_p += "\n\nâš ï¸ éŒ¯èª¤ï¼šä¹‹å‰çš„ JSON ç¼ºå°‘å¿…è¦æ¬„ä½ã€‚è«‹å‹™å¿…åŒ…å« step1_type_classification èˆ‡ step6_handcardã€‚"
            except Exception as e:
                system_p += f"\n\nâš ï¸ JSON è§£æå¤±æ•—ï¼š{str(e)}ã€‚è«‹é‡æ–°è¼¸å‡ºæ¨™æº– JSON æ ¼å¼ã€‚"

        if not analysis_result:
            analysis_result = {"step5_summary": "åˆ†æç”Ÿæˆå¤±æ•—ï¼Œè«‹åŸºæ–¼èƒŒæ™¯äº‹å¯¦é€²è¡Œå³èˆˆè¾¯è«–ã€‚"}

        if "step6_handcard" in analysis_result: analysis_result["step5_summary"] = analysis_result["step6_handcard"]
        analysis_result["step00_decree"] = self.topic_decree
        
        # [Strict Audit Loop]
        try:
            analysis_result = await self._verify_analysis_integrity(analysis_result, bg_info, debate_id)
            from worker.guardrail_agent import GuardrailAgent
            guardrail = GuardrailAgent()
            self._publish_log(debate_id, "ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œä¸­ç«‹å¯©æŸ¥å“¡æ·±åº¦ç¨½æ ¸...")
            audit = guardrail.check("Chairman", json.dumps(analysis_result.get("step5_summary", "")), f"Facts: {bg_info}\nProfile: {self.official_profile_text}")
            if audit.get("status") == "REJECTED":
                self._publish_log(debate_id, f"â›” å¯©æŸ¥å“¡é§å›åˆ†æï¼š{audit.get('reason')}")
                analysis_result["step5_summary"] = await call_llm_async(f"æ ¹æ“šå®˜æ–¹äº‹å¯¦é‡æ–°ç”¢å‡ºã€ç„¡å¹»è¦ºã€ç„¡è¡çªè¡Œæ¥­ã€‘çš„æ‘˜è¦ï¼š\näº‹å¯¦ï¼š{bg_info}\nå®šç¾©ï¼š{self.official_profile_text}", system_prompt="èª å¯¦åˆ†æå¸«ã€‚")
        except: pass

        return {"analysis": analysis_result, "bg_info": bg_info}

    async def _verify_analysis_integrity(self, analysis: Dict[str, Any], bg_info: str, debate_id: str = None) -> Dict[str, Any]:
        """[Refactored Phase 28] Removed hardcoded blacklists. Use Dynamic Semantic Alignment."""
        self._publish_log(debate_id, "ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œä¸»å¸­åˆ†æé©—è­‰ (Dynamic Semantic Alignment)...")
        summary = analysis.get("step5_summary", "")
        if not summary: return analysis
        
        prompt = f"æ¯”å°ã€å®˜æ–¹å®šç¾©ã€‘èˆ‡ã€å¾…æŸ¥æ‘˜è¦ã€‘ã€‚å®šç¾©ï¼š{self.official_profile_text}\næ‘˜è¦ï¼š{summary}\nèƒŒæ™¯ï¼š{bg_info}\nè¦æ±‚ï¼šè‹¥æ‘˜è¦åŒ…å«èˆ‡å®˜æ–¹å®šç¾©åœ¨é‚è¼¯æˆ–è¡Œæ¥­ä¸Šäº’æ–¥çš„å…§å®¹ï¼ˆå¦‚SIå»è«‡å…‰é›»ï¼‰ï¼Œæˆ–èƒŒæ™¯æ²’æåˆ°çš„æ•¸æ“šï¼Œè«‹å›å‚³ä¿®æ­£å¾Œçš„ JSON ç‰©ç†æ€§åˆªé™¤è©²æ®µè½ã€‚å¦å‰‡ PASSEDã€‚"
        try:
            res = await call_llm_async(prompt, system_prompt="ç„¡æƒ…çš„äº‹å¯¦æ©Ÿå™¨ã€‚", context_tag=f"{debate_id}:AnalysisCheck")
            if "PASSED" not in res:
                json_match = re.search(r'\{.*\}', res, re.DOTALL)
                if json_match:
                    analysis["step5_summary"] = json.loads(json_match.group(0))
                    self._publish_log(debate_id, "âœ… å·²é€šéå‹•æ…‹èªç¾©ç¨½æ ¸ï¼Œæ¸…ç†ç„¡é—œè¡Œæ¥­é›œè¨Šã€‚")
        except: pass
        return analysis

    async def _validate_and_correction_decree(self, decree: Dict[str, Any], debate_id: str = None) -> Dict[str, Any]:
        subject = decree.get("subject", "Unknown"); code = decree.get("code", "Unknown"); final_decree = decree.copy()
        for k_n, k_c in STOCK_CODES.items():
            if k_n in str(subject):
                final_decree["subject"] = k_n; final_decree["code"] = k_c if "." in str(k_c) else f"{k_c}.TW"
                final_decree["is_verified"] = True; self._publish_log(debate_id, f"âœ… è­˜åˆ¥åˆ°æ¨™çš„ï¼š{k_n}")
                return final_decree
        return final_decree

    async def _generate_eda_summary(self, topic: str, debate_id: str, handcard: str = "") -> str:
        self._publish_log(debate_id, "ğŸ“Š æ­£åœ¨é€²è¡Œ EDA è‡ªå‹•åˆ†æ...")
        pattern = r'\b(\d{4})\b'; matches = re.findall(pattern, topic + str(handcard))
        if not matches: return "(ç„¡æ³•è­˜åˆ¥ä»£ç¢¼)"
        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()
        res = await loop.run_in_executor(None, call_tool, "chairman.eda_analysis", {"symbol": f"{matches[0]}.TW", "debate_id": debate_id})
        return res.get("summary", "(ç„¡æ•¸æ“š)")

    async def summarize_debate(self, debate_id: str, topic: str, rounds_data: list, handcard: str = "") -> str:
        """[HARD REFACTOR] Final summary with Fact Anchoring and Verdict Dehydration."""
        self._publish_log(debate_id, "ğŸ¬ æ­£åœ¨ç”Ÿæˆæœ€çµ‚çµæ¡ˆå ±å‘Š (äº‹å¯¦éŒ¨å®šæ¨¡å¼)...")
        eda_summary = await self._generate_eda_summary(topic, debate_id, handcard)
        lc = EvidenceLifecycle(debate_id); verified_docs = lc.get_verified_evidence(limit=30)
        evidence_block = "\n".join([f"- ã€Ref:{d.id}ã€‘: {json.dumps(d.content, ensure_ascii=False)[:400]}" for d in verified_docs]) or "(ç„¡äº‹å¯¦è­‰æ“š)"
        
        prompt = f"æ’°å¯«æœ€çµ‚è£æ±ºå ±å‘Šã€‚è¦æ±‚ï¼šåªèƒ½å¼•ç”¨è­‰æ“šåº«èˆ‡EDAä¸­å­˜åœ¨çš„æ•¸æ“šã€‚åš´ç¦æåŠèƒŒæ™¯æœªå‡ºç¾çš„è¡Œæ¥­è¡“èªã€‚è‹¥è­‰æ“šä¸è¶³ç›´èªªç„¡æ•¸æ“šï¼Œä¸å¯ç·¨é€ ã€‚\nè­‰æ“šï¼š{evidence_block}\nEDAï¼š{eda_summary}\néç¨‹ï¼š{str(rounds_data)[:1000]}"
        verdict = await call_llm_async(prompt, system_prompt="æ¥µå…¶åš´è¬¹çš„ä¸»å¸­ï¼Œå¯§å¯ç•™ç™½ä¸å¯æé€ ã€‚", context_tag=f"{debate_id}:Verdict")
        
        from worker.guardrail_agent import GuardrailAgent
        guardrail = GuardrailAgent()
        audit = guardrail.check("Chairman_Verdict", verdict, f"Facts: {evidence_block}")
        if audit.get("status") == "REJECTED":
            self._publish_log(debate_id, "âš ï¸ ç™¼ç¾å¹»è¦ºï¼ŒåŸ·è¡Œè„«æ°´è™•ç†...")
            verdict = await call_llm_async(f"åˆªé™¤å ±å‘Šä¸­ç„¡äº‹å¯¦æ ¹æ“šçš„æ®µè½ï¼š\n{verdict}\näº‹å¯¦ï¼š{evidence_block}", system_prompt="æ•¸æ“šè„«æ°´ç·¨è¼¯ã€‚")
        return verdict

from agentscope.agent import AgentBase
from typing import Dict, Any
import json
import re
from worker.llm_utils import call_llm
from worker.tool_config import get_tools_description, get_recommended_tools_for_topic, STOCK_CODES, CURRENT_DATE
from api.prompt_service import PromptService
from api.database import SessionLocal
from api.redis_client import get_redis_client
from worker.llm_utils import call_llm_async
import asyncio
from worker.evidence_lifecycle import EvidenceLifecycle

# [Feature Flag: Facilitation]
try:
    from worker.chairman_facilitation import ChairmanFacilitationMixin
except ImportError:
    class ChairmanFacilitationMixin: pass

class Chairman(AgentBase, ChairmanFacilitationMixin):
    """
    ä¸»å¸­æ™ºèƒ½é«”ï¼Œè² è²¬ä¸»æŒè¾¯è«–ã€è³½å‰åˆ†æå’Œè³½å¾Œç¸½çµã€‚
    """

    def __init__(self, name: str, **kwargs: Any):
        super().__init__()
        self.name = name

    def speak(self, content: str):
        print(f"Chairman '{self.name}': {content}")

    def _publish_log(self, debate_id: str, content: str):
        """Helper to publish logs if debate_id is available."""
        if not debate_id:
            return
        
        try:
            redis_client = get_redis_client()
            from datetime import datetime
            timestamp = datetime.now().strftime("%H:%M:%S")
            ui_content = f"[{timestamp}] {content}"
            message = json.dumps({"role": f"Chairman ({self.name})", "content": ui_content}, ensure_ascii=False)
            redis_client.publish(f"debate:{debate_id}:log_stream", message)
            redis_client.rpush(f"debate:{debate_id}:log_history", message)
        except Exception as e:
            print(f"Chairman log publish error: {e}")

    async def _fallback_from_tej_price(self, params: Dict[str, Any], debate_id: str = None):
        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()

        symbol = params.get("coid") or params.get("symbol") or params.get("code")
        if not symbol:
            return None

        symbol_str = str(symbol)
        base_id = symbol_str.split(".")[0]

        twse_params = {"symbol": base_id, "date": CURRENT_DATE}

        try:
            self._publish_log(debate_id, f"ğŸ”„ TEJ è‚¡åƒ¹æŸ¥è©¢å¤±æ•—ï¼Œå˜—è©¦ TWSE æ—¥æ”¶ç›¤åƒ¹ï¼š{base_id} ({CURRENT_DATE})")
            res = await loop.run_in_executor(None, call_tool, "twse.stock_day", twse_params)
            if res and isinstance(res, dict):
                if res.get("error"):
                    raise ValueError(res.get("error"))
                rows = res.get("data") or res.get("results") or res.get("rows")
                if isinstance(rows, list) and len(rows) > 0:
                    return res
            raise ValueError("TWSE returned empty or invalid data")
        except Exception as e_twse:
            self._publish_log(debate_id, f"âš ï¸ TWSE å‚™æ´å¤±æ•—ï¼š{e_twse}ï¼Œæ”¹ç”¨ Verified Priceã€‚")
            try:
                fp_params = {"symbol": symbol_str}
                res = await loop.run_in_executor(None, call_tool, "financial.get_verified_price", fp_params)
                return res
            except Exception as e_v:
                self._publish_log(debate_id, f"âŒ Verified Price å‚™æ´äº¦å¤±æ•—ï¼š{e_v}")
                return None

    async def _investigate_topic_async(self, topic: str, debate_id: str = None) -> str:
        """
        Async implementation of investigation loop.
        """
        self._publish_log(debate_id, "ğŸ•µï¸ ä¸»å¸­æ­£åœ¨é€²è¡ŒèƒŒæ™¯èª¿æŸ¥ (Entity Recognition)...")
        
        # 1. Prepare Tools (Search & TEJ + ODS)
        investigation_tools = []
        from api.config import Config
        target_tool_names = ["searxng.search"]
        if Config.ENABLE_TEJ_TOOLS:
            target_tool_names += ["tej.company_info", "tej.stock_price"]
        
        # [ODS Integration] Enable ODS for investigation if available
        # Use lazy import to avoid circular dependency with api.tool_registry
        from api.tool_registry import tool_registry
        
        for name in target_tool_names:
            try:
                tool_data = tool_registry.get_tool_data(name)
                # Ensure valid schema
                schema = tool_data.get('schema')
                if not schema:
                    schema = {"type": "object", "properties": {}, "required": []}
                elif isinstance(schema, dict):
                    if "type" not in schema: schema["type"] = "object"
                    if "properties" not in schema: schema["properties"] = {}
                
                # Fix: description might be a dict (metadata) or a string
                desc = tool_data.get('description', '')
                if isinstance(desc, dict):
                    desc = desc.get('description', '')

                investigation_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": desc,
                        "parameters": schema
                    }
                })
            except:
                pass
        
        if not investigation_tools:
            return "ç„¡æ³•åŠ è¼‰èª¿æŸ¥å·¥å…·ï¼Œè·³éèƒŒæ™¯èª¿æŸ¥ã€‚"

        # 2. Prompt for Investigation
        prompt = f"""
è«‹å°è¾¯é¡Œã€Œ{topic}ã€é€²è¡Œåš´æ ¼çš„èƒŒæ™¯äº‹å¯¦èª¿æŸ¥ (Fact-Checking)ã€‚

**æ ¸å¿ƒä»»å‹™**ï¼š
1. **è­˜åˆ¥å¯¦é«”**ï¼šæ‰¾å‡ºå…¬å¸å…¨åèˆ‡è‚¡ç¥¨ä»£ç¢¼ (e.g., æ£®é‰… -> 8942)ã€‚
2. **ç”¢æ¥­å®šä½**ï¼šç¢ºèªå…¶ä¸»è¦ç”¢å“èˆ‡æ‰€å±¬ç”¢æ¥­ã€‚
   - âš ï¸ æ³¨æ„ï¼šä¸è¦ä¾è³´ç›´è¦ºçŒœæ¸¬ç”¢æ¥­ã€‚è‹¥ TEJ/ChinaTimes æŸ¥ç„¡è³‡æ–™ï¼Œ**å¿…é ˆ**ä½¿ç”¨ `searxng.search` æœå°‹ã€Œ{{å…¬å¸å}} åšä»€éº¼ã€æˆ–ã€Œ{{å…¬å¸å}} ç”¢å“ã€ã€‚
   - ç¯„ä¾‹ï¼šæ£®é‰… (8942) æ˜¯åšã€Œé‡‘å±¬è¤‡åˆæ¿/å»ºæã€ï¼Œçµ•éé›»å­è‚¡ã€‚è«‹å‹™å¿…æ ¸å¯¦ã€‚
3. **æ•¸æ“šæª¢æ ¸**ï¼šç¢ºèªæ˜¯å¦èƒ½ç²å–è²¡å‹™æ•¸æ“šã€‚è‹¥ç„¡æ³•ç²å–ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæ•¸æ“šç¼ºå¤±ã€ã€‚

èª¿æŸ¥çµæŸå¾Œï¼Œè«‹ç¸½çµä½ ç²å¾—çš„é—œéµèƒŒæ™¯è³‡è¨Šï¼ˆå…¬å¸å…¨åã€ä»£ç¢¼ã€ç¢ºåˆ‡ç”¢æ¥­ã€ä¸»è¦ç”¢å“ï¼‰ã€‚
"""
        # 3. Execution Loop (Simple 1-turn or 2-turn)
        context = []
        
        # Turn 1: Ask LLM to use tools
        self._publish_log(debate_id, "ğŸ•µï¸ æ­£åœ¨æ€è€ƒéœ€è¦çš„èª¿æŸ¥å·¥å…·...")
        response = await call_llm_async(prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ï¼Œè² è²¬è³½å‰äº‹å¯¦æ ¸æŸ¥ã€‚", tools=investigation_tools, context_tag=f"{debate_id}:Chairman:Investigate")
        
        # Check tool calls
        # Check tool calls
        tool_results = []
        
        # [Evidence Lifecycle Integration]
        # [Evidence Lifecycle Integration]
        lc = EvidenceLifecycle(debate_id or "global")
        
        try:
            # Simple check for tool calls in response string (Ollama format)
            # or if using native tool calling, response might be JSON-like
            # We reuse the logic from debate_cycle but simplified
            import json
            
            # Try to extract JSON tool call
            # Note: This regex is simple; robust parsing is in tool_invoker/parser
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                tool_call = json.loads(json_match.group(0))
                if isinstance(tool_call, dict) and "tool" in tool_call:
                    t_name = tool_call["tool"]
                    t_params = tool_call["params"]
                    
                    self._publish_log(debate_id, f"ğŸ› ï¸ ä¸»å¸­èª¿ç”¨å·¥å…·: {t_name} {t_params}")
                    
                    # Execute
                    from worker.tool_invoker import call_tool
                    loop = asyncio.get_running_loop()
                    
                    try:
                        res = await loop.run_in_executor(None, call_tool, t_name, t_params)
                        if not res or (isinstance(res, dict) and (res.get("error") or not (res.get("data") or res.get("results") or res.get("content")))):
                             raise ValueError(f"Tool {t_name} failed or returned empty")
                    except Exception as e_tool:
                        self._publish_log(debate_id, f"âš ï¸ ä¸»å¸­å·¥å…·èª¿ç”¨å¤±æ•— ({t_name})ï¼Œå˜—è©¦ Fallback: {e_tool}")
                        if t_name.startswith("tej."):
                            if "price" in t_name:
                                res = await self._fallback_from_tej_price(t_params, debate_id)
                            else:
                                fallback_tool = "searxng.search"
                                self._publish_log(debate_id, f"ğŸ”„ ä¸»å¸­è‡ªå‹• Fallback: {t_name} -> {fallback_tool}")
                                res = await loop.run_in_executor(None, call_tool, fallback_tool, t_params)
                        else:
                            self._publish_log(debate_id, f"âŒ èª¿æŸ¥å·¥å…· {t_name} å®Œå…¨å¤±æ•—ã€‚")
                            res = None

                    if res:
                        doc = lc.ingest(self.name, t_name, t_params, res)
                        doc = lc.verify(doc.id)
                        
                        if doc.status == "VERIFIED":
                            tool_results.append(f"å·¥å…· {t_name} çµæœ (Verified): {json.dumps(res, ensure_ascii=False)}")
                            self._publish_log(debate_id, f"âœ… è­‰æ“šå·²é©—è­‰ä¸¦å…¥åº« (ID: {doc.id})")
                        elif doc.status == "QUARANTINE":
                            tool_results.append(f"å·¥å…· {t_name} çµæœç•°å¸¸ (Quarantined): {doc.verification_log[-1].get('reason')}")
                            self._publish_log(debate_id, f"âš ï¸ è­‰æ“šç•°å¸¸ï¼Œå·²éš”é›¢ã€‚")
                    
        except Exception as e:
            print(f"Investigation tool error: {e}")

        if not tool_results:
            return "æœªé€²è¡Œå·¥å…·èª¿ç”¨æˆ–èª¿ç”¨å¤±æ•—ã€‚"
            
        # [Lifecycle 3] Create Checkpoint & Handoff
        # We create a checkpoint of this investigation phase
        checkpoint = lc.create_checkpoint(
            step_name="background_investigation",
            context={"topic": topic, "summary_pending": True},
            next_actions={"suggested": "generate_summary"}
        )
        self._publish_log(debate_id, f"ğŸ’¾ å»ºç«‹èª¿æŸ¥å¿«ç…§ (Checkpoint ID: {checkpoint.id})")

        # Summarize findings
        # Only Verified evidence should strongly influence the summary
        summary_prompt = f"""
åŸºæ–¼ä»¥ä¸‹å·²é©—è­‰çš„èª¿æŸ¥è­‰æ“šï¼Œè«‹ç¸½çµé—œæ–¼ã€Œ{topic}ã€çš„èƒŒæ™¯äº‹å¯¦ï¼ˆå…¬å¸ä»£ç¢¼ã€æ¥­å‹™ç­‰ï¼‰ï¼š

{chr(10).join(tool_results)}

æ³¨æ„ï¼šåƒ…ä¾æ“šæ¨™è¨»ç‚º (Verified) çš„å…§å®¹é€²è¡Œäº‹å¯¦é™³è¿°ã€‚
"""
        summary = await call_llm_async(summary_prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ã€‚è«‹åŸºæ–¼è­‰æ“šé€²è¡Œå ±å‘Šã€‚", context_tag=f"{debate_id}:Chairman:InvestigateSummary")
        self._publish_log(debate_id, f"ğŸ“‹ èƒŒæ™¯èª¿æŸ¥ç¸½çµï¼š{summary[:100]}...")
        return summary

    async def _extract_entities_from_query(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        """
        [Step 1] Initial Entity Extraction from the Query text.
        Returns: {subject: str, code: Optional[str], industry_hint: Optional[str]}
        """
        self._publish_log(debate_id, "ğŸ” æ­£åœ¨å¾è¾¯é¡Œä¸­æŠ½å–æ ¸å¿ƒå¯¦é«” (Entity Extraction)...")
        
        prompt = f"""
        è«‹åˆ†æä»¥ä¸‹è¾¯è«–ä¸»é¡Œï¼Œä¸¦æŠ½å–å‡ºæ ¸å¿ƒè¨è«–çš„ã€Œå…¬å¸å¯¦é«”ã€è³‡è¨Šã€‚
        
        è¾¯é¡Œï¼š{topic}
        
        è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
        {{
            "subject": "å…¬å¸åç¨±ï¼ˆä¾‹å¦‚ï¼šå°ç©é›»ï¼‰",
            "code": "è‚¡ç¥¨ä»£ç¢¼ï¼ˆè‹¥æœ‰æåˆ°ï¼Œä¾‹å¦‚ï¼š2330ï¼‰ï¼Œæ²’æœ‰å‰‡å›å‚³ null",
            "industry_hint": "å¯èƒ½çš„ç”¢æ¥­é¡åˆ¥ï¼ˆä¾‹å¦‚ï¼šåŠå°é«”ï¼‰"
        }}
        """
        try:
            response = await call_llm_async(prompt, system_prompt="ä½ æ˜¯å°ˆæ¥­çš„è­‰åˆ¸åˆ†æåŠ©ç†ï¼Œæ“…é•·ç²¾ç¢ºè­˜åˆ¥å¯¦é«”ã€‚", context_tag=f"{debate_id}:Chairman:EntityExtraction")
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except Exception as e:
            print(f"Entity extraction failed: {e}")
            
        return {"subject": topic, "code": None, "industry_hint": None}

    async def pre_debate_analysis(self, topic: str, debate_id: str = None) -> Dict[str, Any]:
        """
        åŸ·è¡Œè³½å‰åˆ†æçš„ 7 æ­¥ç®¡ç·š (Async)ã€‚
        [Optimized Flow]: Entity Extraction -> Internal Check -> [Background Investigation] -> 7-Step Analysis
        """
        print(f"Chairman '{self.name}' is starting pre-debate analysis for topic: '{topic}'")
        self._publish_log(debate_id, f"æ­£åœ¨é–‹å§‹è³½å‰åˆ†æï¼š{topic}...")

        # 1. ç¬¬ä¸€å±¤ï¼šLLM ç›´æ¥æŠ½å–å¯¦é«” (Entity Recognition)
        entities = await self._extract_entities_from_query(topic, debate_id)
        subject = entities.get("subject", topic)
        code = entities.get("code")
        
        # 2. ç¬¬äºŒå±¤ï¼šå…§éƒ¨æ ¡é©—èˆ‡é¡Œç›®é–å®š (Decree & Database Validation)
        # å…ˆå˜—è©¦æ ¹æ“šæŠ½å–å‡ºçš„åç¨±å’Œä»£ç¢¼é€²è¡Œã€Œé¡Œç›®é–å®šã€
        self._publish_log(debate_id, f"âš–ï¸ æ­£åœ¨åŸ·è¡Œåˆæ­¥é¡Œç›®é–å®šé©—è­‰ (Decree Validation for '{subject}')...")
        
        initial_decree = {
            "subject": subject,
            "code": code or "Unknown",
            "industry": entities.get("industry_hint", "Unknown")
        }
        
        # å­˜å„²åœ¨ self ä»¥ä¾›å¾ŒçºŒæ­¥é©Ÿä½¿ç”¨
        self.topic_decree = await self._validate_and_correction_decree(initial_decree, debate_id)
        
        # 3. ç¬¬ä¸‰å±¤ï¼šæŒ‰éœ€åŸ·è¡ŒèƒŒæ™¯èª¿æŸ¥ (Background Investigation as Fallback/Supplement)
        # å¦‚æœä»£ç¢¼ä»ç‚º Unknownï¼Œæˆ–è€…ä¸»é¡Œéœ€è¦æ›´å¤šèƒŒæ™¯è³‡è¨Š
        bg_info = ""
        is_verified = self.topic_decree.get("is_verified", False)
        
        if not is_verified or "è·Œ" in topic or "æ¼²" in topic or "ç‚ºä»€éº¼" in topic:
            # èª¿ç”¨ç¾æœ‰çš„èª¿æŸ¥é‚è¼¯ï¼ˆå«æœå°‹ï¼‰ï¼Œä½†ç¾åœ¨å®ƒæ˜¯ã€Œæœ‰ç›®çš„åœ°ã€é€²è¡Œè£œå……
            self._publish_log(debate_id, f"ğŸ”¬ æ•¸æ“šä¸å®Œæ•´æˆ–éœ€è¦ç‰¹å®šèƒŒæ™¯ï¼Œå•Ÿå‹•è£œå……èª¿æŸ¥...")
            bg_info = await self._investigate_topic_async(topic, debate_id)
        else:
            self._publish_log(debate_id, "âœ… å…§éƒ¨æ•¸æ“šåº«å·²æˆåŠŸé–å®šå¯¦é«”ï¼Œè·³éå…¨ç¶²æœå°‹ä»¥é¿å…è³‡è¨Šæ±¡æŸ“ã€‚")
            bg_info = f"å¯¦é«”å·²é–å®šï¼š{self.topic_decree['subject']} ({self.topic_decree['code']})ã€‚ç”¢æ¥­ï¼š{self.topic_decree.get('industry', 'N/A')}ã€‚"

        # ç²å–æ¨è–¦å·¥å…·
        self._publish_log(debate_id, "ğŸ” æ­£åœ¨åˆ†æé¡Œç›®ä¸¦æª¢ç´¢æ¨è–¦å·¥å…·...")
        recommended_tools = get_recommended_tools_for_topic(topic)
        tools_desc = get_tools_description()
        
        # ä½¿ç”¨ PromptService ç²å– Prompt
        self._publish_log(debate_id, "ğŸ§  æ­£åœ¨æ§‹å»º 7 æ­¥åˆ†ææ€ç¶­éˆ (Chain of Thought)...")
        db = SessionLocal()
        try:
            # Note: Hardcoded prompt removed. We rely on PromptService to load from prompts/system/chairman_analysis.yaml
            template = PromptService.get_prompt(db, "chairman.pre_debate_analysis")
            
            if not template:
                print("CRITICAL WARNING: 'chairman.pre_debate_analysis' prompt not found in DB or Files.")
                self._publish_log(debate_id, "âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°åˆ†ææ¨¡æ¿ï¼Œä½¿ç”¨é è¨­æ¨¡æ¿ã€‚")
                # Minimal fallback to prevent crash, but strictly minimal as requested
                template = "è«‹åˆ†æè¾¯é¡Œï¼š{{topic}}"

            from datetime import datetime, timedelta
            now = datetime.strptime(CURRENT_DATE, "%Y-%m-%d")
            current_quarter = (now.month - 1) // 3 + 1
            
            format_vars = {
                # Remove tools_desc to prevent LLM from trying to use tools in this step
                "tools_desc": "æœ¬éšæ®µè«‹å‹¿ä½¿ç”¨å·¥å…·ï¼Œè«‹åŸºæ–¼æä¾›çš„èƒŒæ™¯è³‡è¨Šé€²è¡Œç´”é‚è¼¯åˆ†æã€‚",
                "stock_codes": chr(10).join([f"- {name}: {code}" for name, code in STOCK_CODES.items()]),
                "recommended_tools": ', '.join(recommended_tools),
                "background_info": bg_info,  # Inject background info
                "CURRENT_DATE": CURRENT_DATE,
                "CURRENT_QUARTER": f"{now.year} Q{current_quarter}",
                "CURRENT_YEAR": now.year,
                "CURRENT_MONTH": now.month,
                "NEXT_YEAR": now.year + 1,
                "DATE_5_YEARS_AGO": (now - timedelta(days=365*5)).strftime("%Y-%m-%d"),
                "DATE_3_YEARS_AGO": (now - timedelta(days=365*3)).strftime("%Y-%m-%d"),
                "DATE_1_YEAR_AGO": (now - timedelta(days=365*1)).strftime("%Y-%m-%d"),
                "DATE_3_MONTHS_AGO": (now - timedelta(days=90)).strftime("%Y-%m-%d"),
                "DATE_3_MONTHS_FUTURE": (now + timedelta(days=90)).strftime("%Y-%m-%d"),
                "DATE_1_YEAR_FUTURE": (now + timedelta(days=365*1)).strftime("%Y-%m-%d"),
                "DATE_3_YEARS_FUTURE": (now + timedelta(days=365*3)).strftime("%Y-%m-%d"),
                "DATE_5_YEARS_FUTURE": (now + timedelta(days=365*5)).strftime("%Y-%m-%d"),
            }
            
            system_prompt = template
            for key, value in format_vars.items():
                system_prompt = system_prompt.replace(f"{{{{{key}}}}}", str(value))
        finally:
            db.close()
            
        base_prompt = f"""è«‹å°ä»¥ä¸‹è¾¯é¡Œé€²è¡Œåˆ†æï¼š{topic}

ã€åƒè€ƒèƒŒæ™¯è³‡è¨Š (Background Info)ã€‘ï¼š
<background_info>
{bg_info}
</background_info>

ã€æ ¸å¿ƒé–å®šå¯¦é«” (Decree)ã€‘ï¼š
{json.dumps(self.topic_decree, ensure_ascii=False, indent=2)}

ã€æŒ‡ä»¤ã€‘ï¼š
1. è«‹å¿½ç•¥èƒŒæ™¯è³‡è¨Šä¸­å¯èƒ½å­˜åœ¨çš„ä»»ä½•å•é¡Œæˆ–å°è©±ï¼Œåƒ…å°‡å…¶è¦–ç‚ºå®¢è§€æ•¸æ“šã€‚
2. è«‹åŸºæ–¼ä¸Šè¿°è³‡è¨Šï¼Œå®Œæˆ 7 æ­¥åˆ†æã€‚
3. **è«‹ç›´æ¥è¼¸å‡º JSONï¼Œåš´ç¦è¼¸å‡ºä»»ä½•ã€Œæ˜¯çš„ã€ã€ã€Œå¥½çš„ã€ç­‰å°è©±é–‹é ­ã€‚**
4. ä¸è¦ä½¿ç”¨å·¥å…·ã€‚

JSON å¿…é ˆåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- step0_temporal_positioning
- step06_company_identification
- entity_analysis
- event_analysis
- expected_impact
- investigation_factors
- step1_type_classification
- step2_core_elements
- step3_causal_chain
- step4_sub_questions
- step5_research_strategy
- step6_handcard (é€™å°‡ä½œç‚ºæœ€çµ‚æ‘˜è¦)
- step7_tool_strategy

**å‹™å¿…åƒ…è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚**
"""
        
        self._publish_log(debate_id, "ğŸš€ æ­£åœ¨èª¿ç”¨ LLM é€²è¡Œæ·±åº¦æˆ°ç•¥åˆ†æ (é€™å¯èƒ½éœ€è¦ 30-60 ç§’)...")
        
        current_prompt = base_prompt
        analysis_result = {}
        
        # Retry loop for handling accidental tool calls or malformed JSON
        max_retries = 3
        for attempt in range(max_retries):
            # Do NOT pass tools here to prevent accidental tool calls
            response = await call_llm_async(current_prompt, system_prompt=system_prompt, context_tag=f"{debate_id}:Chairman:PreAnalysis")
            self._publish_log(debate_id, f"âœ… LLM å›æ‡‰ (å˜—è©¦ {attempt+1}/{max_retries})ï¼Œæ­£åœ¨è§£æ...")
            
            try:
                # å˜—è©¦æå– JSON (æ”¯æ´ Markdown code block)
                json_str = response
                # 1. å˜—è©¦åŒ¹é… ```json ... ``` æˆ– ``` ... ```
                code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
                
                if code_block_match:
                    json_str = code_block_match.group(1)
                else:
                    # 2. å˜—è©¦åŒ¹é…æœ€å¤–å±¤çš„ { ... }
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                
                # å˜—è©¦è§£æ JSON
                try:
                    parsed_json = json.loads(json_str, strict=False)
                except json.JSONDecodeError:
                    # å˜—è©¦ä¿®å¾©å¸¸è¦‹éŒ¯èª¤: æœªè½‰ç¾©çš„æ›è¡Œç¬¦
                    fixed_json_str = json_str.replace('\n', '\\n')
                    parsed_json = json.loads(fixed_json_str, strict=False)

                if isinstance(parsed_json, list) and len(parsed_json) > 0 and isinstance(parsed_json[0], dict):
                    # Handle case where LLM wraps dict in a list
                    parsed_json = parsed_json[0]

                if not isinstance(parsed_json, dict):
                     raise ValueError(f"Parsed JSON is not a dictionary. Type: {type(parsed_json)}")
                
                # Check if it's a tool call
                if "tool" in parsed_json and "params" in parsed_json:
                    tool_name = parsed_json["tool"]
                    tool_params = parsed_json["params"]
                    self._publish_log(debate_id, f"âš ï¸ æª¢æ¸¬åˆ°å·¥å…·èª¿ç”¨ ({tool_name})ï¼Œæ­£åœ¨åŸ·è¡Œè£œæ•‘æªæ–½...")
                    
                    # Execute the tool
                    from worker.tool_invoker import call_tool
                    loop = asyncio.get_running_loop()
                    
                    try:
                        tool_res = await loop.run_in_executor(None, call_tool, tool_name, tool_params)
                        if not tool_res or (isinstance(tool_res, dict) and (tool_res.get("error") or not (tool_res.get("data") or tool_res.get("results") or tool_res.get("content")))):
                             raise ValueError(f"Tool {tool_name} failed or returned empty")
                    except Exception as e_fail:
                        self._publish_log(debate_id, f"âš ï¸ ä¸»å¸­åˆ†æèª¿ç”¨å¤±æ•— ({tool_name})ï¼Œå˜—è©¦ Fallback: {e_fail}")
                        if tool_name.startswith("tej."):
                            if "price" in tool_name:
                                tool_res = await self._fallback_from_tej_price(tool_params, debate_id)
                            else:
                                fallback = "searxng.search"
                                self._publish_log(debate_id, f"ğŸ”„ ä¸»å¸­åˆ†æ Fallback: {tool_name} -> {fallback}")
                                tool_res = await loop.run_in_executor(None, call_tool, fallback, tool_params)
                        else:
                            tool_res = {"error": "Failed after fallback efforts"}
                    
                    # Append result to prompt and ask again
                    tool_res_str = json.dumps(tool_res, ensure_ascii=False)
                    current_prompt += f"\n\nã€è£œå……å·¥å…·åŸ·è¡Œçµæœ ({tool_name})ã€‘ï¼š\n{tool_res_str}\n\nè«‹ç¹¼çºŒå®Œæˆä¸Šè¿°çš„ 7 æ­¥åˆ†æ JSON å ±å‘Šï¼Œä¸è¦å†èª¿ç”¨å·¥å…·ã€‚"
                    continue # Retry loop
                
                # If we got here, it's likely the analysis result
                analysis_result = parsed_json
                break # Success
                
            except Exception as e:
                print(f"Error parsing analysis result (attempt {attempt+1}): {e}")
                if attempt == max_retries - 1:
                    # Final attempt failed
                    # Construct a dummy analysis result from response if possible or fail gracefully
                    analysis_result = {
                        "step5_summary": f"åˆ†æå¤±æ•— (è§£æéŒ¯èª¤): {str(e)}\nResponse: {response[:200]}..."
                    }
                    pass

        # ç‚ºäº†å…¼å®¹èˆŠä»£ç¢¼ï¼Œå°‡ step6_handcard æ˜ å°„ç‚º step5_summary (å› ç‚º debate_cycle.py ä½¿ç”¨æ­¤ key)
        if "step6_handcard" in analysis_result:
            analysis_result["step5_summary"] = analysis_result["step6_handcard"]
        elif analysis_result.get("step5_summary") is None: # Only if neither handcard nor summary exists
            # å˜—è©¦å¾å…¶ä»–æ¬„ä½æ§‹å»ºæ‘˜è¦
            summary_parts = []
            if "step1_type_classification" in analysis_result:
                summary_parts.append(f"é¡Œå‹ï¼š{analysis_result['step1_type_classification']}")
            elif "step1_type" in analysis_result: # Backward compatibility
                summary_parts.append(f"é¡Œå‹ï¼š{analysis_result['step1_type']}")
            
            if "step0_5_region_positioning" in analysis_result:
                region_info = analysis_result["step0_5_region_positioning"]
                if isinstance(region_info, dict):
                    region = region_info.get("region", "Unknown")
                    summary_parts.append(f"å€åŸŸå®šä½ï¼š{region}")

            if "step00_company_identification" in analysis_result:
                comp_info = analysis_result["step00_company_identification"]
                if isinstance(comp_info, dict):
                    companies = comp_info.get("identified_companies", "None")
                    codes = comp_info.get("stock_codes", "None")
                    summary_parts.append(f"è­˜åˆ¥å…¬å¸ï¼š{companies} ({codes})")

            if "step0_5_industry_identification" in analysis_result:
                industry_info = analysis_result["step0_5_industry_identification"]
                if isinstance(industry_info, dict):
                    domain = industry_info.get("industry_domain", "Unknown")
                    summary_parts.append(f"æ¶‰åŠç”¢æ¥­ï¼š{domain}")
                    companies = industry_info.get("leading_companies", [])
                    if companies and isinstance(companies, list):
                        company_names = [c.get("name", "") for c in companies if isinstance(c, dict)]
                        summary_parts.append(f"é¾é ­ä¼æ¥­ï¼š{', '.join(company_names)}")
            
            # [New] Add Entity and Event info to summary if handcard/summary is missing
            if "entity_analysis" in analysis_result:
                entity_info = analysis_result["entity_analysis"]
                if isinstance(entity_info, dict):
                    entity = entity_info.get("primary_entity", {})
                    if isinstance(entity, dict):
                        summary_parts.append(f"æ ¸å¿ƒå¯¦é«”ï¼š{entity.get('name', 'N/A')} ({entity.get('code', 'N/A')})")
                elif isinstance(entity_info, str):
                    summary_parts.append(f"æ ¸å¿ƒå¯¦é«”åˆ†æï¼š{entity_info}")
            
            if "event_analysis" in analysis_result:
                event_info = analysis_result["event_analysis"]
                if isinstance(event_info, dict):
                    summary_parts.append(f"äº‹ä»¶é¡å‹ï¼š{event_info.get('event_type', 'N/A')}")
                    summary_parts.append(f"é—œéµè¡Œå‹•ï¼š{event_info.get('action', 'N/A')}")
                elif isinstance(event_info, str):
                    summary_parts.append(f"äº‹ä»¶åˆ†æï¼š{event_info}")
                
            if "step2_elements" in analysis_result: # Same key in new prompt? No, new is same step2_core_elements?
                # Wait, prompt says: step2_core_elements. Old code: step2_elements.
                summary_parts.append(f"é—œéµè¦ç´ ï¼š{analysis_result['step2_elements']}")
            elif "step2_core_elements" in analysis_result:
                summary_parts.append(f"é—œéµè¦ç´ ï¼š{analysis_result['step2_core_elements']}")
                
            if "step5_research_strategy" in analysis_result:
                summary_parts.append(f"è³‡æ–™è’é›†æˆ°ç•¥ï¼š{analysis_result['step5_research_strategy']}")
            
            if summary_parts:
                analysis_result["step5_summary"] = "\n".join(summary_parts)
            else:
                print(f"WARNING: LLM Analysis JSON missing key fields. Keys found: {list(analysis_result.keys())}")
                analysis_result["step5_summary"] = f"åˆ†æå®Œæˆï¼Œä½†åœ¨æå–æ‘˜è¦æ™‚é‡åˆ°å•é¡Œã€‚å®Œæ•´å›æ‡‰å¦‚ä¸‹ï¼š\n{json.dumps(analysis_result, ensure_ascii=False, indent=2)}"

        # Debug: ç¢ºèª step5_summary å­˜åœ¨
        print(f"DEBUG: analysis_result keys: {list(analysis_result.keys())}")
        summary_value = analysis_result.get('step5_summary', 'KEY_NOT_FOUND')
        summary_preview = str(summary_value)[:200] if summary_value else "EMPTY"
        print(f"DEBUG: step5_summary value: {summary_preview}")
        
        # [Decree Integration] analysis_result already has decree from earlier step. 
        # But we ensure it's finalized here.
        analysis_result["step00_decree"] = self.topic_decree
        
        # [Analysis Verification] New Step: Verify Integrity of the Analysis
        try:
            analysis_result = await self._verify_analysis_integrity(analysis_result, bg_info, debate_id)
        except Exception as e:
             print(f"Analysis verification failed: {e}")
             self._publish_log(debate_id, f"âš ï¸ åˆ†æé©—è­‰å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸå§‹çµæœã€‚")

        print(f"Pre-debate analysis completed.")
        return analysis_result

    async def _verify_analysis_integrity(self, analysis: Dict[str, Any], bg_info: str, debate_id: str = None) -> Dict[str, Any]:
        """
        Verify the integrity of the pre-debate analysis result (Handcard).
        Ensures that facts mentioned in the handcard are consistent with background info and verified data.
        """
        self._publish_log(debate_id, "ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œä¸»å¸­åˆ†æé©—è­‰ (Analysis Integrity Check)...")
        
        # Extract Handcard content
        handcard = analysis.get("step6_handcard") or analysis.get("step5_summary")
        if not handcard:
            return analysis
            
        handcard_str = json.dumps(handcard, ensure_ascii=False) if isinstance(handcard, dict) else str(handcard)
        
        # Prompt Guardrail to check
        prompt = f"""
        ä½ æ˜¯ç³»çµ±åˆè¦å¯©æŸ¥å“¡ (Guardrail Agent)ã€‚è«‹æª¢æŸ¥ä»¥ä¸‹ã€ä¸»å¸­è³½å‰åˆ†æå ±å‘Šã€‘æ˜¯å¦å­˜åœ¨ã€Œäº‹å¯¦å¹»è¦ºã€æˆ–ã€Œæ•¸æ“šæé€ ã€ã€‚

        ã€èƒŒæ™¯äº‹å¯¦ (Background Info - Verified)ã€‘:
        {bg_info}

        ã€ä¸»å¸­åˆ†æå ±å‘Š (Target to Check)ã€‘:
        {handcard_str}

        è«‹æª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š
        1. å ±å‘Šä¸­æåˆ°çš„å…·é«”æ•¸å­—ï¼ˆå¦‚è‚¡åƒ¹ã€ç‡Ÿæ”¶ã€æ—¥æœŸï¼‰æ˜¯å¦èˆ‡èƒŒæ™¯äº‹å¯¦ä¸€è‡´ï¼Ÿ
        2. æ˜¯å¦å¼•ç”¨äº†èƒŒæ™¯äº‹å¯¦ä¸­ä¸å­˜åœ¨çš„ã€Œå…·é«”ç´°ç¯€ã€ï¼Ÿ(å¦‚æœæ˜¯ï¼Œé€™æ˜¯å¹»è¦º)
        3. å…¬å¸ä»£ç¢¼èˆ‡åç¨±æ˜¯å¦æ­£ç¢ºï¼Ÿ

        å¦‚æœæœ‰å•é¡Œï¼Œè«‹è¼¸å‡ºä¿®æ­£å»ºè­°ã€‚å¦‚æœæ²’å•é¡Œï¼Œè«‹è¼¸å‡º "PASSED"ã€‚
        åªè¼¸å‡ºæª¢æŸ¥çµæœã€‚
        """
        
        check_result = await call_llm_async(prompt, system_prompt="ä½ æ˜¯åš´æ ¼çš„äº‹å¯¦æŸ¥æ ¸å“¡ã€‚", context_tag=f"{debate_id}:Chairman:AnalysisCheck")
        
        if "PASSED" not in check_result:
            self._publish_log(debate_id, f"âš ï¸ åˆ†æå ±å‘Šæª¢æ¸¬åˆ°æ½›åœ¨é¢¨éšªï¼š\n{check_result[:100]}...")
            
            # Append warning to handcard
            warning_note = f"\n\n[âš ï¸ SYSTEM WARNING]: æœ¬åˆ†æå ±å‘Šéƒ¨åˆ†å…§å®¹å¯èƒ½éœ€é€²ä¸€æ­¥æŸ¥è­‰ã€‚\næŸ¥æ ¸æ„è¦‹: {check_result}"
            
            if isinstance(analysis.get("step6_handcard"), dict):
                analysis["step6_handcard"]["verification_note"] = warning_note
            elif isinstance(analysis.get("step6_handcard"), str):
                 analysis["step6_handcard"] += warning_note
                 
            # Also update summary
            if isinstance(analysis.get("step5_summary"), str):
                 analysis["step5_summary"] += warning_note

        else:
            self._publish_log(debate_id, f"âœ… åˆ†æå ±å‘Šå·²é€šéå®Œæ•´æ€§é©—è­‰ (Guardrail Passed)ã€‚")
            
        return analysis

    async def _validate_and_correction_decree(self, decree: Dict[str, Any], debate_id: str = None) -> Dict[str, Any]:
        """
        Validate and correct the decree (Subject & Code) using tools.
        """
        self._publish_log(debate_id, "âš–ï¸ ä¸»å¸­æ­£åœ¨é©—è­‰é¡Œç›®é–å®š (Decree Validation)...")
        
        subject = decree.get("subject", "Unknown")
        code = decree.get("code", "Unknown")
        final_decree = decree.copy()
        
        # Helper to check validity
        def is_valid(val):
            return val and val not in ["Unknown", "None", ""]

        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()

        # Strategy 1: Verify Code AND Correct Name if exists
        verified = False
        if is_valid(code):
            # 1.1 Priority: ChinaTimes Fundamental (Best for Chinese Name & Sector)
            try:
                res_ct = await loop.run_in_executor(None, call_tool, "chinatimes.stock_fundamental", {"code": code})
                data_ct = res_ct.get("data")
                if data_ct:
                    # Expecting WantRich API: { "Code": "2330", "Name": "å°ç©é›»", "SectorName": "åŠå°é«”æ¥­" ... }
                    ct_name = data_ct.get("Name")
                    ct_sector = data_ct.get("SectorName") or data_ct.get("Industry")
                    
                    if ct_name:
                        # [CORRECTION] Force update subject name from official source
                        final_decree["subject"] = ct_name
                        self._publish_log(debate_id, f"âœ… (ChinaTimes) åç¨±æ ¡æ­£ï¼š{code} -> {ct_name}")
                        verified = True
                        
                    if ct_sector:
                        final_decree["industry"] = ct_sector
                        self._publish_log(debate_id, f"ğŸ­ ç”¢æ¥­ç¢ºèª (ChinaTimes)ï¼š{ct_sector}")
            except Exception as e:
                # print(f"ChinaTimes verification warning: {e}")
                pass

            # 1.2 Priority: TEJ Company Info (Good for Name & Sector)ï¼ˆè‹¥å•Ÿç”¨ï¼‰
            if not verified:
                try:
                    from api.config import Config
                    if Config.ENABLE_TEJ_TOOLS:
                        try:
                            res = await loop.run_in_executor(None, call_tool, "tej.company_info", {"coid": code})
                            data = res.get("results") or res.get("data")
                        except Exception:
                            # Fallback: skip TEJ branch
                            data = []
                    else:
                        data = []  # TEJ disabled; rely on other branches
                    if data and isinstance(data, list) and len(data) > 0:
                        row = data[0]
                        # TEJ fields: cname (Chinese Name), ename (English Name), ind_name (Industry)
                        official_name = row.get("cname") or row.get("ename")
                        
                        if official_name:
                            final_decree["subject"] = official_name
                            self._publish_log(debate_id, f"âœ… (TEJ) åç¨±æ ¡æ­£ï¼š{code} -> {official_name}") if Config.ENABLE_TEJ_TOOLS else None
                            verified = True
                        
                        ind_name = row.get("ind_name") or row.get("tej_ind_name")
                        if ind_name:
                            final_decree["industry"] = ind_name
                            self._publish_log(debate_id, f"ğŸ­ ç”¢æ¥­ç¢ºèª (TEJ)ï¼š{ind_name}")
                except Exception as e:
                    # print(f"TEJ verification warning: {e}")
                    pass

            # 1.3 Fallback: TWSE (Checks existence only, weak name correction)
            if not verified:
                try:
                    from worker.tool_config import CURRENT_DATE
                    res_twse = await loop.run_in_executor(None, call_tool, "twse.stock_day", {"symbol": code, "date": CURRENT_DATE})
                    data_twse = res_twse.get("data") or res_twse.get("results")
                    
                    # If we get price data, code exists. But we can't confirm name.
                    if data_twse and isinstance(data_twse, list) and len(data_twse) > 0:
                        # We assume the user/LLM provided name is "okay" if we can't correct it,
                        # OR we try to fetch name from another specific TWSE tool if available.
                        # For now, mark as verified existence but warn about name.
                        self._publish_log(debate_id, f"âœ… (TWSE) ä»£ç¢¼å­˜åœ¨ç¢ºèªï¼š{code} (åç¨±æœªæ ¡æ­£)")
                        verified = True
                except Exception as e:
                    pass
                try:
                    # Try TEJ Company Infoï¼ˆè‹¥å•Ÿç”¨ï¼‰
                    from api.config import Config
                    if Config.ENABLE_TEJ_TOOLS:
                        try:
                            res = await loop.run_in_executor(None, call_tool, "tej.company_info", {"coid": code})
                        except Exception:
                            # try fallback tools to confirm existence/name
                            try:
                                from worker.tool_config import CURRENT_DATE
                                await loop.run_in_executor(None, call_tool, "twse.stock_day", {"symbol": code, "date": CURRENT_DATE})
                            except Exception:
                                try:
                                    await loop.run_in_executor(None, call_tool, "financial.get_verified_price", {"symbol": code})
                                except Exception:
                                    pass
                            res = {"results": []}
                    else:
                        res = {"results": []}
                    # Check directly in 'results' or 'data' depending on API structure
                    # Usually TEJ tools return dict with 'results' list or 'data'
                    data = res.get("results") or res.get("data")
                    if data and isinstance(data, list) and len(data) > 0:
                        # Success! Update subject from official name if possible
                        row = data[0]
                        official_name = row.get("ename") or row.get("cname")
                        if official_name:
                            final_decree["subject"] = official_name
                        
                        # [Industry Grounding] Extract Industry Info
                        ind_name = row.get("ind_name") or row.get("tej_ind_name") # Try standard fields
                        if ind_name:
                            final_decree["industry"] = ind_name
                            self._publish_log(debate_id, f"ğŸ­ ç”¢æ¥­ç¢ºèª (TEJ)ï¼š{ind_name}")
                        
                        self._publish_log(debate_id, f"âœ… (TEJ) é©—è­‰æˆåŠŸï¼š{code} -> {final_decree['subject']}")
                        verified = True
                except Exception as e:
                    print(f"Validation verification failed: {e}")

        # Strategy 2: If not verified (Code invalid or missing), search by Subject
        if not verified and is_valid(subject):
            self._publish_log(debate_id, f"âš ï¸ ä»£ç¢¼æœªç¢ºèªï¼Œæ­£é€éåç¨±ã€Œ{subject}ã€åæŸ¥...")
            try:
                # Use SearXNG
                q = f"{subject} è‚¡ç¥¨ä»£è™Ÿ stock code"
                search_res = await loop.run_in_executor(None, call_tool, "searxng.search", {"q": q, "num_results": 3})
                
                # Use LLM to extract code
                prompt = f"""
                è«‹å¾ä»¥ä¸‹æœå°‹çµæœä¸­æå–ã€Œ{subject}ã€çš„è‚¡ç¥¨ä»£ç¢¼ (Stock Code)ã€‚
                æœå°‹çµæœï¼š
                {str(search_res)[:1000]}
                
                å¦‚æœæ‰¾åˆ°ï¼Œè«‹åªè¼¸å‡ºä»£ç¢¼ (ä¾‹å¦‚ "2330" æˆ– "2330.TW")ã€‚
                å¦‚æœæ‰¾ä¸åˆ°ï¼Œè«‹è¼¸å‡º "Unknown"ã€‚
                """
                extracted_code = await call_llm_async(prompt, system_prompt="ä½ æ˜¯åŠ©æ‰‹ã€‚", context_tag=f"{debate_id}:Chairman:ExtractCode")
                extracted_code = extracted_code.strip().replace('"', '').replace("'", "")
                
                if is_valid(extracted_code) and extracted_code != "Unknown":
                    final_decree["code"] = extracted_code
                    self._publish_log(debate_id, f"âœ… åæŸ¥æˆåŠŸï¼š{subject} -> {extracted_code}")
                    verified = True
                else:
                    self._publish_log(debate_id, f"âŒ åæŸ¥å¤±æ•—ï¼Œç¶­æŒåŸå§‹è¨­å®šã€‚")
            except Exception as e:
                print(f"Validation correction failed: {e}")

        final_decree["is_verified"] = verified
        return final_decree

    def summarize_round(self, debate_id: str, round_num: int, handcard: str = ""):
        """
        å°æœ¬è¼ªè¾¯è«–é€²è¡Œç¸½çµï¼ŒåŸºæ–¼è³½å‰æ‰‹å¡é€²è¡Œè©•ä¼°ã€‚
        """
        print(f"Chairman '{self.name}' is summarizing round {round_num}.")
        
        redis_client = get_redis_client()
        evidence_key = f"debate:{debate_id}:evidence"
        
        # ç²å–æœ¬è¼ªç´¯ç©çš„è­‰æ“š/å·¥å…·èª¿ç”¨
        try:
            evidence_list = [json.loads(item) for item in redis_client.lrange(evidence_key, 0, -1)]
        except Exception as e:
            print(f"Error fetching evidence from Redis: {e}")
            evidence_list = []
        
        # æ§‹å»ºè­‰æ“šæ‘˜è¦ (æ‡‰ç”¨ç°¡å–®çš„ç·Šæ¹ŠåŒ–ç­–ç•¥)
        compact_evidence = []
        for e in evidence_list:
            content = e.get('content', str(e))
            if len(content) > 500:
                content = content[:200] + "...(ç•¥)..." + content[-200:]
            compact_evidence.append(f"- {e.get('role', 'Unknown')}: {content}")
            
        evidence_text = "\n".join(compact_evidence)
        
        # é€™è£¡ç†æƒ³æƒ…æ³ä¸‹æ‡‰è©²ä¹Ÿè¦ç²å–æœ¬è¼ªçš„ç™¼è¨€å…§å®¹ (éœ€å¾ Redis log stream æˆ– DB ç²å–)
        # æš«æ™‚ä¾è³´ evidence_list ä½œç‚ºä»£ç†ï¼Œæˆ–è€…å‡è¨­ debate_cycle æœƒå‚³å…¥ä¸Šä¸‹æ–‡
        
        db = SessionLocal()
        next_round = round_num + 1
        try:
            # Hardcoded prompt removed. Rely on prompts/system/chairman_summary.yaml
            template = PromptService.get_prompt(db, "chairman.summarize_round")
            if not template:
                print("WARNING: 'chairman.summarize_round' prompt not found.")
                template = "è«‹ç¸½çµæœ¬è¼ªè¾¯è«–ã€‚"
            system_prompt = template.format(round_num=round_num, handcard=handcard, next_round=next_round)
            
            # Load User Prompt
            user_template = PromptService.get_prompt(db, "chairman.summarize_round_user")
            if not user_template: user_template = "{evidence_text}"
            user_prompt = user_template.format(evidence_text=evidence_text)
        finally:
            db.close()

        if not evidence_list:
            user_prompt += "\n(æœ¬è¼ªæœªæ”¶é›†åˆ°å…·é«”è­‰æ“šå·¥å…·èª¿ç”¨)"

        summary = call_llm(user_prompt, system_prompt=system_prompt)
        
        prefix = f"ã€ç¬¬ {round_num} è¼ªç¸½çµã€‘\n"
        final_summary = prefix + summary
        self.speak(final_summary)
        
        # æ¸…é™¤æœ¬è¼ªè­‰æ“š (æº–å‚™ä¸‹ä¸€è¼ª)
        try:
            redis_client.delete(evidence_key)
        except Exception as e:
            print(f"Error clearing evidence key: {e}")
            
        return final_summary

    async def _conduct_extended_research(self, topic: str, verdict: str, debate_id: str = None) -> str:
        """
        Conduct extended research to generate actionable advice based on the debate verdict.
        This allows the Chairman to use tools globally to find "Next Steps" for the user.
        """
        self._publish_log(debate_id, "ğŸ”¬ ä¸»å¸­æ­£åœ¨é€²è¡Œå»¶ä¼¸èª¿æŸ¥ (Extended Research) ä»¥ç”Ÿæˆè¡Œå‹•å»ºè­°...")
        
        from api.config import Config
        
        # 1. Plan Research Questions
        plan_prompt = f"""
        åŸºæ–¼è¾¯é¡Œã€Œ{topic}ã€èˆ‡åˆæ­¥çµè«–ã€Œ{verdict[:200]}...ã€ï¼Œè«‹åˆ—å‡º 3 å€‹å…·é«”çš„å»¶ä¼¸èª¿æŸ¥å•é¡Œï¼Œä»¥ä¾¿ç‚ºæŠ•è³‡è€…ç”Ÿæˆå¯åŸ·è¡Œçš„è¡Œå‹•å»ºè­°ã€‚
        å•é¡Œæ–¹å‘ç¯„ä¾‹ï¼š
        - å¦‚ä½•ä¸‹è¼‰æŸ ETF çš„æŒè‚¡æ¸…å–®ï¼Ÿ
        - æŸé¾é ­ä¼æ¥­çš„æœ€æ–°è‚¡æ¯æ”¯ä»˜ç‡æ˜¯å¤šå°‘ï¼Ÿ
        - å“ªè£¡å¯ä»¥æŸ¥çœ‹æœ€æ–°çš„ç”¢æ¥­é¢¨éšªå ±å‘Šï¼Ÿ
        
        è«‹ç›´æ¥åˆ—å‡ºå•é¡Œï¼Œæ¯è¡Œä¸€å€‹ã€‚
        """
        questions_text = await call_llm_async(plan_prompt, system_prompt="ä½ æ˜¯å°ˆæ¥­æŠ•è³‡é¡§å•ã€‚", context_tag=f"{debate_id}:Chairman:AdvicePlan")
        questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
        
        # 2. Execute Research (Smart Tool Selection)
        research_results = []
        
        # Prepare Tools (Prioritize High-Value Paid Tools)
        target_tool_names = [
            # Premium Paid Tools (ChinaTimes & Google)
            "chinatimes.news_search",
            "chinatimes.stock_fundamental",
            "chinatimes.financial_ratios",
            "google.search", # Paid/Official Google Search
            
            # Standard/Trial Tools (TEJ)
            # ï¼ˆå¯é¸ï¼‰TEJ å·¥å…·åƒ…åœ¨å•Ÿç”¨æ™‚åˆ—å‡º
            "tej.company_info" if Config.ENABLE_TEJ_TOOLS else None,
            "tej.stock_price" if Config.ENABLE_TEJ_TOOLS else None,
            
            # Fallback
            "searxng.search"
        ]
        
        research_tools = []
        for name in target_tool_names:
            try:
                tool_data = tool_registry.get_tool_data(name)
                # Ensure valid schema
                schema = tool_data.get('schema', {"type": "object", "properties": {}})
                if isinstance(schema, dict):
                     if "type" not in schema: schema["type"] = "object"
                
                desc = tool_data.get('description', '')
                if isinstance(desc, dict): desc = desc.get('description', '')
                
                research_tools.append({
                    "type": "function",
                    "function": {
                        "name": name,
                        "description": desc,
                        "parameters": schema
                    }
                })
            except:
                pass

        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()
        
        for q in questions[:3]: # Limit to 3 queries
            try:
                self._publish_log(debate_id, f"ğŸ” å»¶ä¼¸èª¿æŸ¥ï¼š{q}")
                
                # Ask LLM to pick the best tool for this question
                selection_prompt = f"""
                ä»»å‹™ï¼šå›ç­”å»¶ä¼¸èª¿æŸ¥å•é¡Œã€Œ{q}ã€ã€‚
                
                è«‹å„ªå…ˆä½¿ç”¨ã€ä»˜è²»é«˜éšå·¥å…·ã€‘(Google Search, ChinaTimes) ä¾†ç²å–æœ€æº–ç¢ºçš„è³‡è¨Šã€‚
                TEJ ç‚ºè©¦ç”¨ç‰ˆå·¥å…·ï¼Œåƒ…åœ¨å…¶ä»–å·¥å…·ç„¡æ³•ç²å–æ•¸æ“šæ™‚ä½œç‚ºè¼”åŠ©ä½¿ç”¨ã€‚

                å·¥å…·é¸æ“‡æŒ‡å—ï¼š
                - **æŸ¥æ¬Šå¨æ–°è/è¼¿è«–** -> `chinatimes.news_search` (é¦–é¸), `google.search` (ä»˜è²»é«˜ç²¾æº–)
                - **æŸ¥åŸºæœ¬é¢/è²¡å‹™æ•¸æ“š** -> `chinatimes.stock_fundamental`, `chinatimes.financial_ratios` (é¦–é¸)
                - **æŸ¥å»£æ³›å¤–éƒ¨è³‡è¨Š** -> `google.search`
                - **è¼”åŠ©æ•¸æ“š (è‹¥ä¸Šè¿°çš†ç„¡ä¸” TEJ å•Ÿç”¨)** -> `tej.company_info`, `tej.stock_price`ï¼›æœªå•Ÿç”¨å‰‡æ”¹ç”¨ `financial.get_verified_price`/`twse.stock_day`
                """
                
                response = await call_llm_async(
                    selection_prompt,
                    system_prompt="ä½ æ˜¯é¦–å¸­ç ”ç©¶å“¡ï¼Œè«‹å„ªå…ˆä½¿ç”¨é«˜æˆæœ¬ä½†é«˜æº–ç¢ºåº¦çš„ä»˜è²»å·¥å…· (ChinaTimes, Google)ã€‚",
                    tools=research_tools,
                    context_tag=f"{debate_id}:Chairman:ResearchExec"
                )
                
                # Parse Tool Call
                tool_output = "No tool used."
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    try:
                        tool_call = json.loads(json_match.group(0))
                        if "tool" in tool_call and "params" in tool_call:
                            t_name = tool_call["tool"]
                            t_params = tool_call["params"]
                            
                            self._publish_log(debate_id, f"ğŸ› ï¸ èª¿ç”¨å·¥å…· ({t_name})...")
                            res = await loop.run_in_executor(None, call_tool, t_name, t_params)
                            tool_output = str(res)[:800] # Increase limit for rich data
                    except Exception as ex:
                        tool_output = f"Tool execution error: {ex}"
                else:
                    # Fallback to search if no tool selected (sometimes LLM just talks)
                    if "search" not in response.lower(): # Avoid re-searching if it was a search intent
                         res = await loop.run_in_executor(None, call_tool, "searxng.search", {"q": q})
                         tool_output = str(res)[:500]

                research_results.append(f"Q: {q}\nResult: {tool_output}")
                
            except Exception as e:
                print(f"Extended research failed for '{q}': {e}")
        
        self._publish_log(debate_id, f"âœ… å»¶ä¼¸èª¿æŸ¥å®Œæˆï¼Œå…±ç²å¾— {len(research_results)} é …ç™¼ç¾ã€‚")
        return "\n\n".join(research_results) if research_results else "å»¶ä¼¸èª¿æŸ¥æœªç²å¾—é¡å¤–è³‡è¨Šã€‚"

    async def _generate_eda_summary(self, topic: str, debate_id: str, handcard: str = "") -> str:
        """
        ç”Ÿæˆ EDA è‡ªå‹•åˆ†ææ‘˜è¦ï¼ˆé€šéå·¥å…·ç³»çµ±ï¼‰ã€‚
        
        æµç¨‹ï¼š
        1. å¾ topic/handcard æå–è‚¡ç¥¨ä»£ç¢¼
        2. èª¿ç”¨ chairman.eda_analysis å·¥å…·
        3. è¿”å›åˆ†ææ‘˜è¦
        
        Returns:
            EDA åˆ†ææ‘˜è¦æ–‡æœ¬
        """
        self._publish_log(debate_id, "ğŸ“Š ä¸»å¸­æ­£åœ¨é€²è¡Œ EDA è‡ªå‹•åˆ†æ...")
        
        try:
            # Step 1: æå–è‚¡ç¥¨ä»£ç¢¼
            # [Fix] Also consider the decree (subject/code) if available in self
            stock_codes = []
            if hasattr(self, 'topic_decree') and self.topic_decree:
                code = self.topic_decree.get("code")
                if code and code != "Unknown":
                    # Clean code (e.g. 2330.TW -> 2330.TW)
                    if "." not in code and re.match(r'^\d{4}$', str(code)):
                        stock_codes.append(f"{code}.TW")
                    else:
                        stock_codes.append(str(code))
            
            if not stock_codes:
                stock_codes = self._extract_stock_codes_from_topic(topic, handcard)
            
            if not stock_codes:
                self._publish_log(debate_id, "âš ï¸ æœªèƒ½è­˜åˆ¥è‚¡ç¥¨ä»£ç¢¼ï¼Œè·³é EDA åˆ†æ")
                return "(æœªé€²è¡Œ EDA åˆ†æï¼šç„¡æ³•è­˜åˆ¥è‚¡ç¥¨ä»£ç¢¼)"
            
            # ä½¿ç”¨ç¬¬ä¸€å€‹è­˜åˆ¥åˆ°çš„ä»£ç¢¼
            symbol = stock_codes[0]
            self._publish_log(debate_id, f"ğŸ¯ è­˜åˆ¥åˆ°è‚¡ç¥¨ä»£ç¢¼: {symbol}")
            
            # Step 2: èª¿ç”¨ EDA å·¥å…·
            from worker.tool_invoker import call_tool
            
            params = {
                "symbol": symbol,
                "debate_id": debate_id,
                "lookback_days": 120
            }
            
            loop = asyncio.get_running_loop()
            result = await loop.run_in_executor(None, call_tool, "chairman.eda_analysis", params)
            
            # Step 3: è™•ç†çµæœ
            if result.get("success"):
                self._publish_log(debate_id, f"âœ… EDA åˆ†æå®Œæˆ")
                return result.get("summary", "(EDA åˆ†æå®Œæˆä½†ç„¡æ‘˜è¦)")
            else:
                error_msg = result.get("error", "Unknown error")
                self._publish_log(debate_id, f"âš ï¸ EDA åˆ†æå¤±æ•—: {error_msg}")
                return f"(EDA åˆ†æå¤±æ•—ï¼š{error_msg})"
            
        except Exception as e:
            self._publish_log(debate_id, f"âŒ EDA åˆ†æç•°å¸¸: {str(e)}")
            print(f"EDA generation error: {e}")
            import traceback
            traceback.print_exc()
            return "(EDA åˆ†æå¤±æ•—ï¼šç³»çµ±ç•°å¸¸)"
    
    def _extract_stock_codes_from_topic(self, topic: str, handcard: str = "") -> list:
        """å¾è¾¯è«–ä¸»é¡Œå’Œæ‰‹å¡ä¸­æå–è‚¡ç¥¨ä»£ç¢¼"""
        import re
        
        codes = []
        
        # å˜—è©¦å¾ topic æå–ï¼ˆæ ¼å¼ï¼š2330.TW, 8942, etc.ï¼‰
        pattern = r'\b(\d{4})(?:\.(?:TW|TWO))?\b'
        matches = re.findall(pattern, topic)
        for code in matches:
            if "." not in code:
                codes.append(f"{code}.TW")
            else:
                codes.append(code)
        
        # å˜—è©¦å¾ handcard æå–
        if handcard:
            handcard_str = json.dumps(handcard, ensure_ascii=False) if isinstance(handcard, (dict, list)) else str(handcard)
            matches = re.findall(pattern, handcard_str)
            for code in matches:
                if "." not in code:
                    codes.append(f"{code}.TW")
                else:
                    codes.append(code)
        
        # [Fallback] Check for common names mapping
        for name, code in STOCK_CODES.items():
            if name in topic:
                codes.append(f"{code}.TW" if "." not in code else code)
        
        # å»é‡
        return list(set(codes))

    async def summarize_debate(self, debate_id: str, topic: str, rounds_data: list, handcard: str = "") -> str:
        """
        å°æ•´å ´è¾¯è«–é€²è¡Œæœ€çµ‚ç¸½çµ (Async)ã€‚
        æ ¸å¿ƒé‚è¼¯ï¼š
        1. è³‡æ–™èšåˆï¼šæ­·å²ç´€éŒ„ + æ­£åæ–¹è«–é»
        2. æˆ°ç•¥å°é½Šï¼šæ³¨å…¥ Handcard æª¢æŸ¥æ˜¯å¦åé¡Œ
        3. è­‰æ“šå¯©æŸ¥ï¼šæ³¨å…¥ Verified EvidenceDoc
        4. EDA è‡ªå‹•åˆ†æï¼šç”Ÿæˆå¯¦è­‰æ•¸æ“šå ±è¡¨ (NEW)
        5. ç¶œåˆè©•åˆ¤ï¼šç”Ÿæˆçµæ§‹åŒ–å ±å‘Š
        6. å»¶ä¼¸å»ºè­°ï¼šç”Ÿæˆå¯åŸ·è¡Œè¡Œå‹•æŒ‡å—
        """
        print(f"Chairman '{self.name}' is making the final conclusion (Async).")
        
        # [NEW] Step 0: EDA è‡ªå‹•åˆ†æ
        eda_summary = await self._generate_eda_summary(topic, debate_id, handcard)
        
        # 1. Fetch Verified Evidence (SSOT)
        lc = EvidenceLifecycle(debate_id)
        verified_docs = lc.get_verified_evidence(limit=20) # Get top 20 verified facts
        
        evidence_summary = []
        for doc in verified_docs:
            # Format: [Tool: X] (Trust: 80) Content Summary
            content_str = json.dumps(doc.content, ensure_ascii=False)[:300]
            evidence_summary.append(f"- ã€å·²é©—è­‰è­‰æ“šã€‘(Tool: {doc.tool_name}): {content_str}")
        
        evidence_block = "\n".join(evidence_summary) if evidence_summary else "(æœ¬å ´è¾¯è«–ç„¡æœ‰æ•ˆé©—è­‰è­‰æ“š)"

        # 2. Build Debate Log
        summary_text = f"è¾¯é¡Œï¼š{topic}\n\n"
        for round_data in rounds_data:
            summary_text += f"--- ç¬¬ {round_data['round']} è¼ª ---\n"
            for key, value in round_data.items():
                if key == "round": continue
                summary_text += f"[{key}]: {str(value)[:500]}...\n" # Truncate for prompt context window
        
        # 3. Construct Structured Prompt for Verdict
        prompt = f"""
        è«‹æ’°å¯«æœ¬å ´è¾¯è«–çš„ã€æœ€çµ‚è£æ±ºå ±å‘Šã€‘ã€‚

        ### è¼¸å…¥è³‡æ–™
        1. **æˆ°ç•¥æ‰‹å¡ (Chairman's Handcard)**ï¼š
        {handcard if handcard else "(ç„¡æˆ°ç•¥æ‰‹å¡)"}

        2. **EDA å¯¦è­‰åˆ†æ (Automated Data Analysis)**ï¼š
           *é€™æ˜¯ç³»çµ±è‡ªå‹•ç”Ÿæˆçš„æ•¸æ“šåˆ†æå ±è¡¨ï¼ŒåŒ…å«é‡åŒ–æŒ‡æ¨™èˆ‡è¦–è¦ºåŒ–åœ–è¡¨ã€‚*
        {eda_summary}

        3. **æ ¸å¿ƒè­‰æ“šåº« (Verified Evidence)**ï¼š
           *é€™æ˜¯ç¶“éç³»çµ±æ ¸å¯¦çš„å–®ä¸€äº‹å¯¦ä¾†æº (SSOT)ï¼Œæ¬Šé‡æœ€é«˜ã€‚*
        {evidence_block}

        4. **è¾¯è«–éç¨‹æ‘˜è¦**ï¼š
        {summary_text}

        ### ä½ çš„ä»»å‹™
        è«‹æ‰®æ¼”å…¬æ­£ã€æ¬Šå¨çš„è¾¯è«–ä¸»å¸­ï¼Œç”Ÿæˆä¸€ä»½çµæ§‹æ¸…æ™°çš„ Markdown å ±å‘Šï¼ŒåŒ…å«ä»¥ä¸‹å››å€‹ç« ç¯€ï¼š

        ## 1. æˆ°ç•¥å°é½Šèˆ‡é›™æ–¹è§€é» (Strategic Alignment & Counterpoints)
        *   å›é¡§æˆ°ç•¥æ‰‹å¡ï¼šæ˜¯å¦èšç„¦æ ¸å¿ƒæˆ°å ´ï¼Ÿ
        *   **å¿…é ˆåŒ…å«åæ–¹è§€é»**ï¼šä¸èƒ½åƒ…å‘ˆç¾æ­£æ–¹ä¸»å¼µã€‚è«‹è£œå……è‡³å°‘ä¸€æ®µåæ–¹çš„æœ‰åŠ›è³ªç–‘ï¼ˆä¾‹å¦‚ï¼šæ–°å¢æˆåˆ†è‚¡çš„é¢¨éšªã€è‚¡æ¯ç¨€é‡‹æ•ˆæ‡‰ç­‰ï¼‰ï¼Œå®Œæ•´å‘ˆç¾äº¤é‹’ã€‚

        ## 2. è­‰æ“šæ•ˆåŠ›èˆ‡é‡åŒ–æŒ‡æ¨™ (Evidence & Quantification)
        *   **åå‘è­‰å½**ï¼šå€åˆ†å¼·è­‰æ“š (Tier 1/2) èˆ‡å¼±è­‰æ“š (Tier 3/4)ã€‚
        *   **é‡åŒ–é—œéµæŒ‡æ¨™**ï¼šé¿å…ç©ºæ³›å½¢å®¹ã€‚è«‹å¼•ç”¨è­‰æ“šä¸­çš„å…·é«”æ•¸å€¼ï¼Œä¾‹å¦‚ï¼š
            *   æ­·å¹´å¹³å‡è‚¡æ¯ç‡ (%)
            *   Beta å€¼ã€VaR æˆ–æ³¢å‹•ç‡ (%)
        *   **è³‡æ–™ä¾†æºå…·é«”åŒ–**ï¼šè‹¥å¼•ç”¨ã€Œå®˜æ–¹å…¬å‘Šã€æˆ–ã€Œè²¡å ±ã€ï¼Œ**å¿…é ˆ**çµ¦å‡ºå…·é«”ä¾†æºï¼ˆå¦‚ï¼šæ–‡ä»¶ç·¨è™Ÿã€å…·é«”æ—¥æœŸã€æˆ– Database IDï¼‰ï¼Œæ–¹ä¾¿é©—è­‰ã€‚

        ## 3. é‚è¼¯å°æ‡‰èˆ‡æ•æ„Ÿåº¦åˆ†æ (Logic & Sensitivity)
        *   **é æ¸¬é€æ˜åº¦**ï¼šé‡å°é—œéµé æ¸¬ï¼ˆå¦‚ç‡Ÿæ”¶æˆé•·ã€è‚¡åƒ¹ç›®æ¨™ï¼‰ï¼Œå¿…é ˆèªªæ˜**èƒŒå¾Œçš„å‡è¨­**èˆ‡**ä¾†æº**ã€‚
        *   **æ•æ„Ÿåº¦åˆ†æ (Sensitivity Analysis)**ï¼š
            *   è«‹æä¾›æƒ…å¢ƒæ¨¡æ“¬ï¼šã€Œè‹¥ [é—œéµè®Šæ•¸] è®Šå‹• X%ï¼Œå‰‡ [çµæœ] é æœŸè®Šå‹• Y%ã€ã€‚
            *   ç¯„ä¾‹ï¼šè‹¥åŠå°é«”åº«å­˜å»åŒ–å»¶å¾Œè‡³ Q3ï¼Œå‰‡é ä¼° EPS ä¸‹ä¿®è‡³ X å…ƒã€‚
        *   **é‚è¼¯æ”»é˜²**ï¼šè©•è¿°é›™æ–¹è«–é»çš„é‚è¼¯å°æ‡‰é—œä¿‚ï¼ˆChallenge & Responseï¼‰ï¼Œè€Œä¸åƒ…æ˜¯å„èªªå„è©±ã€‚

        ## 4. é¢¨éšªè©•ä¼°èˆ‡è­‰æ“šéˆæ¥ (Risk & Citations)
        *   **è­‰æ“šéˆæ¥ (Evidence Linking)**ï¼šæ¯å€‹é—œéµè«–é»å¾Œ**å¿…é ˆ**é™„ä¸Šè­‰æ“šä¾†æºç·¨è™Ÿæˆ–é€£çµï¼ˆä¾‹å¦‚ï¼š[Ref: TEJ-2023Q3] æˆ– [Ref: å®˜æ–¹å…¬å‘Š 2024-01-15]ï¼‰ã€‚
        *   **é¢¨éšªæŒ‡æ¨™çŸ©é™£**ï¼šè«‹ä»¥ Markdown è¡¨æ ¼å‘ˆç¾ï¼š
            | é¢¨éšªå› å­ | è§€æ¸¬æŒ‡æ¨™ (KPI) | è§¸ç™¼æ¢ä»¶ (Trigger) | è¡æ“Šç¨‹åº¦ (High/Med/Low) |
            | :--- | :--- | :--- | :--- |
            | ... | ... | ... | ... |

        ## 5. æœ€çµ‚è£æ±ºèˆ‡å¯¦è­‰æ‘˜è¦ (Verdict & Evidence Summary)
        *   **å‹è² å‚¾å‘**ï¼š(å¯é¸)
        *   **å…±è­˜äº‹å¯¦**ï¼šé›™æ–¹éƒ½èªåŒçš„å®¢è§€é»ã€‚

        ## 6. è¡Œå‹•å»ºè­°æ¡†æ¶ (Draft Actionable Framework)
        *   è«‹æ ¹æ“šè¾¯è«–çµæœï¼Œç‚ºæŠ•è³‡è€…ã€æ”¿ç­–åˆ¶å®šè€…èˆ‡ä¼æ¥­ä¸»åˆ†åˆ¥æ§‹æ€ 3 å€‹åˆæ­¥çš„æ“ä½œæ­¥é©Ÿèˆ‡å°æ‡‰çš„ KPIã€‚
        *   æ³¨æ„ï¼šä½ å¿…é ˆåœ¨æ­¤è™•ç‚ºæ¯å€‹ KPI æŒ‡å®šä¸€å€‹å»ºè­°çš„**é©—è­‰å·¥å…·**ï¼ˆä¾‹å¦‚ï¼štwse.stock_day, worldbank.*, fred.*, stockq.*ï¼‰ã€‚

        è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£å°ˆæ¥­ä¸”å…·å»ºè¨­æ€§ã€‚
        """
        # Call LLM for Initial Verdict
        initial_verdict = await call_llm_async(prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ï¼Œè«‹ä¾ç…§æŒ‡ç¤ºç”Ÿæˆçµæ§‹åŒ–çµæ¡ˆå ±å‘Šä¸¦åˆæ­¥æ§‹æ€è¡Œå‹•å»ºè­°ã€‚", context_tag=f"{debate_id}:Chairman:FinalVerdict")
        
        # 4. Extended Research for Actionable Advice
        extended_research_data = await self._conduct_extended_research(topic, initial_verdict, debate_id)
        
        # 5. Generate Final Actionable Advice (with Tool Use Capability)
        db = SessionLocal()
        try:
             advice_template = PromptService.get_prompt(db, "chairman.generate_advice")
             if not advice_template:
                 advice_template = "è«‹åŸºæ–¼è¾¯è«–çµè«–èˆ‡å»¶ä¼¸èª¿æŸ¥ï¼Œç‚ºç”¨æˆ¶ç”Ÿæˆå…·é«”çš„ã€Œä¸‹ä¸€æ­¥è¡Œå‹•å»ºè­°ã€ã€‚"
        finally:
             db.close()
             
        # Inject context and force tool use instructions
        advice_instruction = f"""
        ã€é‡è¦ä»»å‹™ã€‘åŸºæ–¼ä»¥ä¸‹è¾¯è«–çµè«–èˆ‡åˆæ­¥èª¿æŸ¥ï¼Œç”¢å‡ºæœ€çµ‚çš„ã€Œä¸‹ä¸€æ­¥è¡Œå‹•å»ºè­°ã€å ±å‘Šã€‚
        
        ### ä»»å‹™è¦æ±‚ï¼š
        1. **å¯¦ä½œèˆ‡é©—è­‰**ï¼šä½ å¿…é ˆå°‡è¾¯è«–ä¸­ç”¢å‡ºçš„åˆæ­¥å»ºè­°èˆ‡å¯¦éš›è’é›†åˆ°çš„å¯¦è­‰æ•¸æ“šï¼ˆInitial Research Dataï¼‰é€²è¡Œå¼·åˆ¶å°é½Šã€‚
        2. **æ•¸æ“šé©…å‹•å ±å‘Š**ï¼šåœ¨ã€Œå¯¦è­‰ç‹€æ…‹ (Evidence)ã€æ¬„ä½ä¸­ï¼Œå¿…é ˆå¼•ç”¨å…·é«”çš„æ•¸å€¼ï¼ˆä¾‹å¦‚ï¼š'2023 Inflation: 3.1%'ï¼‰ï¼Œåš´ç¦ä½¿ç”¨ã€Œå·²ç¢ºèªã€ç­‰æ¨¡ç³Šå­—çœ¼ã€‚
        3. **é‡åŒ– KPI**ï¼šæ‰€æœ‰æ“ä½œæ­¥é©Ÿå¿…é ˆå°æ‡‰ä¸€å€‹å¯åº¦é‡çš„æ•¸å€¼é–€æª»ã€‚

        ### å ±å‘Šçµæ§‹ (Markdown Table):
        | åƒèˆ‡è€… | æ“ä½œæ­¥é©Ÿ | å¯ä½¿ç”¨å·¥å…· / è³‡æº | å…·é«”æŒ‡æ¨™ (KPI) | å¯¦è­‰ç‹€æ…‹ (Evidence) |
        | :--- | :--- | :--- | :--- | :--- |
        | æŠ•è³‡è€… | 1. ... 2. ... | ... | ... | [å¯¦è­‰æ•¸æ“šæˆ–éºæ¼èªªæ˜] |
        | æ”¿ç­–åˆ¶å®šè€… | ... | ... | ... | ... |
        | ä¼æ¥­ä¸» | ... | ... | ... | ... |

        ### è¾¯è«–çµè«–
        {initial_verdict[-2000:]}
        
        ### åˆæ­¥ç ”ç©¶æ•¸æ“š
        {extended_research_data}
        
        ### è¦æ±‚
        1. ä½ å¿…é ˆé‡å°ã€ŒæŠ•è³‡è€…ã€ã€ã€Œæ”¿ç­–åˆ¶å®šè€…ã€èˆ‡ã€Œä¼æ¥­ä¸»ã€åˆ†åˆ¥ç”¢å‡ºåŒ…å« KPI çš„æ“ä½œè¡¨æ ¼ã€‚
        2. **å¿…é ˆèª¿ç”¨å·¥å…·**ï¼šè‹¥è¡¨æ ¼ä¸­æåˆ°çš„ KPI éœ€è¦å…·é«”æ•¸å€¼ï¼ˆå¦‚ï¼šæŸå…¬å¸çš„æ¯›åˆ©ç‡åŸºæº–ã€æŸåœ‹çš„æœ€æ–° CPIï¼‰ï¼Œè«‹ç«‹å³èª¿ç”¨å·¥å…·ç²å–ï¼Œä¸å¯ç·¨é€ ã€‚
        3. è«‹ç¢ºä¿å ±å‘Šçµæ§‹å®Œæ•´ï¼ŒåŒ…å«ï¼šè¡Œå‹•å»ºè­°è¡¨æ ¼ã€ç›£æ¸¬æ©Ÿåˆ¶ã€è³‡è¨Šå‚³éå»ºè­°ã€‚
        """
        
        # Equip chairman with strategic tools for final advice refinement
        final_research_tools = []
        target_tools = ["twse.stock_day", "chinatimes.financial_ratios", "fred.get_latest_release", "worldbank.global_inflation"]
        
        # Lazy import to avoid circular dependency
        from api.tool_registry import tool_registry
        
        for t_name in target_tools:
            try:
                t_data = tool_registry.get_tool_data(t_name)
                final_research_tools.append({
                    "type": "function",
                    "function": {
                        "name": t_name,
                        "description": t_data.get('description', ''),
                        "parameters": t_data.get('schema', {})
                    }
                })
            except: pass

        # Run a multi-step loop for advice generation to allow data gathering
        current_advice_prompt = advice_instruction
        actionable_advice = ""
        max_advice_steps = 3
        
        from worker.tool_invoker import call_tool
        loop = asyncio.get_running_loop()

        for step in range(max_advice_steps):
            self._publish_log(debate_id, f"ğŸ“ æ­£åœ¨ç²¾ç…‰è¡Œå‹•å»ºè­° (Step {step+1}/3)...")
            response = await call_llm_async(
                current_advice_prompt,
                system_prompt="ä½ æ˜¯é¦–å¸­æŠ•è³‡é¡§å•ï¼Œè² è²¬ç”¢å‡ºå…·å‚™é‡åŒ–æŒ‡æ¨™çš„è¡Œå‹•æŒ‡å—ã€‚",
                tools=final_research_tools,
                context_tag=f"{debate_id}:Chairman:ActionableAdvice"
            )
            
            # Check for tool calls
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    tool_call = json.loads(json_match.group(0))
                    if "tool" in tool_call and "params" in tool_call:
                        t_name = tool_call["tool"]
                        t_params = tool_call["params"]
                        self._publish_log(debate_id, f"ğŸ› ï¸ é¡§å•ç²¾ç…‰èª¿ç”¨å·¥å…·: {t_name}")
                        res = await loop.run_in_executor(None, call_tool, t_name, t_params)
                        current_advice_prompt += f"\n\nå·¥å…· {t_name} å›å‚³æ•¸æ“šï¼š\n{json.dumps(res, ensure_ascii=False)}\nè«‹ç¹¼çºŒå®Œæˆå»ºè­°å ±å‘Šã€‚"
                        continue
                except: pass
            
            # If no tool call, this is our final advice
            actionable_advice = response
            break
        
        # Combine
        final_conclusion = initial_verdict + "\n\n" + actionable_advice
        
        self._publish_log(debate_id, f"ğŸ¬ æœ€çµ‚è¾¯è«–ç¸½çµèˆ‡è¡Œå‹•å»ºè­°å®Œæˆã€‚")
        
        return final_conclusion

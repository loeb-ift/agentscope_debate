from typing import List, Dict, Any
from worker.chairman import Chairman
from agentscope.agent import AgentBase
import json
import re
import os
import sys
import yaml
import asyncio
import resource
from datetime import datetime
from worker.llm_utils import call_llm, call_llm_async
from worker.tool_config import get_tools_description, get_tools_examples, STOCK_CODES, CURRENT_DATE
from api.prompt_service import PromptService
from api.database import SessionLocal
from worker.memory import ReMePersonalLongTermMemory, ReMeTaskLongTermMemory, ReMeToolLongTermMemory
from api.tool_registry import tool_registry
from api.toolset_service import ToolSetService
from api.redis_client import get_redis_client

class DebateCycle:
    """
    ç®¡ç†æ•´ä¸ªè¾©è®ºå¾ªç¯ï¼ŒåŒ…æ‹¬ä¸»å¸­å¼•å¯¼ã€æ­£åæ–¹å‘è¨€å’Œæ€»ç»“ã€‚
    """

    def __init__(self, debate_id: str, topic: str, chairman: Chairman, teams: List[Dict], rounds: int):
        self.debate_id = debate_id
        self.topic = topic
        self.chairman = chairman
        self.teams = teams # List of dicts: [{"name": "...", "side": "...", "agents": [AgentBase...]}]
        self.rounds = rounds
        self.redis_client = get_redis_client()
        self.evidence_key = f"debate:{self.debate_id}:evidence"
        self.rounds_data = []
        self.analysis_result = {}
        self.history = []
        self.full_history = []  # å®Œæ•´æ­·å²è¨˜éŒ„ï¼ˆä¸å£“ç¸®ï¼Œç”¨æ–¼å ±å‘Šï¼‰
        self.compressed_history = "ç„¡"  # å­˜å„² LLM å£“ç¸®å¾Œçš„æ­·å²æ‘˜è¦
        self.agent_tools_map = {} # å­˜å„²æ¯å€‹ Agent é¸æ“‡çš„å·¥å…·åˆ—è¡¨

    def _get_memory_usage(self) -> str:
        """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        try:
            usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
            # MacOS: bytes, Linux: KB
            if sys.platform == 'darwin':
                return f"{usage / 1024 / 1024:.2f} MB"
            return f"{usage / 1024:.2f} MB"
        except Exception:
            return "N/A"

    def _publish_log(self, role: str, content: str):
        """
        ç™¼å¸ƒæ—¥èªŒåˆ° Redisï¼Œä¾›å‰ç«¯ SSE è¨‚é–±ã€‚
        """
        message = json.dumps({"role": role, "content": content}, ensure_ascii=False)
        self.redis_client.publish(f"debate:{self.debate_id}:log_stream", message)

    def _publish_progress(self, percentage: int, message: str, stage: str = "setup"):
        """
        ç™¼å¸ƒé€²åº¦æ›´æ–°äº‹ä»¶ï¼Œä¾›å‰ç«¯é¡¯ç¤ºé€²åº¦æ¢ã€‚
        """
        event_data = {
            "type": "progress_update",
            "progress": percentage,
            "message": message,
            "stage": stage,
            "timestamp": datetime.now().isoformat()
        }
        self.redis_client.publish(f"debate:{self.debate_id}:log_stream", json.dumps(event_data, ensure_ascii=False))

    def _save_report_to_file(self, conclusion: str, jury_report: str = None):
        """
        å°‡è¾¯è«–éç¨‹ä¿å­˜ç‚º Markdown æ–‡ä»¶ã€‚
        """
        import re
        from datetime import datetime
        
        report_dir = "data/replays"
        os.makedirs(report_dir, exist_ok=True)
        
        # æ¸…ç†é¡Œç›®ï¼Œç§»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦
        safe_topic = re.sub(r'[<>:"/\\|?*]', '', self.topic)
        safe_topic = safe_topic.replace(' ', '_')[:50]  # é™åˆ¶é•·åº¦
        
        # ç”Ÿæˆæ™‚é–“æˆ³ï¼ˆå¯è®€æ ¼å¼ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµ„åˆæª”åï¼šé¡Œç›®_æ™‚é–“.md
        filename = f"{safe_topic}_{timestamp}.md"
        filepath = os.path.join(report_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# è¾¯è«–å ±å‘Šï¼š{self.topic}\n\n")
            f.write(f"**ID**: {self.debate_id}\n")
            f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ† æœ€çµ‚çµè«–\n\n")
            f.write(f"{conclusion}\n\n")

            if jury_report:
                f.write("## âš–ï¸ è©•å¯©åœ˜è©•ä¼°å ±å‘Š\n\n")
                f.write(f"{jury_report}\n\n")
            
            f.write("## ğŸ“ è¾¯è«–éç¨‹è¨˜éŒ„\n\n")
            for item in self.full_history:
                role = item.get("role", "Unknown")
                content = item.get("content", "")
                f.write(f"### {role}\n")
                f.write(f"{content}\n\n")
                f.write("---\n\n")
                
        print(f"Report saved to {filepath}")

    def start(self) -> Dict[str, Any]:
        """
        å¼€å§‹è¾©è®ºå¾ªç¯ (Sync wrapper around async start).
        """
        return asyncio.run(self.start_async())

    async def start_async(self) -> Dict[str, Any]:
        """
        å¼€å§‹è¾©è®ºå¾ªç¯ (Async).
        """
        print(f"Debate '{self.debate_id}' has started. Mem: {self._get_memory_usage()}")
        self._publish_log("System", f"Debate '{self.debate_id}' has started.")
        self._publish_progress(5, "åˆå§‹åŒ–è¾¯è«–ç’°å¢ƒ...", "init")
        
        # 0. è³½å‰åˆ†æ
        # Check Task LTM for similar past debates
        with ReMeTaskLongTermMemory() as task_mem:
            similar_tasks = task_mem.retrieve_similar_tasks(self.topic)
            if similar_tasks:
                print(f"DEBUG: Found similar past debates:\n{similar_tasks}")
                self._publish_log("System", f"Found similar past debates:\n{similar_tasks}")

        self._publish_progress(10, "ä¸»å¸­æ­£åœ¨é€²è¡Œè³½å‰åˆ†æ...", "analysis")
        
        # Note: Chairman analysis is still sync for now as it's complex, but could be made async too.
        self.analysis_result = self.chairman.pre_debate_analysis(self.topic)
        summary = self.analysis_result.get('step5_summary', 'ç„¡')
        self.chairman.speak(f"è³½å‰åˆ†æå®Œæˆã€‚æˆ°ç•¥æ‘˜è¦ï¼š{summary}")
        self._publish_log("Chairman (Analysis)", f"è³½å‰åˆ†æå®Œæˆã€‚\næˆ°ç•¥æ‘˜è¦ï¼š{summary}")
        
        self._publish_progress(30, "åˆ†æå®Œæˆï¼Œæº–å‚™ Agent å·¥å…·...", "tool_selection")
        
        # 1. Agent å‹•æ…‹é¸æ“‡å·¥å…· (Initialization Phase)
        print("Agents are selecting their tools...")
        self._publish_log("System", "ğŸ¯ è¾¯è«–æº–å‚™éšæ®µï¼šå„ Agent æ­£åœ¨é¸æ“‡æœ€é©åˆçš„å·¥å…·...")
        
        total_agents = sum(len(team['agents']) for team in self.teams)
        if total_agents == 0:
            total_agents = 1 # Avoid division by zero
        
        # Run tool selection sequentially
        self._publish_log("System", f"ğŸš€ å•Ÿå‹• {total_agents} å€‹ Agent é †åºå·¥å…·é¸æ“‡...")
        
        agent_processed_count = 0
        for team in self.teams:
            side = team.get('side', 'neutral')
            for agent in team['agents']:
                 await self._agent_select_tools_async(agent, side)
                 agent_processed_count += 1
                 # Calculate progress from 30% to 90%
                 progress = 30 + int((agent_processed_count / total_agents) * 60)
                 self._publish_progress(progress, f"Agent {agent.name} å·¥å…·é…ç½®å®Œæˆ ({agent_processed_count}/{total_agents})", "tool_selection")

        self._publish_log("System", "âœ… æ‰€æœ‰ Agent å·¥å…·é¸æ“‡å®Œæˆã€‚")
        self._publish_progress(100, "æº–å‚™å°±ç·’ï¼Œè¾¯è«–é–‹å§‹ï¼", "start")

        
        for i in range(1, self.rounds + 1):
            print(f"--- Round {i} --- (Mem: {self._get_memory_usage()})")
            self._publish_log("System", f"--- Round {i} ---")
            round_result = await self._run_round_async(i)
            self.rounds_data.append(round_result)
        
        # 4. æœ€çµ‚ç¸½çµ
        handcard = self.analysis_result.get('step6_handcard') or self.analysis_result.get('step5_summary', 'ç„¡æ‰‹å¡')
        final_conclusion = self.chairman.summarize_debate(self.debate_id, self.topic, self.rounds_data, handcard)
        self._publish_log("Chairman (Conclusion)", final_conclusion)

        # 5. Jury è©•ä¼°
        jury_report = self._run_jury_evaluation(final_conclusion)

        # Record outcome to Task LTM
        with ReMeTaskLongTermMemory() as task_mem:
            task_mem.record(self.topic, final_conclusion)
            
        # Save to File (Markdown Report)
        self._save_report_to_file(final_conclusion, jury_report)

        print(f"Debate '{self.debate_id}' has ended.")
        self._publish_log("System", f"Debate '{self.debate_id}' has ended.")
        
        return {
            "topic": self.topic,
            "rounds_data": self.rounds_data,
            "analysis": self.analysis_result,
            "final_conclusion": final_conclusion,
            "jury_report": jury_report
        }

    def _run_jury_evaluation(self, final_conclusion: str) -> str:
        """
        åŸ·è¡Œè©•å¯©åœ˜ (Jury) è©•ä¼°ï¼Œç”Ÿæˆè©•åˆ†èˆ‡åˆ†æå ±å‘Šã€‚
        """
        print("Jury is evaluating the debate...")
        self._publish_log("System", "è©•å¯©åœ˜æ­£åœ¨é€²è¡Œæœ€çµ‚è©•ä¼°...")

        try:
            # Load Jury System Prompt (Priority: PromptService -> File -> Default)
            file_system_prompt = "ä½ æ˜¯è¾¯è«–è©•å¯©åœ˜ã€‚"
            try:
                with open("prompts/agents/jury.yaml", "r", encoding="utf-8") as f:
                    jury_config = yaml.safe_load(f)
                    file_system_prompt = jury_config.get("system_prompt", file_system_prompt)
            except Exception as e:
                print(f"Warning: Failed to load jury.yaml: {e}")

            db = SessionLocal()
            try:
                system_prompt = PromptService.get_prompt(db, "jury.system_prompt", default=file_system_prompt)
            finally:
                db.close()
            
            # æ§‹å»ºå®Œæ•´è¾¯è«–è¨˜éŒ„æ–‡å­—
            debate_log = ""
            for item in self.full_history:
                role = item.get("role", "Unknown")
                content = item.get("content", "")
                debate_log += f"[{role}]: {content}\n\n"
                
            debate_log += f"[Chairman Final Conclusion]: {final_conclusion}\n"

            user_prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹å®Œæ•´çš„è¾¯è«–è¨˜éŒ„ï¼Œç”Ÿæˆã€Œæœ€çµ‚è©•ä¼°å ±å‘Šã€ã€‚

**é‡è¦ï¼šè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«è©•ä¼°å ±å‘Šã€‚**

è¾¯é¡Œï¼š{self.topic}

è¾¯è«–è¨˜éŒ„ï¼š
{debate_log}

è«‹æŒ‰ç…§ System Prompt çš„è¦æ±‚ï¼Œè¼¸å‡ºåŒ…å«è©•åˆ†è¡¨èˆ‡æ–‡å­—åˆ†æçš„å ±å‘Šã€‚
"""
            # Call LLM
            jury_report = call_llm(user_prompt, system_prompt=system_prompt)
            
            self._publish_log("Jury", jury_report)
            print("Jury evaluation completed.")
            return jury_report
            
        except Exception as e:
            error_msg = f"Jury evaluation failed: {str(e)}"
            print(error_msg)
            self._publish_log("System", error_msg)
            return error_msg

    def _update_team_score(self, side: str, delta: float, reason: str):
        """
        æ›´æ–°åœ˜éšŠè©•åˆ†ä¸¦æ¨é€é€šçŸ¥ã€‚
        """
        score_key = f"debate:{self.debate_id}:scores"
        # Initial scores if not set (default 100)
        if not self.redis_client.hexists(score_key, side):
            self.redis_client.hset(score_key, side, 100.0)
        
        new_score = self.redis_client.hincrbyfloat(score_key, side, delta)
        
        # Publish score update event
        event_data = {
            "type": "score_update",
            "side": side,
            "new_score": new_score,
            "delta": delta,
            "reason": reason,
            "timestamp": datetime.now().isoformat()
        }
        self.redis_client.publish(f"debate:{self.debate_id}:log_stream", json.dumps(event_data, ensure_ascii=False))
        self._publish_log("System (Score)", f"âš–ï¸ ã€{side}ã€‘åˆ†æ•¸è®Šæ›´: {delta} ({reason}) => ç•¶å‰åˆ†æ•¸: {new_score}")

    def _neutral_verification_turn(self, agent: AgentBase, team_name: str, round_num: int) -> str:
        return asyncio.run(self._neutral_verification_turn_async(agent, team_name, round_num))

    async def _neutral_verification_turn_async(self, agent: AgentBase, team_name: str, round_num: int) -> str:
        """
        ä¸­ç«‹æ–¹çš„ç‰¹æ®Šå›åˆï¼šæ ¸å¯¦è­‰æ“šä¸¦é€²è¡Œè©•åˆ† (Async)ã€‚
        """
        print(f"Neutral Agent {agent.name} is verifying evidence...")
        self._publish_log(f"{agent.name} (Verification)", "ğŸ” æ­£åœ¨å¯©æŸ¥å„æ–¹æå‡ºçš„è­‰æ“šé€²è¡Œæ ¸å¯¦...")

        # 1. Fetch unverified evidence from Redis
        all_evidence = [json.loads(e) for e in self.redis_client.lrange(self.evidence_key, 0, -1)]
        # Filter: from current round (or previous), not neutral, not verified
        target_evidence = [e for e in all_evidence if e.get('side') != 'neutral' and not e.get('verified', False)]
        
        verification_report = ""
        
        if not target_evidence:
            return await self._agent_turn_async(agent, 'neutral', round_num) # Fallback to normal turn if no evidence

        # 2. Verify each evidence (Limit to 1-2 to save time/cost)
        for ev in target_evidence[:2]:
            tool_name = ev.get('tool')
            params = ev.get('params')
            original_result = ev.get('result')
            provider_side = ev.get('side', 'Unknown')
            
            self._publish_log(f"{agent.name} (Verification)", f"æ­£åœ¨æ ¸å¯¦ {provider_side} æ–¹ä½¿ç”¨çš„å·¥å…·: {tool_name}...")
            
            try:
                # Re-execute tool
                from worker import tasks
                # Execute sync tool in thread pool
                loop = asyncio.get_running_loop()
                verify_result = await loop.run_in_executor(None, tasks.execute_tool, tool_name, params)
                
                # Simple comparison (Equality check might be too strict for some dynamic data, but good for now)
                # Ideally, we ask LLM to compare.
                
                # Construct verification prompt
                comparison_prompt = f"""
è«‹æ¯”è¼ƒä»¥ä¸‹å…©æ¬¡å·¥å…·èª¿ç”¨çš„çµæœï¼Œåˆ¤æ–·æ˜¯å¦ä¸€è‡´ï¼Œä»¥åŠåŸå§‹å¼•ç”¨æ˜¯å¦æº–ç¢ºã€‚

å·¥å…·ï¼š{tool_name}
åƒæ•¸ï¼š{params}

åŸå§‹çµæœï¼ˆç”± {provider_side} æ–¹æä¾›ï¼‰ï¼š
{str(original_result)[:1000]}...

æ ¸å¯¦çµæœï¼ˆç”±ä¸­ç«‹æ–¹é‡æ–°åŸ·è¡Œï¼‰ï¼š
{str(verify_result)[:1000]}...

è«‹è¼¸å‡º JSONï¼š
{{
    "consistent": true/false,
    "score_penalty": 0 åˆ° -10,
    "comment": "ç°¡çŸ­è©•èª"
}}
"""
                # Call LLM for judgement
                judge_response = await call_llm_async(comparison_prompt, system_prompt="ä½ æ˜¯å…¬æ­£çš„æ•¸æ“šæ ¸å¯¦å“¡ã€‚")
                
                # Parse JSON
                try:
                    # Robust JSON extraction
                    json_match = re.search(r'\{.*\}', judge_response, re.DOTALL)
                    if json_match:
                        judge_json = json.loads(json_match.group(0))
                        
                        consistent = judge_json.get('consistent', True)
                        penalty = judge_json.get('score_penalty', 0)
                        comment = judge_json.get('comment', '')
                        
                        if consistent:
                            verification_report += f"- âœ… æ ¸å¯¦é€šé ({tool_name}): æ•¸æ“šä¸€è‡´ã€‚\n"
                        else:
                            verification_report += f"- âŒ æ ¸å¯¦å¤±æ•— ({tool_name}): {comment} (æ‰£åˆ†: {penalty})\n"
                            if penalty < 0:
                                self._update_team_score(provider_side, float(penalty), f"è­‰æ“šæ ¸å¯¦å¤±æ•—: {comment}")
                    else:
                        verification_report += f"- âš ï¸ ç„¡æ³•åˆ¤æ–· ({tool_name}): {judge_response[:50]}\n"

                except Exception as e:
                    print(f"Verification judgment parsing error: {e}")
                    verification_report += f"- âš ï¸ æ ¸å¯¦åˆ¤è®€éŒ¯èª¤ ({tool_name})\n"

            except Exception as e:
                verification_report += f"- âš ï¸ å·¥å…·é‡è·‘å¤±æ•— ({tool_name}): {e}\n"

        # 3. Generate Speech based on verification
        final_prompt = f"""
ä½ æ˜¯ä¸­ç«‹æ–¹è¾¯æ‰‹ {agent.name}ã€‚
é€™æ˜¯ç¬¬ {round_num} è¼ªã€‚

ä½ çš„æ ¸å¿ƒä»»å‹™æ˜¯æ“”ä»»ã€Œäº‹å¯¦æŸ¥æ ¸è€…ã€ã€‚
ä½ å‰›å‰›å°å…¶ä»–åœ˜éšŠçš„è­‰æ“šé€²è¡Œäº†æ ¸å¯¦ï¼Œçµæœå¦‚ä¸‹ï¼š
{verification_report}

è«‹åŸºæ–¼ä»¥ä¸Šæ ¸å¯¦çµæœï¼Œç™¼è¡¨ä½ çš„è§€é»ã€‚
1. å¦‚æœæœ‰æ ¸å¯¦å¤±æ•—ï¼Œåš´å²æŒ‡å‡ºä¸¦æ‰¹è©•ã€‚
2. å¦‚æœæ•¸æ“šéƒ½å¯é ï¼Œå‰‡é‡å°è¾¯é¡Œç™¼è¡¨ä¸­ç«‹åˆ†æã€‚
3. ä¿æŒå®¢è§€ã€å…¬æ­£ã€‚
"""
        response = await call_llm_async(final_prompt, system_prompt=f"ä½ æ˜¯ {agent.name}ï¼Œå…¬æ­£çš„ç¬¬ä¸‰æ–¹ã€‚")
        return response

    def _run_round(self, round_num: int) -> Dict[str, Any]:
         """Sync wrapper around async _run_round_async"""
         return asyncio.run(self._run_round_async(round_num))

    async def _run_round_async(self, round_num: int) -> Dict[str, Any]:
        """
        è¿è¡Œä¸€è½®è¾©è®º (Async, Parallel Team Execution).
        åŒ…å«ï¼šå„åœ˜éšŠå…§éƒ¨è¨è«– (Parallel) -> åœ˜éšŠç¸½çµ -> ä¸»å¸­å½™æ•´èˆ‡ä¸‹ä¸€è¼ªå¼•å°
        """
        from worker import tasks # Lazy import to avoid circular dependency
        
        # 1. ä¸»å¸­å¼•å¯¼
        opening = f"ç°åœ¨é–‹å§‹ç¬¬ {round_num} è¼ªè¾¯è«–ã€‚"
        self.chairman.speak(opening)
        self.history.append({"role": "Chairman", "content": opening})
        self.full_history.append({"role": "Chairman", "content": opening})
        self._publish_log("Chairman", opening)

        # 2. å„åœ˜éšŠå…§éƒ¨è¾¯è«–èˆ‡ç¸½çµ (Intra-Team Debate & Summary)
        round_team_summaries = {}
        
        total_teams = len(self.teams)
        
        # Run all teams sequentially
        self._publish_log("System", f"ğŸš€ å•Ÿå‹• {total_teams} éšŠé †åºè¨è«–...")
        team_results = []
        for team in self.teams:
            result = await self._process_team_deliberation(team, round_num)
            team_results.append(result)
        
        # Process results from all teams
        for team_result in team_results:
            team_name = team_result['name']
            team_summary = team_result['summary']
            discussion_log = team_result['log']
            
            round_team_summaries[team_name] = team_summary
            
            # Store history (Note: Order might be mixed in real-time logs, but here we append block by block)
            # Ideally, we want to interleave them in history based on timestamp, but for simplicity:
            for item in discussion_log:
                 self.history.append(item)
                 self.full_history.append(item)
            
            self.history.append({"role": f"{team_name} Summary", "content": team_summary})
            self.full_history.append({"role": f"{team_name} Summary", "content": team_summary})
            
        # 3. ä¸»å¸­å½™æ•´èˆ‡ä¸‹ä¸€è¼ªæ–¹å‘
        handcard = self.analysis_result.get('step6_handcard') or self.analysis_result.get('step5_summary', 'ç„¡æ‰‹å¡')
        
        # è‡¨æ™‚æ–¹æ¡ˆï¼šå°‡ team_summaries å¯«å…¥ Redis evidenceï¼Œè®“ä¸»å¸­è®€å–åˆ°
        for t_name, t_summary in round_team_summaries.items():
            summary_evidence = {
                "role": f"{t_name} Summary",
                "content": t_summary,
                "type": "team_summary"
            }
            self.redis_client.rpush(self.evidence_key, json.dumps(summary_evidence, ensure_ascii=False))
            
        next_direction = self.chairman.summarize_round(self.debate_id, round_num, handcard=handcard)
        self._publish_log("Chairman", f"Round {round_num} summary completed. Next Direction: {next_direction}")
        
        # å°‡ä¸‹ä¸€è¼ªæ–¹å‘åŠ å…¥æ­·å²ï¼Œä¾›ä¸‹ä¸€è¼ª Agent åƒè€ƒ
        self.history.append({"role": "Chairman (Next Direction)", "content": next_direction})
        self.full_history.append({"role": "Chairman (Next Direction)", "content": next_direction})
        
        return {
            "round": round_num,
            "team_summaries": round_team_summaries,
            "next_direction": next_direction
        }
        
    async def _process_team_deliberation(self, team: Dict, round_num: int) -> Dict[str, Any]:
        """
        Process a single team's deliberation asynchronously.
        """
        team_name = team['name']
        team_side = team.get('side', 'neutral')
        team_agents = team['agents']
        total_agents_in_team = len(team_agents)
        
        team_icon = "ğŸŸ¦" if team_side == "pro" else "ğŸŸ¥" if team_side == "con" else "ğŸŸ©"
        self._publish_log("System", f"{team_icon} ã€{team_name}ã€‘é–‹å§‹å…§éƒ¨è¨è«–...")
        
        team_discussion_log_text = [] # For summary generation
        team_history_entries = [] # For returning to main thread
        
        # Within a team, agents might still need to speak in order, OR parallel?
        # Usually debate implies responding to each other.
        # However, "Intra-Team Debate" in this simplified version is just each agent speaking once.
        # We can make agents within a team parallel too!
        
        agent_results = []
        for agent in team_agents:
            if team_side == "neutral":
                 content = await self._neutral_verification_turn_async(agent, team_name, round_num)
            else:
                 content = await self._agent_turn_async(agent, team_name, round_num)
            agent_results.append(content)
        
        for idx, (agent, content) in enumerate(zip(team_agents, agent_results)):
             role_label = f"{team_name} - {agent.name}"
             
             entry = {"role": role_label, "content": content}
             team_history_entries.append(entry)
             team_discussion_log_text.append(f"{agent.name}: {content}")
             
             # Publish individual log (Note: Might arrive out of order visually if not carefully handled on frontend,
             # but here we publish as soon as done)
             self._publish_log(role_label, content)

        # ç”Ÿæˆåœ˜éšŠå…±è­˜èˆ‡åˆ†æ­§ç¸½çµ
        self._publish_log("System", f"ğŸ“Š {team_name} æ­£åœ¨æ•´ç†åœ˜éšŠå…±è­˜...")
        team_summary = await self._generate_team_summary_async(team_name, team_discussion_log_text)
        self._publish_log(f"{team_name} (Summary)", team_summary)
        
        return {
            "name": team_name,
            "summary": team_summary,
            "log": team_history_entries
        }

    def _generate_team_summary(self, team_name: str, discussion_log: List[str]) -> str:
         return asyncio.run(self._generate_team_summary_async(team_name, discussion_log))

    async def _generate_team_summary_async(self, team_name: str, discussion_log: List[str]) -> str:
        """
        ç”Ÿæˆåœ˜éšŠå…§éƒ¨çš„å…±è­˜èˆ‡åˆ†æ­§ç¸½çµ (Async).
        """
        discussion_text = "\n".join(discussion_log)
        
        db = SessionLocal()
        try:
            default_system = "ä½ æ˜¯ {team_name} çš„è¨˜éŒ„å“¡ã€‚è«‹æ ¹æ“šåœ˜éšŠæˆå“¡çš„ç™¼è¨€ï¼Œç¸½çµæœ¬è¼ªè¨è«–çš„ã€Œå…±åŒè§€é»ã€èˆ‡ã€Œå…§éƒ¨åˆ†æ­§ã€ã€‚"
            sys_template = PromptService.get_prompt(db, "debate.team_summary_system", default=default_system)
            system_prompt = sys_template.format(team_name=team_name)

            default_user = "è¨è«–è¨˜éŒ„ï¼š\n{discussion_text}\n\nè«‹è¼¸å‡ºç¸½çµï¼š"
            user_template = PromptService.get_prompt(db, "debate.team_summary_user", default=default_user)
            user_prompt = user_template.format(discussion_text=discussion_text)
        finally:
            db.close()
            
        return await call_llm_async(user_prompt, system_prompt=system_prompt)

    def _agent_select_tools(self, agent: AgentBase, side: str):
         """Sync wrapper for backward compatibility"""
         return asyncio.run(self._agent_select_tools_async(agent, side))

    async def _agent_select_tools_async(self, agent: AgentBase, side: str):
        """
        Agent åœ¨è¾¯è«–é–‹å§‹å‰å‹•æ…‹é¸æ“‡æœ€é©åˆçš„å·¥å…· (Async)ã€‚
        """
        db = SessionLocal()
        try:
            # ç²å–è©² Agent å¯ç”¨çš„å·¥å…·åˆ—è¡¨ (å¾ DB ToolSet)
            agent_id = getattr(agent, 'id', None)
            if not agent_id:
                # å˜—è©¦å¾ DB æŸ¥æ‰¾
                db_agent = db.query(models.Agent).filter(models.Agent.name == agent.name).first()
                if db_agent:
                    agent_id = db_agent.id
            
            if agent_id:
                available_tools_list = ToolSetService.get_agent_available_tools(db, agent_id)
            else:
                # Fallback: å¦‚æœæ‰¾ä¸åˆ° IDï¼Œåˆ—å‡ºæ‰€æœ‰å·¥å…· (æˆ–åƒ… Global)
                all_tools_dict = tool_registry.list()
                available_tools_list = []
                for name, data in all_tools_dict.items():
                    available_tools_list.append({"name": name, "description": data['description']})

            tools_list_text = "\n".join([f"- {t['name']}: {t['description']}" for t in available_tools_list])
        finally:
            db.close()
        
        # DB session for prompt
        db = SessionLocal()
        try:
            default_system = "ä½ æ˜¯ {agent_name}ï¼Œå³å°‡ä»£è¡¨{side}åƒåŠ é—œæ–¼ã€Œ{topic}ã€çš„è¾¯è«–ã€‚ä½ çš„ä»»å‹™æ˜¯å¾å¯ç”¨å·¥å…·åº«ä¸­é¸æ“‡å°ä½ æœ€æœ‰å¹«åŠ©çš„å·¥å…·ã€‚"
            sys_template = PromptService.get_prompt(db, "debate.tool_selection_system", default=default_system)
            system_prompt = sys_template.format(agent_name=agent.name, side=side, topic=self.topic)

            default_user = """
å¯ç”¨å·¥å…·åˆ—è¡¨ï¼š
{tools_list_text}

è«‹åˆ†æè¾¯é¡Œèˆ‡ä½ çš„ç«‹å ´ï¼Œé¸æ“‡ 3-5 å€‹æœ€é—œéµçš„å·¥å…·ã€‚
**é‡è¦ï¼š** è«‹ä»”ç´°æŸ¥çœ‹æ¯å€‹å·¥å…·æè¿°ä¸­çš„ **Schema**ï¼Œé¸æ“‡é‚£äº›è¼¸å…¥/è¼¸å‡ºæ¬„ä½æœ€ç¬¦åˆä½ æ•¸æ“šéœ€æ±‚çš„å·¥å…·ã€‚ä¸è¦åƒ…æ†‘å·¥å…·åç¨±çŒœæ¸¬åŠŸèƒ½ã€‚

è«‹ç›´æ¥è¿”å›å·¥å…·åç¨±çš„ JSON åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š["searxng.search", "tej.stock_price"]
ä¸è¦è¼¸å‡ºå…¶ä»–æ–‡å­—ã€‚
"""
            user_template = PromptService.get_prompt(db, "debate.tool_selection_user", default=default_user)
            user_prompt = user_template.format(tools_list_text=tools_list_text)
        finally:
            db.close()

        try:
            # Async LLM Call
            response = await call_llm_async(user_prompt, system_prompt=system_prompt)
            
            # å˜—è©¦è§£æ JSON
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                selected_tools = json.loads(json_match.group(0))
                self.agent_tools_map[agent.name] = selected_tools
                print(f"Agent {agent.name} selected tools: {selected_tools}")
                
                # æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨é¡¯ç¤º
                tools_display = "\n".join([f"  â€¢ {tool}" for tool in selected_tools])
                self._publish_log(f"{agent.name} (Setup)", f"âœ… å·²é¸æ“‡ {len(selected_tools)} å€‹å·¥å…·ï¼š\n{tools_display}")
            else:
                print(f"Agent {agent.name} failed to select tools (no JSON), using defaults.")
                self.agent_tools_map[agent.name] = []
                self._publish_log(f"{agent.name} (Setup)", "âš ï¸ å·¥å…·é¸æ“‡å¤±æ•—ï¼Œå°‡ä½¿ç”¨é»˜èªé…ç½®")
        except Exception as e:
            print(f"Error in tool selection for {agent.name}: {e}")
            self.agent_tools_map[agent.name] = []
            self._publish_log(f"{agent.name} (Setup)", f"âŒ å·¥å…·é¸æ“‡éŒ¯èª¤: {str(e)}")

    def _compress_history(self):
        """
        ä½¿ç”¨ LLM å£“ç¸®èˆŠçš„è¾¯è«–æ­·å² (Compression ç­–ç•¥)ã€‚
        """
        keep_recent = 3
        # åªæœ‰ç•¶ç´¯ç©çš„æ­·å²è¨Šæ¯è¶…éä¸€å®šæ•¸é‡æ™‚æ‰è§¸ç™¼å£“ç¸®
        if len(self.history) <= keep_recent + 2:
            return

        # æå–éœ€è¦å£“ç¸®çš„èˆŠè¨Šæ¯
        to_compress = self.history[:-keep_recent]
        # æ›´æ–° self.historyï¼Œåªä¿ç•™æœ€è¿‘çš„è¨Šæ¯
        self.history = self.history[-keep_recent:]
        
        # æ§‹å»ºå£“ç¸® Prompt
        conversation_text = "\n".join([f"{item.get('role')}: {str(item.get('content'))[:500]}" for item in to_compress])
        
        db = SessionLocal()
        try:
            default_system = "ä½ æ˜¯è¾¯è«–è¨˜éŒ„å“¡ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡å°è©±æ­·å²å£“ç¸®æˆç°¡ç·´çš„æ‘˜è¦ï¼Œä¿ç•™é—œéµè«–é»å’Œæ•¸æ“šï¼Œå»é™¤å†—é¤˜å…§å®¹ã€‚"
            sys_template = PromptService.get_prompt(db, "debate.history_compression_system", default=default_system)
            system_prompt = sys_template

            default_user = "è«‹å£“ç¸®ä»¥ä¸‹å°è©±æ­·å²ï¼ˆæ¥çºŒä¹‹å‰çš„æ‘˜è¦ï¼‰ï¼š\n\nä¹‹å‰çš„æ‘˜è¦ï¼š{compressed_history}\n\næ–°çš„å°è©±ï¼š\n{conversation_text}"
            user_template = PromptService.get_prompt(db, "debate.history_compression_user", default=default_user)
            user_prompt = user_template.format(compressed_history=self.compressed_history, conversation_text=conversation_text)
        finally:
            db.close()
        
        try:
            summary = call_llm(user_prompt, system_prompt=system_prompt)
            if summary:
                self.compressed_history = summary
                print(f"DEBUG: History compressed. New summary length: {len(summary)}")
                self._publish_log("System", "å·²å°èˆŠçš„è¾¯è«–æ­·å²é€²è¡Œå£“ç¸®è™•ç†ã€‚")
        except Exception as e:
            print(f"WARNING: History compression failed: {e}")

    def _get_compact_history(self, max_length=2000) -> str:
        """
        ç²å–å„ªåŒ–å¾Œçš„è¾¯è«–æ­·å² (ReMe ç­–ç•¥ï¼šCompression + Smart Retention)
        """
        # 1. å˜—è©¦è§¸ç™¼å£“ç¸® (Compression)
        self._compress_history()
        
        # 2. æ§‹å»ºè¿‘æœŸå®Œæ•´æ­·å² (Smart Retention)
        active_history_text = ""
        for item in self.history:
            content = item.get("content", "")
            # å°æ–¼è¿‘æœŸçš„ Tool Outputï¼Œå¦‚æœå¤ªé•·ä¹Ÿé€²è¡Œç°¡å–®æˆªæ–· (Compaction)
            if len(content) > 800:
                content = content[:300] + "...(ç•¥)..." + content[-300:]
            active_history_text += f"{item.get('role')}: {content}\n\n"
        
        full_text = f"ã€æ—©æœŸè¾¯è«–æ‘˜è¦ã€‘ï¼š\n{self.compressed_history}\n\nã€è¿‘æœŸå°è©±ã€‘ï¼š\n{active_history_text}"
        return full_text

    def _agent_turn(self, agent: AgentBase, side: str, round_num: int) -> str:
        return asyncio.run(self._agent_turn_async(agent, side, round_num))

    async def _agent_turn_async(self, agent: AgentBase, side: str, round_num: int) -> str:
        """
        åŸ·è¡Œå–®å€‹ Agent çš„å›åˆï¼šæ€è€ƒ -> å·¥å…· -> ç™¼è¨€ (Async)
        """
        print(f"Agent {agent.name} ({side}) is thinking...")
        self._publish_log(f"{agent.name} (Thinking)", f"{agent.name} æ­£åœ¨æ€è€ƒä¸¦æ±ºå®šä½¿ç”¨çš„ç­–ç•¥...")
        
        # æ§‹å»º Prompt - ä½¿ç”¨ Agent è‡ªå·±é¸æ“‡çš„å·¥å…·
        selected_tool_names = self.agent_tools_map.get(agent.name, [])
        
        # å¦‚æœæœ‰é¸æ“‡ï¼Œå‰‡åªé¡¯ç¤ºé¸æ“‡çš„å·¥å…·ï¼›å¦å‰‡é¡¯ç¤ºæ‰€æœ‰ã€Œå¯ç”¨ã€çš„å·¥å…·
        if selected_tool_names:
            filtered_tools = {}
            for name in selected_tool_names:
                try:
                    # Using get_tool_data ensures lazy tools are loaded and schema is available
                    # Assuming version 'v1' for now as selection doesn't specify version
                    tool_data = tool_registry.get_tool_data(name)
                    filtered_tools[name] = tool_data
                except Exception as e:
                    print(f"Warning: Selected tool '{name}' not found or failed to load: {e}")

            if not filtered_tools:
                 # å¦‚æœé¸æ“‡ç„¡æ•ˆï¼Œå›é€€åˆ°é¡¯ç¤ºè©² Agent æ‰€æœ‰å¯ç”¨çš„å·¥å…· (ToolSet)
                 tools_desc = get_tools_description()
            else:
                 tools_desc = "ä½ å·²é¸æ“‡ä¸¦æ¿€æ´»ä»¥ä¸‹å·¥å…·ï¼š\n" + "\n".join([f"### {name}\n{data['description']}\nSchema: {json.dumps(data['schema'], ensure_ascii=False)}" for name, data in filtered_tools.items()])
        else:
            # å¦‚æœæ²’æœ‰é¸æ“‡ï¼ˆä¾‹å¦‚åˆå§‹åŒ–å¤±æ•—ï¼‰ï¼Œé¡¯ç¤ºæ‰€æœ‰å·¥å…·
            tools_desc = get_tools_description()
            
        # Append Meta-Tool Description
        tools_desc += "\n\n### reset_equipped_tools\nDescription: å‹•æ…‹åˆ‡æ›å·¥å…·çµ„ (active tool group)ã€‚\nParameters: {'group': 'browser_use' | 'financial_data' | 'basic'}"
        
        # Append Chairman Intervention Tool (Virtual)
        tools_desc += "\n\n### call_chairman\nDescription: ç•¶ä½ ç™¼ç¾è¾¯é¡Œè³‡è¨Šåš´é‡ä¸è¶³ï¼ˆå¦‚ç¼ºä¹èƒŒæ™¯ã€å®šç¾©ä¸æ¸…ï¼‰ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆåˆ†ææ™‚ï¼Œè«‹ä½¿ç”¨æ­¤å·¥å…·é€šçŸ¥ä¸»å¸­ä»‹å…¥è™•ç†ã€‚\nParameters: {'reason': 'èªªæ˜å…·é«”ç¼ºå°‘ä»€éº¼è³‡è¨Šæˆ–èƒŒæ™¯'}"

        tools_examples = get_tools_examples() # Examples æš«æ™‚ä¿æŒå…¨é›†ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥éæ¿¾
        
        # Retrieve Tool LTM hints
        tool_hints = ""
        with ReMeToolLongTermMemory() as tool_mem:
            tool_hints = tool_mem.retrieve(self.topic) # Use topic as context for now
            if tool_hints:
                tools_examples += f"\n\n**éå¾€æˆåŠŸå·¥å…·èª¿ç”¨åƒè€ƒ (ReMe Tool LTM)**:\n{tool_hints}"

        history_text = self._get_compact_history()
        
        db = SessionLocal()
        try:
            # 1. System Prompt Construction
            # Strategy: Combine Agent's Custom Persona with System's Operational Rules
            
            # A. Operational Rules (Mandatory)
            operational_rules = """
**ç³»çµ±æ“ä½œè¦ç¯„ (Operational Rules)**ï¼š
1. **å·¥å…·å„ªå…ˆ**ï¼šå¿…é ˆå…ˆä½¿ç”¨å·¥å…·ç²å–çœŸå¯¦æ•¸æ“šï¼Œå†ç™¼è¡¨è«–é»ã€‚
2. **ç²¾æº–èª¿ç”¨**ï¼šä»”ç´°é–±è®€å·¥å…· Schemaã€‚TEJ å·¥å…·å¿…é ˆæä¾› `coid` (å…¬å¸ä»£ç¢¼)ï¼Œè«‹åƒè€ƒã€é‡è¦å¸¸æ•¸ã€‘ã€‚
3. **æ™‚é–“æ„ŸçŸ¥**ï¼šå·¥å…·æ—¥æœŸåƒæ•¸ (start_date/end_date) å¿…é ˆæ ¹æ“šå•é¡Œæ™‚é–“å‹•æ…‹è¨ˆç®—ï¼Œä¸å¯çœç•¥ã€‚
4. **è¼¸å‡ºæ ¼å¼**ï¼šèª¿ç”¨å·¥å…·æ™‚ï¼Œå¿…é ˆè¼¸å‡ºç´” JSONï¼Œä¸è¦åŒ…å« Markdown ä»£ç¢¼å¡Šæˆ–å…¶ä»–æ–‡å­—ã€‚
"""
            
            # B. Agent Persona (Custom or Default)
            custom_prompt = getattr(agent, 'system_prompt', '').strip()
            if custom_prompt:
                persona_section = f"""
**ä½ çš„è§’è‰²è¨­å®š (Persona)**ï¼š
{custom_prompt}

ä½ æ˜¯ {agent.name}ï¼Œä»£è¡¨ {side} æ–¹ã€‚
è¾¯é¡Œï¼š{self.topic}
"""
            else:
                persona_section = f"""
**ä½ çš„è§’è‰²è¨­å®š (Persona)**ï¼š
ä½ æ˜¯ {agent.name}ï¼Œä»£è¡¨ {side} æ–¹ã€‚
è¾¯é¡Œï¼š{self.topic}
"""

            # Combine
            default_system = f"{persona_section}\n{operational_rules}"
            
            # Try to get override from DB, but prioritize constructing it dynamically if not found
            # Note: We don't use PromptService here for the full prompt to avoid losing the dynamic custom_prompt.
            # However, if we want to allow DB overrides of the *structure*, we could.
            # For now, let's stick to the dynamic construction to ensure custom prompts work.
            system_prompt = default_system

            # 2. User Prompt (Tool Instruction)
            default_user = """
è¿™æ˜¯ç¬¬ {round_num} è¼ªè¾¯è«–ã€‚

**è¾¯è«–æ­·å²æ‘˜è¦**ï¼š
{history_text}

**ä¸»å¸­æˆ°ç•¥æ‘˜è¦**ï¼š{chairman_summary}

**èƒŒæ™¯è³‡è¨Š**ï¼š
- ç•¶å‰æ—¥æœŸï¼š{current_date}
- è¾¯é¡Œæ¶‰åŠï¼š2024 å¹´ Q4ï¼ˆ2024-10-01 è‡³ 2024-12-31ï¼‰
- ä½ éœ€è¦æŸ¥è©¢ 2024 å¹´çš„å¯¦éš›è‚¡åƒ¹æ•¸æ“šé€²è¡Œæ¯”è¼ƒ

**é‡è¦å¸¸æ•¸**ï¼š
{stock_codes}

**ç¬¬ä¸€æ­¥ï¼šå¿…é ˆå…ˆèª¿ç”¨å·¥å…·ç²å–æ•¸æ“š**

{tools_desc}

{tools_examples}

**è«‹ç¾åœ¨å°±èª¿ç”¨å·¥å…·**ï¼ˆåªè¼¸å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
"""
            user_template = PromptService.get_prompt(db, "debater.tool_instruction", default=default_user)
            user_prompt = user_template.format(
                round_num=round_num,
                history_text=history_text,
                chairman_summary=self.analysis_result.get('step5_summary', 'ç„¡'),
                current_date=CURRENT_DATE,
                stock_codes=chr(10).join([f"- {name}: {code}" for name, code in STOCK_CODES.items()]),
                tools_desc=tools_desc,
                tools_examples=tools_examples
            )
        finally:
            db.close()
        
        # Async LLM Call
        response = await call_llm_async(user_prompt, system_prompt=system_prompt)
        print(f"DEBUG: Agent {agent.name} raw response: {response[:500]}")  # åªå°å‰ 500 å­—ç¬¦

        # Retry æ©Ÿåˆ¶
        if not response:
            print(f"WARNING: Empty response from {agent.name}, retrying with simple prompt...")
            retry_prompt = f"è«‹é‡å°è¾¯é¡Œã€Œ{self.topic}ã€ç™¼è¡¨ä½ çš„{side}è«–é»ã€‚è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
            response = await call_llm_async(retry_prompt, system_prompt=system_prompt)
            print(f"DEBUG: Agent {agent.name} retry response: {response[:500]}")
        
        # æª¢æŸ¥æ˜¯å¦èª¿ç”¨å·¥å…·
        print(f"DEBUG: Checking for tool call in response (length: {len(response)})")
        
        try:
            # å˜—è©¦æå– JSON (æ”¯æ´ç´” JSON æˆ–æ··åœ¨æ–‡å­—ä¸­çš„ JSON)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                print(f"DEBUG: Extracted JSON string: {json_str[:200]}...")
                
                try:
                    tool_call = json.loads(json_str)
                    print(f"DEBUG: Successfully parsed JSON: {tool_call}")
                except json.JSONDecodeError as e:
                    print(f"WARNING: JSON decode failed: {e}")
                    print(f"DEBUG: Failed JSON string: {json_str}")
                    # å¦‚æœè§£æå¤±æ•—ï¼Œè¦–ç‚ºæ™®é€šæ–‡æœ¬å›æ‡‰
                    return response

                if isinstance(tool_call, dict) and "error" in tool_call:
                    error_msg = tool_call["error"]
                    print(f"WARNING: Agent returned error JSON: {error_msg}")
                    # è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼šå¼·åˆ¶å¼•å°ä½¿ç”¨æœå°‹å·¥å…·
                    if "æœªæä¾›å…·é«”ä»»å‹™" in str(error_msg) or "ç„¡æ³•ç¢ºå®š" in str(error_msg):
                        retry_prompt = f"""ä½ ä¼¼ä¹ä¸ç¢ºå®šè©²åšä»€éº¼ã€‚è«‹ä½œç‚º{side}æ–¹ï¼Œé‡å°è¾¯é¡Œã€Œ{self.topic}ã€é€²è¡Œäº‹å¯¦æŸ¥æ ¸ã€‚
è«‹å‹™å¿…èª¿ç”¨ `searxng.search` å·¥å…·ï¼ŒæŸ¥è©¢ç›¸é—œæ–°èæˆ–æ•¸æ“šã€‚
ä¾‹ï¼š{{"tool": "searxng.search", "params": {{"q": "{self.topic} çˆ­è­°é»"}}}}"""
                        print(f"DEBUG: Auto-retrying with guidance...")
                        return await call_llm_async(retry_prompt, system_prompt=system_prompt)
                
                if isinstance(tool_call, dict) and "tool" in tool_call and "params" in tool_call:
                    tool_name = tool_call["tool"]
                    params = tool_call["params"]
                    
                    # --- Meta-Tool: reset_equipped_tools ---
                    if tool_name == "reset_equipped_tools":
                        target_group = params.get("group", "basic")
                        print(f"âš™ï¸ Agent {agent.name} is resetting equipped tools to group: {target_group}")
                        self._publish_log(f"{agent.name} (Meta-Tool)", f"Resetting tools to group: {target_group}")
                        
                        # Update Agent's tool selection
                        # Get all tools in this group
                        group_tools = tool_registry.list(groups=[target_group])
                        self.agent_tools_map[agent.name] = list(group_tools.keys())
                        
                        # Re-prompt agent with new tools (Recursive call or loop? Loop is safer)
                        return await self._agent_turn_async(agent, side, round_num)

                    # --- Meta-Tool: call_chairman (Intervention) ---
                    if tool_name == "call_chairman":
                        reason = params.get("reason", "æœªèªªæ˜åŸå› ")
                        print(f"ğŸš¨ Agent {agent.name} is calling Chairman for help: {reason}")
                        self._publish_log(f"{agent.name} (SOS)", f"è«‹æ±‚ä¸»å¸­ä»‹å…¥ï¼š{reason}")

                        # 1. Chairman generates clarification
                        chairman_prompt = f"""
Agent {agent.name} ({side}æ–¹) åœ¨åˆ†æè¾¯é¡Œã€Œ{self.topic}ã€æ™‚é‡åˆ°å›°é›£ã€‚
å›å ±åŸå› ï¼š{reason}

è«‹æ ¹æ“šä½ çš„è³½å‰åˆ†ææ‰‹å¡ï¼ˆHandcardï¼‰ï¼Œç‚ºè©² Agent æä¾›ä¸€æ®µã€ŒèƒŒæ™¯è£œå……èªªæ˜ã€æˆ–ã€Œå¼•å°æŒ‡ç¤ºã€ã€‚
è«‹ä¿æŒç°¡çŸ­ã€æ˜ç¢ºï¼Œå¹«åŠ©å®ƒç¹¼çºŒé€²è¡Œåˆ†æã€‚
"""
                        clarification = await call_llm_async(chairman_prompt, system_prompt="ä½ æ˜¯è¾¯è«–ä¸»å¸­ã€‚ä½ çš„ä»»å‹™æ˜¯å”åŠ©é‡åˆ°å›°é›£çš„è¾¯æ‰‹ï¼Œæä¾›å¿…è¦çš„èƒŒæ™¯è³‡è¨Šå¼•å°ï¼Œä½†ä¸è¦ç›´æ¥æ›¿å®ƒè¾¯è«–ã€‚")
                        
                        self._publish_log("Chairman (Intervention)", f"ä¸»å¸­å›æ‡‰ï¼š{clarification}")
                        print(f"ğŸ’¡ Chairman provided clarification: {clarification}")

                        # 2. Retry Agent Turn with Clarification
                        # We need to inject this clarification into the next prompt.
                        # For simplicity, we can recurse but append the clarification to history or a special context.
                        # Here we append it to history temporarily for the retry.
                        
                        intervention_msg = {"role": "Chairman (Intervention)", "content": f"é‡å°ä½ çš„å•é¡Œã€Œ{reason}ã€ï¼Œè£œå……èªªæ˜å¦‚ä¸‹ï¼š\n{clarification}\n\nè«‹æ ¹æ“šæ­¤è³‡è¨Šç¹¼çºŒä½ çš„åˆ†æã€‚"}
                        self.history.append(intervention_msg)
                        
                        # Retry
                        return await self._agent_turn_async(agent, side, round_num)
                    
                    print(f"âœ“ Agent {agent.name} is calling tool: {tool_name}")
                    print(f"âœ“ Tool parameters: {json.dumps(params, ensure_ascii=False)}")
                    self._publish_log(f"{agent.name} (Tool)", f"Calling {tool_name} with {params}")
                    
                    # åŸ·è¡Œå·¥å…· (æ”¯æ´æ‰€æœ‰è¨»å†Šçš„å·¥å…·)
                    # Note: Tools might still be sync (requests). We run them in executor to avoid blocking loop.
                    try:
                        print(f"DEBUG: Executing tool {tool_name}...")
                        from worker import tasks  # Lazy import to avoid circular dependency
                        
                        # Execute sync tool in thread pool
                        loop = asyncio.get_running_loop()
                        tool_result = await loop.run_in_executor(None, tasks.execute_tool, tool_name, params)
                        
                        print(f"âœ“ Tool execution successful")
                        print(f"DEBUG: Tool result preview: {str(tool_result)[:300]}...")
                        self._publish_log(f"{agent.name} (Tool)", f"å·¥å…· {tool_name} åŸ·è¡ŒæˆåŠŸç²å–æ•¸æ“šã€‚")
                        
                        # Record successful tool usage to Tool LTM
                        with ReMeToolLongTermMemory() as tool_mem:
                            tool_mem.record(
                                intent=f"Debate on {self.topic}",
                                tool_name=tool_name,
                                params=params,
                                result=tool_result,
                                success=True
                            )
                        
                        # --- Evidence Recording for Neutral Verification ---
                        # Record full evidence details to Redis for verification
                        evidence_entry = {
                            "role": f"{agent.name} ({side})",
                            "agent_name": agent.name,
                            "side": side,
                            "tool": tool_name,
                            "params": params,
                            "result": tool_result,
                            "timestamp": datetime.now().isoformat(),
                            "verified": False,
                            "round": round_num
                        }
                        self.redis_client.rpush(self.evidence_key, json.dumps(evidence_entry, ensure_ascii=False))
                        # ------------------------------------------------

                    except Exception as e:
                        tool_result = {"error": f"Tool execution error: {str(e)}"}
                        print(f"ERROR: Tool {tool_name} execution failed: {e}")
                        
                        # Record failed tool usage
                        with ReMeToolLongTermMemory() as tool_mem:
                            tool_mem.record(
                                intent=f"Debate on {self.topic}",
                                tool_name=tool_name,
                                params=params,
                                result=str(e),
                                success=False
                            )
                    
                    # å°‡å·¥å…·çµæœåé¥‹çµ¦ Agent ç”Ÿæˆæœ€çµ‚ç™¼è¨€
                    prompt_with_tool = f"""å·¥å…· {tool_name} çš„åŸ·è¡Œçµæœï¼š
{json.dumps(tool_result, ensure_ascii=False, indent=2)}

è«‹æ ¹æ“šé€™äº›è­‰æ“šé€²è¡Œç™¼è¨€ã€‚è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä¸¦å¼•ç”¨å…·é«”æ•¸æ“šã€‚"""
                    
                    print(f"DEBUG: Asking agent to generate final response based on tool result...")
                    final_response = await call_llm_async(prompt_with_tool, system_prompt=system_prompt)
                    print(f"DEBUG: Agent {agent.name} final response: {final_response[:500]}...")
                    return final_response
                else:
                    print(f"DEBUG: JSON parsed but missing 'tool' or 'params' keys: {tool_call.keys()}")
            else:
                print(f"DEBUG: No JSON structure found in response")
        except Exception as e:
            print(f"ERROR: Tool execution parsing failed: {e}")
            import traceback
            traceback.print_exc()
        
        return response

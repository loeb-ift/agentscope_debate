import asyncio
import json
import logging
import sys
import os
import re
from datetime import datetime
from typing import List, Dict, Any

# Add current directory to path
sys.path.append(os.getcwd())

from worker.llm_utils import call_llm
from api.tool_registry import tool_registry
from worker.dynamic_tool_loader import DynamicToolLoader, OpenAPIToolAdapter

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("ReActSimulator")

def ensure_chinatimes_tool():
    """Ensure ChinaTimes tool is registered for testing"""
    tool_name = "news.search_chinatimes"
    tools = tool_registry.list_tools()
    
    if any(k.startswith(tool_name) for k in tools.keys()):
        logger.info(f"âœ… å·¥å…· {tool_name} å·²å­˜åœ¨æ–¼ Registryã€‚")
        return

    logger.info(f"âš ï¸ å·¥å…· {tool_name} æœªæ‰¾åˆ°ï¼Œæ­£åœ¨é€²è¡Œæ‰‹å‹•è¨»å†Š...")
    openapi_spec = {
        "openapi": "3.0.0",
        "paths": {
            "/search/content": {
                "get": {
                    "summary": "æœå°‹ä¸­æ™‚æ–°èç¶²",
                    "description": "æ ¹æ“šé—œéµå­—æœå°‹ç›¸é—œæ–°è",
                    "operationId": "search_news",
                    "parameters": [
                        {
                            "name": "Keyword",
                            "in": "query",
                            "required": True,
                            "schema": {"type": "string"},
                            "description": "æœå°‹é—œéµå­— (ä¾‹å¦‚: å…¬å¸åç¨±, è­°é¡Œ)"
                        }
                    ]
                }
            }
        }
    }

    adapter = OpenAPIToolAdapter({
        'name': tool_name,
        'version': 'v1',
        'description': 'æœå°‹ä¸­æ™‚æ–°èç¶² (ChinaTimes) çš„æœ€æ–°æ–°èå ±å°',
        'openapi_spec': openapi_spec,
        'base_url': 'https://es.chinatimes.com',
        'auth_type': 'none',
        'provider': 'chinatimes',
        'timeout': 15
    })

    tool_registry.register(adapter, group="news")
    logger.info(f"âœ… {tool_name} æ‰‹å‹•è¨»å†Šå®Œæˆã€‚")

class MockAgent:
    def __init__(self, name):
        self.name = name
        self.system_prompt = ""

class ReActSimulator:
    def __init__(self):
        self.debate_id = "sim_react_001"
        self.topic = "åˆ†æä¸­å…‰é›»(5371)çš„è¿‘æœŸå¸‚å ´å‹•æ…‹èˆ‡æ“´å» è¨ˆç•«"
        self.agent_tools_map = {}
        self.debug_trace = []
        self.tool_stats = {"count": 0, "total_time": 0.0}
        
    def _publish_log(self, role, content):
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] [{role}]: {str(content)[:200]}...")

    async def call_llm_async_sim(self, prompt, system_prompt, context_tag, tools=None):
        """Wrapper to call sync call_llm in executor"""
        loop = asyncio.get_running_loop()
        logger.info(f"ğŸ¤– Calling LLM (Tag: {context_tag})...")
        
        # Use verify_debate_tool_usage.py style call
        return await loop.run_in_executor(
            None, 
            lambda: call_llm(prompt=prompt, system_prompt=system_prompt, tools=tools)
        )

    async def _agent_turn_async(self, agent: MockAgent, side: str, round_num: int) -> str:
        """
        Simulated _agent_turn_async from worker/debate_cycle.py
        """
        print(f"\n=== Agent {agent.name} ({side}) Turn Starts ===")
        self._publish_log(f"{agent.name} (Thinking)", "æ­£åœ¨æ€è€ƒä¸¦æ±ºå®šä½¿ç”¨çš„ç­–ç•¥...")
        
        # 1. æ§‹å»º Tool Definitions (Ollama Format)
        selected_tool_names = self.agent_tools_map.get(agent.name, [])
        ollama_tools = []
        filtered_tools = {}

        if selected_tool_names:
            for name in selected_tool_names:
                try:
                    tool_data = tool_registry.get_tool_data(name)
                    filtered_tools[name] = tool_data
                    
                    params_schema = tool_data.get('schema', {"type": "object", "properties": {}})
                    desc = tool_data.get('description', '')
                    if isinstance(desc, dict): desc = desc.get('description', '')

                    ollama_tools.append({
                        "type": "function",
                        "function": {
                            "name": name,
                            "description": desc,
                            "parameters": params_schema
                        }
                    })
                except Exception as e:
                    logger.warning(f"Tool setup error for {name}: {e}")

            tools_desc = "ä½ å·²é¸æ“‡ä¸¦æ¿€æ´»ä»¥ä¸‹å·¥å…·ï¼š\n" + "\n".join([f"- {name}: {data['description']}" for name, data in filtered_tools.items()])
        else:
            tools_desc = "No tools selected."

        # 2. Construct Prompts (Simulated PromptService)
        system_prompt = f"""
ä½ æ˜¯ {agent.name}ï¼Œä»£è¡¨ {side} æ–¹ã€‚
è¾¯é¡Œï¼š{self.topic}
ç«‹å ´ï¼š{side}

# Operational Rules
System Rules: Use tools first to gather evidence. Do NOT fabricate data.
If you have enough information, output your final argument as text.
"""
        
        user_prompt = f"""
Current Round: {round_num}
Instructions: 
1. Analyze the topic: "{self.topic}"
2. Use available tools to gather data if needed.
3. Available Tools: 
{tools_desc}

è«‹é–‹å§‹åˆ†æã€‚
"""

        # 3. ReAct Loop
        max_steps = 3
        current_step = 0
        current_prompt = user_prompt
        collected_evidence = []
        
        while current_step < max_steps:
            current_step += 1
            print(f"\n--- Step {current_step}/{max_steps} ---")
            
            # Call LLM
            response = await self.call_llm_async_sim(
                current_prompt,
                system_prompt=system_prompt,
                context_tag=f"{self.debate_id}:{agent.name}",
                tools=ollama_tools if ollama_tools else None
            )
            
            print(f"LLM Response (Raw Preview): {response[:200]}")

            # Check for tool call
            is_tool_call = False
            tool_call_data = None
            
            try:
                # å˜—è©¦æå– JSON
                # ç°¡å–®æ­£å‰‡æå–ç¬¬ä¸€å€‹ JSON å°è±¡
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    possible_json = json_match.group(0)
                    try:
                        parsed = json.loads(possible_json)
                        if "tool" in parsed and "params" in parsed:
                            tool_call_data = parsed
                            is_tool_call = True
                    except:
                        pass
            except Exception as e:
                logger.error(f"JSON Parsing Error: {e}")

            if not is_tool_call:
                # No tool call -> Assume final speech
                self._publish_log(f"{agent.name} (Speech)", f"Final Response: {response}")
                return response

            # Execute Tool
            tool_name = tool_call_data["tool"]
            params = tool_call_data["params"]
            
            self._publish_log(f"{agent.name} (Tool)", f"Calling {tool_name} with {params}")
            
            try:
                # Direct invocation (Bypassing Hippocampus/Redis for simulation)
                tool_instance = tool_registry.get_tool_data(tool_name)["instance"]
                
                start_time = datetime.now()
                # Run sync tool in executor to mimic async behavior
                loop = asyncio.get_running_loop()
                tool_result = await loop.run_in_executor(None, lambda: tool_instance.invoke(**params))
                duration = (datetime.now() - start_time).total_seconds()
                
                # Format result
                result_str = json.dumps(tool_result, ensure_ascii=False)
                if len(result_str) > 500:
                    result_preview = result_str[:500] + "... (truncated)"
                else:
                    result_preview = result_str
                
                self._publish_log(f"{agent.name} (Result)", f"Result: {result_preview}")
                
                # Append to evidence
                collected_evidence.append(f"ã€Tool: {tool_name}ã€‘\nParams: {params}\nResult: {result_preview}")
                
                # Update Prompt for next step
                current_prompt = f"""
Tool '{tool_name}' execution result:
{result_str}

ã€ç³»çµ±æç¤ºã€‘
1. è«‹æª¢æŸ¥ä¸Šè¿°çµæœã€‚
2. å¦‚æœè­‰æ“šå·²è¶³å¤ æ”¯æŒä½ çš„è«–é»ï¼Œè«‹è¼¸å‡ºæœ€çµ‚ç™¼è¨€ï¼ˆç´”æ–‡å­—ï¼‰ã€‚
3. å¦‚æœéœ€è¦æ›´å¤šè³‡è¨Šï¼Œè«‹ç¹¼çºŒèª¿ç”¨å·¥å…·ã€‚
"""
                
            except Exception as e:
                error_msg = f"Tool execution error: {str(e)}"
                logger.error(error_msg)
                current_prompt = f"ç³»çµ±éŒ¯èª¤ï¼š{error_msg}\nè«‹é‡æ–°é¸æ“‡æœ‰æ•ˆçš„å·¥å…·æˆ–ç™¼è¡¨è¨€è«–ã€‚"

        # Force Conclusion if loop ends
        print("Max steps reached. Forcing conclusion.")
        evidence_text = "\n".join(collected_evidence)
        final_prompt = f"""
ã€ç³»çµ±å¼·åˆ¶æŒ‡ä»¤ã€‘
ä½ å·²ç¶“é”åˆ°å·¥å…·èª¿ç”¨æ¬¡æ•¸ä¸Šé™ã€‚
è«‹æ ¹æ“šä½ ç›®å‰å·²è’é›†åˆ°çš„è­‰æ“šï¼Œç«‹å³ç™¼è¡¨ä½ çš„æœ¬è¼ªè«–é»ã€‚

**å·²è’é›†çš„è­‰æ“š**ï¼š
{evidence_text}

è«‹ç›´æ¥è¼¸å‡ºä½ çš„è¾¯è«–ç™¼è¨€ï¼ˆç´”æ–‡å­—ï¼‰ï¼š
"""
        final_response = await self.call_llm_async_sim(
            final_prompt,
            system_prompt=system_prompt,
            context_tag=f"{self.debate_id}:{agent.name}:Force",
            tools=None
        )
        self._publish_log(f"{agent.name} (ForceSpeech)", final_response)
        return final_response

async def main():
    print("ğŸš€ Starting ReAct Simulation...")
    
    # 1. Setup Environment
    sim = ReActSimulator()
    ensure_chinatimes_tool()
    
    # 2. Setup Agent
    agent = MockAgent("Analyst_Wang")
    
    # Assign the new tool to the agent
    sim.agent_tools_map[agent.name] = ["news.search_chinatimes"]
    
    # 3. Run Turn
    print(f"\nTopic: {sim.topic}")
    print(f"Agent: {agent.name}")
    print(f"Tools: {sim.agent_tools_map[agent.name]}")
    
    final_output = await sim._agent_turn_async(agent, "pro", 1)
    
    print("\n=== Simulation Complete ===")
    print("Final Output Length:", len(final_output))

if __name__ == "__main__":
    asyncio.run(main())
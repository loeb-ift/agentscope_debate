import requests
import re
import json
import time
from typing import List, Dict, Any

class ChinatimesProbe:
    """ä¸­æ™‚æ–°èç¶²è‚¡å¸‚ API æ¢æ¸¬å™¨"""
    
    BASE_URL = "https://wantrich.chinatimes.com"
    STOCK_ID = "2330"
    
    # æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚º
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Referer": "https://wantrich.chinatimes.com/tw-market/listed/stock/2330"
    }

    def __init__(self, stock_id="2330"):
        self.stock_id = stock_id
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)

    def log(self, level: str, message: str, **kwargs):
        """ç°¡å–®çš„çµæ§‹åŒ–æ—¥èªŒè¼¸å‡º"""
        context = " ".join([f"{k}={v}" for k, v in kwargs.items()])
        print(f"[{level}] {message} {context}")

    def probe_html_source(self):
        """æ­¥é©Ÿ 1 & 2: è«‹æ±‚ä¸»é é¢ä¸¦å°‹æ‰¾ JavaScript ä¸­çš„ API ç—•è·¡"""
        target_url = f"{self.BASE_URL}/tw-market/listed/stock/{self.stock_id}"
        self.log("INFO", "é–‹å§‹åˆ†æä¸»é é¢ HTML", url=target_url)

        try:
            response = self.session.get(target_url, timeout=10)
            self.log("INFO", "ä¸»é é¢è«‹æ±‚å®Œæˆ", status_code=response.status_code, size=len(response.text))

            if response.status_code != 200:
                self.log("ERROR", "ä¸»é é¢è«‹æ±‚å¤±æ•—")
                return

            # Regex æ¨¡å¼: å°‹æ‰¾å¸¸è¦‹çš„ API èª¿ç”¨ç‰¹å¾µ
            # 1. å°‹æ‰¾ /api/ é–‹é ­çš„å­—ä¸²
            # 2. å°‹æ‰¾ .json çµå°¾çš„å­—ä¸²
            # 3. å°‹æ‰¾ ajax/fetch èª¿ç”¨
            patterns = [
                r'["\'](/api/[^"\']+)["\']',  # '/api/...'
                r'["\']([^"\']+\.json)["\']',  # '... .json'
                r'url\s*:\s*["\']([^"\']+)["\']', # url: '...'
            ]

            found_urls = set()
            for pattern in patterns:
                matches = re.findall(pattern, response.text)
                for match in matches:
                    # éæ¿¾æ‰æ˜é¡¯ä¸æ˜¯ API çš„è³‡æº (css, png ç­‰)
                    if not any(ext in match.lower() for ext in ['.css', '.png', '.jpg', '.js', '.gif']):
                        found_urls.add(match)

            if found_urls:
                print("\nğŸ” [Regex] åœ¨ HTML åŸå§‹ç¢¼ä¸­ç™¼ç¾çš„æ½›åœ¨ URL:")
                for url in sorted(found_urls):
                    print(f"   - {url}")
            else:
                self.log("WARN", "Regex æœªç™¼ç¾æ˜é¡¯çš„ API è·¯å¾‘")

        except Exception as e:
            self.log("ERROR", "ä¸»é é¢åˆ†æç™¼ç”ŸéŒ¯èª¤", error=str(e))

    def probe_subpages(self):
        """æ­¥é©Ÿ 3: å°‹æ‰¾å­é é¢é€£çµ"""
        target_url = f"{self.BASE_URL}/tw-market/listed/stock/{self.stock_id}"
        print(f"\nğŸ”— [Links] åˆ†æä¸»é é¢å°èˆªé€£çµ: {target_url}")
        
        try:
            response = self.session.get(target_url, timeout=10)
            if response.status_code != 200:
                return

            # å°‹æ‰¾æ‰€æœ‰åŒ…å« stock_id çš„é€£çµ
            # ä¾‹å¦‚: href="/tw-market/listed/stock/2330/financial"
            pattern = fr'href=["\'](/tw-market/listed/stock/{self.stock_id}/[^"\']+)["\']'
            links = set(re.findall(pattern, response.text))
            
            if links:
                for link in sorted(links):
                    print(f"   - å­é é¢: {link}")
                    # é †ä¾¿çŒœæ¸¬é€™äº›å­é é¢æ˜¯å¦å°æ‡‰ API
                    # ä¾‹å¦‚ /tw-market/.../financial -> /api/stock/stk_tw/.../financial
                    suffix = link.split('/')[-1]
                    self.fuzz_targets.append(suffix)
            else:
                self.log("WARN", "æœªç™¼ç¾æ˜é¡¯çš„å­é é¢é€£çµ")
                
        except Exception as e:
            self.log("ERROR", "å­é é¢åˆ†æå¤±æ•—", error=str(e))

    def probe_api_endpoints(self):
        """æ­¥é©Ÿ 4: ä¸»å‹•æ¢æ¸¬ API (åŒ…å« Fuzzing)"""
        print("\nğŸš€ [Probe] é–‹å§‹ä¸»å‹•æ¢æ¸¬ API è·¯å¾‘...")
        
        # åŸºç¤å·²çŸ¥æ¨¡å¼
        base_api = f"/api/stock/stk_tw/{self.stock_id}"
        
        # Fuzzing åˆ—è¡¨
        candidates = [
            "k1", "k", "quote", "realtime", "info", "detail", # åŸºæœ¬
            "financial", "finance", "revenue", "eps", # è²¡å ±
            "dividend", "yield", # è‚¡åˆ©
            "institutional", "3insti", "trust", "foreign", # ç±Œç¢¼
            "margin", "short", # ä¿¡ç”¨
            "news", "announcement" # æ–°è
        ]
        
        # åŠ å…¥å¾å­é é¢ç™¼ç¾çš„å¾Œç¶´
        if hasattr(self, 'fuzz_targets'):
            candidates.extend(self.fuzz_targets)
            
        # å»é‡
        candidates = sorted(list(set(candidates)))

        for suffix in candidates:
            # æ§‹é€ é¡ä¼¼å·²çŸ¥æˆåŠŸçš„è·¯å¾‘çµæ§‹
            endpoint = f"{base_api}/{suffix}"
            full_url = f"{self.BASE_URL}{endpoint}"
            
            try:
                time.sleep(0.5) # å¢åŠ å»¶é²
                
                # æŸäº› API åš´æ ¼æª¢æŸ¥ Referer
                headers = self.HEADERS.copy()
                headers['Referer'] = f"https://wantrich.chinatimes.com/tw-market/listed/stock/{self.stock_id}"
                
                response = self.session.get(full_url, headers=headers, timeout=5)
                status = response.status_code
                
                if status == 200:
                    content_type = response.headers.get('Content-Type', '')
                    is_json = 'application/json' in content_type
                    
                    if is_json:
                        try:
                            data = response.json()
                            if not data:
                                print(f"âš ï¸ [200] {endpoint:<40} | Empty JSON")
                            else:
                                preview = json.dumps(data, ensure_ascii=False)[:100] + "..."
                                print(f"âœ… [200] {endpoint:<40} | JSON: Yes | {preview}")
                        except:
                            print(f"âš ï¸ [200] {endpoint:<40} | Invalid JSON Body")
                    else:
                        # è§£æ HTML Title ä»¥è­˜åˆ¥ Soft 404
                        title_match = re.search(r'<title>(.*?)</title>', response.text, re.IGNORECASE)
                        title = title_match.group(1).strip() if title_match else "No Title"
                        # åªé¡¯ç¤ºéé è¨­æ¨™é¡Œçš„çµæœï¼Œéæ¿¾é›œè¨Š
                        if "æ—ºå¾—å¯Œ" not in title and "ä¸­æ™‚" not in title:
                            print(f"âš ï¸ [200] {endpoint:<40} | HTML: {title[:20]}")
                        elif suffix == "k1": # k1 æ˜¯æˆ‘å€‘å·²çŸ¥çš„ï¼Œç‰¹åˆ¥é—œæ³¨å®ƒç‚ºä»€éº¼å¤±æ•—
                             print(f"âŒ [200] {endpoint:<40} | Failed (Got HTML Page: {title[:20]})")
                        
                elif status == 404:
                    pass
                elif status == 403:
                    print(f"ğŸš« [403] {endpoint:<40} | Forbidden")
                else:
                    print(f"â“ [{status}] {endpoint:<40}")

            except Exception as e:
                print(f"ğŸ’¥ [ERR] {endpoint:<40} | {str(e)}")

    def run(self):
        self.fuzz_targets = []
        print(f"=== é–‹å§‹æ¢æ¸¬ Chinatimes è‚¡ç¥¨ API (Target: {self.stock_id}) ===")
        self.probe_html_source()
        self.probe_subpages() # æ–°å¢: åˆ†æå­é é¢
        self.probe_api_endpoints()
        print("\n=== æ¢æ¸¬çµæŸ ===")

if __name__ == "__main__":
    probe = ChinatimesProbe(stock_id="2330")
    probe.run()